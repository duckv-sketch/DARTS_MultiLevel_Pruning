{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0171405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error.  nthreads cannot be larger than environment variable \"NUMEXPR_MAX_THREADS\" (64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader_darts import get_dataloaders_simple\n",
    "# from darts_search_bdp import train_darts_search_bdp\n",
    "from darts_search_bdp import train_darts_search_bdp\n",
    "from model_build import FinalNetwork\n",
    "from cell_plot import plot_cell\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48d0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading 50/50 split data...\n",
      "[DEBUG] Loaded ./PSG/SC4001E0.npz → 841 samples\n",
      "[DEBUG] Loaded ./PSG/SC4002E0.npz → 1127 samples\n",
      "[DEBUG] Loaded ./PSG/SC4011E0.npz → 1103 samples\n",
      "[DEBUG] Loaded ./PSG/SC4012E0.npz → 1186 samples\n",
      "[DEBUG] Loaded ./PSG/SC4021E0.npz → 1025 samples\n",
      "[DEBUG] Loaded ./PSG/SC4022E0.npz → 1009 samples\n",
      "[DEBUG] Loaded ./PSG/SC4031E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4032E0.npz → 911 samples\n",
      "[DEBUG] Loaded ./PSG/SC4041E0.npz → 1235 samples\n",
      "[DEBUG] Loaded ./PSG/SC4042E0.npz → 1200 samples\n",
      "[DEBUG] Loaded ./PSG/SC4051E0.npz → 672 samples\n",
      "[DEBUG] Loaded ./PSG/SC4052E0.npz → 1246 samples\n",
      "[DEBUG] Loaded ./PSG/SC4061E0.npz → 843 samples\n",
      "[DEBUG] Loaded ./PSG/SC4062E0.npz → 1016 samples\n",
      "[DEBUG] Loaded ./PSG/SC4071E0.npz → 976 samples\n",
      "[DEBUG] Loaded ./PSG/SC4072E0.npz → 1273 samples\n",
      "[DEBUG] Loaded ./PSG/SC4081E0.npz → 1134 samples\n",
      "[DEBUG] Loaded ./PSG/SC4082E0.npz → 1054 samples\n",
      "[DEBUG] Loaded ./PSG/SC4091E0.npz → 1132 samples\n",
      "[DEBUG] Loaded ./PSG/SC4092E0.npz → 1105 samples\n",
      "[DEBUG] Loaded ./PSG/SC4101E0.npz → 1104 samples\n",
      "[DEBUG] Loaded ./PSG/SC4102E0.npz → 1092 samples\n",
      "[DEBUG] Loaded ./PSG/SC4111E0.npz → 928 samples\n",
      "[DEBUG] Loaded ./PSG/SC4112E0.npz → 802 samples\n",
      "[DEBUG] Loaded ./PSG/SC4121E0.npz → 1052 samples\n",
      "[DEBUG] Loaded ./PSG/SC4122E0.npz → 977 samples\n",
      "[DEBUG] Loaded ./PSG/SC4131E0.npz → 1028 samples\n",
      "[DEBUG] Loaded ./PSG/SC4141E0.npz → 1004 samples\n",
      "[DEBUG] Loaded ./PSG/SC4142E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4151E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4152E0.npz → 1762 samples\n",
      "[DEBUG] Loaded ./PSG/SC4161E0.npz → 1144 samples\n",
      "[DEBUG] Loaded ./PSG/SC4162E0.npz → 1003 samples\n",
      "[DEBUG] Loaded ./PSG/SC4171E0.npz → 1002 samples\n",
      "[DEBUG] Loaded ./PSG/SC4172E0.npz → 1773 samples\n",
      "[DEBUG] Loaded ./PSG/SC4181E0.npz → 964 samples\n",
      "[DEBUG] Loaded ./PSG/SC4182E0.npz → 920 samples\n",
      "[DEBUG] Loaded ./PSG/SC4191E0.npz → 1535 samples\n",
      "[DEBUG] Loaded ./PSG/SC4192E0.npz → 1274 samples\n",
      "[DEBUG] Loaded ./PSG/SC4201E0.npz → 1022 samples\n",
      "[DEBUG] Loaded ./PSG/SC4202E0.npz → 1021 samples\n",
      "[DEBUG] Loaded ./PSG/SC4211E0.npz → 1578 samples\n",
      "[DEBUG] Loaded ./PSG/SC4212E0.npz → 808 samples\n",
      "[DEBUG] Loaded ./PSG/SC4221E0.npz → 1099 samples\n",
      "[DEBUG] Loaded ./PSG/SC4222E0.npz → 1108 samples\n",
      "[DEBUG] Loaded ./PSG/SC4231E0.npz → 904 samples\n",
      "[DEBUG] Loaded ./PSG/SC4232E0.npz → 1729 samples\n",
      "[DEBUG] Loaded ./PSG/SC4241E0.npz → 1673 samples\n",
      "[DEBUG] Loaded ./PSG/SC4242E0.npz → 1775 samples\n",
      "[DEBUG] Loaded ./PSG/SC4251E0.npz → 972 samples\n",
      "[DEBUG] Loaded ./PSG/SC4252E0.npz → 1020 samples\n",
      "[DEBUG] Loaded ./PSG/SC4261F0.npz → 1597 samples\n",
      "[DEBUG] Loaded ./PSG/SC4262F0.npz → 980 samples\n",
      "[DEBUG] Loaded ./PSG/SC4271F0.npz → 1052 samples\n",
      "[DEBUG] Loaded ./PSG/SC4272F0.npz → 1090 samples\n",
      "[DEBUG] Loaded ./PSG/SC4281G0.npz → 1127 samples\n",
      "[DEBUG] Loaded ./PSG/SC4282G0.npz → 1070 samples\n",
      "[DEBUG] Loaded ./PSG/SC4291G0.npz → 1131 samples\n",
      "[DEBUG] Loaded ./PSG/SC4292G0.npz → 1605 samples\n",
      "[DEBUG] Loaded ./PSG/SC4301E0.npz → 929 samples\n",
      "[DEBUG] Loaded ./PSG/SC4302E0.npz → 854 samples\n",
      "[DEBUG] Loaded ./PSG/SC4311E0.npz → 1054 samples\n",
      "[DEBUG] Loaded ./PSG/SC4312E0.npz → 1181 samples\n",
      "[DEBUG] Loaded ./PSG/SC4321E0.npz → 1560 samples\n",
      "[DEBUG] Loaded ./PSG/SC4322E0.npz → 1021 samples\n",
      "[DEBUG] Loaded ./PSG/SC4331F0.npz → 1888 samples\n",
      "[DEBUG] Loaded ./PSG/SC4332F0.npz → 1312 samples\n",
      "[DEBUG] Loaded ./PSG/SC4341F0.npz → 1501 samples\n",
      "[DEBUG] Loaded ./PSG/SC4342F0.npz → 1582 samples\n",
      "[DEBUG] Loaded ./PSG/SC4351F0.npz → 976 samples\n",
      "[DEBUG] Loaded ./PSG/SC4352F0.npz → 963 samples\n",
      "[DEBUG] Loaded ./PSG/SC4362F0.npz → 824 samples\n",
      "[DEBUG] Loaded ./PSG/SC4371F0.npz → 918 samples\n",
      "[DEBUG] Loaded ./PSG/SC4372F0.npz → 1509 samples\n",
      "[DEBUG] Loaded ./PSG/SC4381F0.npz → 1776 samples\n",
      "[DEBUG] Loaded ./PSG/SC4382F0.npz → 1871 samples\n",
      "[DEBUG] Loaded ./PSG/SC4401E0.npz → 1064 samples\n",
      "[DEBUG] Loaded ./PSG/SC4402E0.npz → 1072 samples\n",
      "[DEBUG] Loaded ./PSG/SC4411E0.npz → 1078 samples\n",
      "[DEBUG] Loaded ./PSG/SC4412E0.npz → 924 samples\n",
      "[DEBUG] Loaded ./PSG/SC4421E0.npz → 785 samples\n",
      "[DEBUG] Loaded ./PSG/SC4422E0.npz → 884 samples\n",
      "[DEBUG] Loaded ./PSG/SC4431E0.npz → 699 samples\n",
      "[DEBUG] Loaded ./PSG/SC4432E0.npz → 962 samples\n",
      "[DEBUG] Loaded ./PSG/SC4441E0.npz → 1195 samples\n",
      "[DEBUG] Loaded ./PSG/SC4442E0.npz → 1092 samples\n",
      "[DEBUG] Loaded ./PSG/SC4451F0.npz → 1208 samples\n",
      "[DEBUG] Loaded ./PSG/SC4452F0.npz → 1166 samples\n",
      "[DEBUG] Loaded ./PSG/SC4461F0.npz → 983 samples\n",
      "[DEBUG] Loaded ./PSG/SC4462F0.npz → 1022 samples\n",
      "[DEBUG] Loaded ./PSG/SC4471F0.npz → 1187 samples\n",
      "[DEBUG] Loaded ./PSG/SC4472F0.npz → 2161 samples\n",
      "[DEBUG] Loaded ./PSG/SC4481F0.npz → 2027 samples\n",
      "[DEBUG] Loaded ./PSG/SC4482F0.npz → 1910 samples\n",
      "[DEBUG] Loaded ./PSG/SC4491G0.npz → 1101 samples\n",
      "[DEBUG] Loaded ./PSG/SC4492G0.npz → 1040 samples\n",
      "[DEBUG] Loaded ./PSG/SC4501E0.npz → 1326 samples\n",
      "[DEBUG] Loaded ./PSG/SC4502E0.npz → 1103 samples\n",
      "[DEBUG] Loaded ./PSG/SC4511E0.npz → 1087 samples\n",
      "[DEBUG] Loaded ./PSG/SC4512E0.npz → 954 samples\n",
      "[DEBUG] Loaded ./PSG/SC4522E0.npz → 997 samples\n",
      "[DEBUG] Loaded ./PSG/SC4531E0.npz → 1096 samples\n",
      "[DEBUG] Loaded ./PSG/SC4532E0.npz → 1056 samples\n",
      "[DEBUG] Loaded ./PSG/SC4541F0.npz → 1716 samples\n",
      "[DEBUG] Loaded ./PSG/SC4542F0.npz → 1148 samples\n",
      "[DEBUG] Loaded ./PSG/SC4551F0.npz → 1047 samples\n",
      "[DEBUG] Loaded ./PSG/SC4552F0.npz → 1090 samples\n",
      "[DEBUG] Loaded ./PSG/SC4561F0.npz → 1237 samples\n",
      "[DEBUG] Loaded ./PSG/SC4562F0.npz → 1148 samples\n",
      "[DEBUG] Loaded ./PSG/SC4571F0.npz → 1236 samples\n",
      "[DEBUG] Loaded ./PSG/SC4572F0.npz → 1095 samples\n",
      "[DEBUG] Loaded ./PSG/SC4581G0.npz → 1095 samples\n",
      "[DEBUG] Loaded ./PSG/SC4582G0.npz → 1175 samples\n",
      "[DEBUG] Loaded ./PSG/SC4591G0.npz → 1840 samples\n",
      "[DEBUG] Loaded ./PSG/SC4592G0.npz → 1231 samples\n",
      "[DEBUG] Loaded ./PSG/SC4601E0.npz → 1349 samples\n",
      "[DEBUG] Loaded ./PSG/SC4602E0.npz → 2043 samples\n",
      "[DEBUG] Loaded ./PSG/SC4611E0.npz → 1652 samples\n",
      "[DEBUG] Loaded ./PSG/SC4612E0.npz → 1062 samples\n",
      "[DEBUG] Loaded ./PSG/SC4621E0.npz → 1445 samples\n",
      "[DEBUG] Loaded ./PSG/SC4622E0.npz → 1823 samples\n",
      "[DEBUG] Loaded ./PSG/SC4631E0.npz → 1063 samples\n",
      "[DEBUG] Loaded ./PSG/SC4632E0.npz → 1107 samples\n",
      "[DEBUG] Loaded ./PSG/SC4641E0.npz → 1271 samples\n",
      "[DEBUG] Loaded ./PSG/SC4642E0.npz → 2049 samples\n",
      "[DEBUG] Loaded ./PSG/SC4651E0.npz → 2644 samples\n",
      "[DEBUG] Loaded ./PSG/SC4652E0.npz → 1929 samples\n",
      "[DEBUG] Loaded ./PSG/SC4661E0.npz → 2026 samples\n",
      "[DEBUG] Loaded ./PSG/SC4662E0.npz → 1994 samples\n",
      "[DEBUG] Loaded ./PSG/SC4671G0.npz → 1968 samples\n",
      "[DEBUG] Loaded ./PSG/SC4672G0.npz → 1021 samples\n",
      "[DEBUG] Loaded ./PSG/SC4701E0.npz → 1717 samples\n",
      "[DEBUG] Loaded ./PSG/SC4702E0.npz → 1515 samples\n",
      "[DEBUG] Loaded ./PSG/SC4711E0.npz → 1413 samples\n",
      "[DEBUG] Loaded ./PSG/SC4712E0.npz → 1241 samples\n",
      "[DEBUG] Loaded ./PSG/SC4721E0.npz → 1031 samples\n",
      "[DEBUG] Loaded ./PSG/SC4722E0.npz → 1130 samples\n",
      "[DEBUG] Loaded ./PSG/SC4731E0.npz → 2667 samples\n",
      "[DEBUG] Loaded ./PSG/SC4732E0.npz → 2318 samples\n",
      "[DEBUG] Loaded ./PSG/SC4741E0.npz → 2210 samples\n",
      "[DEBUG] Loaded ./PSG/SC4742E0.npz → 1063 samples\n",
      "[DEBUG] Loaded ./PSG/SC4751E0.npz → 2044 samples\n",
      "[DEBUG] Loaded ./PSG/SC4752E0.npz → 1049 samples\n",
      "[DEBUG] Loaded ./PSG/SC4761E0.npz → 1683 samples\n",
      "[DEBUG] Loaded ./PSG/SC4762E0.npz → 2662 samples\n",
      "[DEBUG] Loaded ./PSG/SC4771G0.npz → 1325 samples\n",
      "[DEBUG] Loaded ./PSG/SC4772G0.npz → 1324 samples\n",
      "[DEBUG] Loaded ./PSG/SC4801G0.npz → 1241 samples\n",
      "[DEBUG] Loaded ./PSG/SC4802G0.npz → 1229 samples\n",
      "[DEBUG] Loaded ./PSG/SC4811G0.npz → 1293 samples\n",
      "[DEBUG] Loaded ./PSG/SC4812G0.npz → 1183 samples\n",
      "[DEBUG] Loaded ./PSG/SC4821G0.npz → 1704 samples\n",
      "[DEBUG] Loaded ./PSG/SC4822G0.npz → 1366 samples\n",
      "[INFO] DARTS will run on 97739 train samples and 97740 val samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Set random seed\n",
    "set_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. Load data\n",
    "print(\"[INFO] Loading 50/50 split data...\")\n",
    "train_loader, val_loader, num_classes = get_dataloaders_simple(batch_size=32)\n",
    "print(f\"[INFO] DARTS will run on {len(train_loader.dataset.y)} train samples and {len(val_loader.dataset.y)} val samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run DARTS search with pruning\n",
    "print(\"[INFO] Running DARTS search with BDP...\")\n",
    "searched_genotype, pruned_train_loader, pruned_val_loader = train_darts_search_bdp(\n",
    "    train_loader, val_loader, num_classes,\n",
    "    epochs=25, prune_every=5, pt=0.05, pv=0.05,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27bb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
    "\n",
    "searched_genotype = Genotype(\n",
    "    normal=[\n",
    "        ('dil_conv_1x5', 1),\n",
    "        ('dil_conv_1x5', 0),\n",
    "        ('sep_conv_1x5', 0),\n",
    "        ('sep_conv_1x5', 1),\n",
    "        ('max_pool_3x3', 2),\n",
    "        ('max_pool_3x3', 1),\n",
    "        ('max_pool_3x3', 4),\n",
    "        ('max_pool_3x3', 2),\n",
    "        ('dil_conv_1x5', 4),\n",
    "        ('dil_conv_1x5', 3)\n",
    "    ],\n",
    "    normal_concat=[0, 1, 2, 3, 4],\n",
    "    \n",
    "    reduce=[\n",
    "        ('sep_conv_1x3', 1),\n",
    "        ('sep_conv_1x5', 0),\n",
    "        ('dil_conv_1x5', 0),\n",
    "        ('max_pool_3x3', 2),\n",
    "        ('max_pool_3x3', 3),\n",
    "        ('max_pool_3x3', 0),\n",
    "        ('max_pool_3x3', 4),\n",
    "        ('max_pool_3x3', 3),\n",
    "        ('max_pool_3x3', 5),\n",
    "        ('max_pool_3x3', 4)\n",
    "    ],\n",
    "    reduce_concat=[0, 1, 2, 3, 4]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67d45699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://deb.nodesource.com/node_20.x nodistro InRelease [12.1 kB]\n",
      "Get:2 https://deb.nodesource.com/node_20.x nodistro/main amd64 Packages [12.4 kB]0m\u001b[33m\u001b[33m\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \u001b[0m\u001b[33m\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]m3m\u001b[33m\u001b[33m\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3207 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]m\u001b[33m\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB][0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB][0m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1575 kB]m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5290 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3518 kB]\u001b[33m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5103 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1270 kB]\n",
      "Fetched 40.6 MB in 5s (8263 kB/s)33m                        \u001b[0m[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "179 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Note, selecting 'libgraphviz-dev' instead of 'graphviz-dev'\n",
      "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
      "The following additional packages will be installed:\n",
      "  libgvc6-plugins-gtk libxdot4\n",
      "The following NEW packages will be installed:\n",
      "  libgraphviz-dev libgvc6-plugins-gtk libxdot4\n",
      "0 upgraded, 3 newly installed, 0 to remove and 179 not upgraded.\n",
      "Need to get 97.4 kB of archives.\n",
      "After this operation, 454 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libxdot4 amd64 2.42.2-6ubuntu0.1 [16.4 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6ubuntu0.1 [22.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgraphviz-dev amd64 2.42.2-6ubuntu0.1 [58.5 kB]\n",
      "Fetched 97.4 kB in 2s (63.2 kB/s)        \u001b[0m\u001b[33m\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libxdot4:amd64.\n",
      "(Reading database ... 92264 files and directories currently installed.)\n",
      "Preparing to unpack .../libxdot4_2.42.2-6ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libgvc6-plugins-gtk.\n",
      "Preparing to unpack .../libgvc6-plugins-gtk_2.42.2-6ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libgraphviz-dev:amd64.\n",
      "Preparing to unpack .../libgraphviz-dev_2.42.2-6ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pygraphviz\n",
      "  Downloading pygraphviz-1.14.tar.gz (106 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pygraphviz\n",
      "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygraphviz: filename=pygraphviz-1.14-cp310-cp310-linux_x86_64.whl size=168671 sha256=e0ea1a0d35a828244d631fd6e4604c879429a1788ab0639c4259d165cd3178cd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvnuohbu/wheels/61/ab/cd/e24a22c32830b8b4948c8887d8714d399f0f806f206a034698\n",
      "Successfully built pygraphviz\n",
      "\u001b[33mWARNING: Error parsing dependencies of devscripts: Invalid version: '2.22.1ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pygraphviz\n",
      "Successfully installed pygraphviz-1.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "[INFO] Visualizing searched cells...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/Duc/NASEEG2/NASEEG/SleepC/sleep_nas/cell_plot.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y graphviz graphviz-dev\n",
    "!pip install pygraphviz\n",
    "\n",
    "# 4. Visualize searched cells\n",
    "print(\"[INFO] Visualizing searched cells...\")\n",
    "plot_cell(searched_genotype, 'normal')\n",
    "plot_cell(searched_genotype, 'reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774adc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running 5-Fold Cross Validation on pruned data...\n",
      "✅ Đã lưu dữ liệu gốc (KHÔNG chuẩn hóa) vào 'pruned_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"[INFO] Running 5-Fold Cross Validation on pruned data...\")\n",
    "\n",
    "# === Hàm trích xuất toàn bộ dữ liệu từ DataLoader ===\n",
    "def extract_from_loader(loader):\n",
    "    X_list, y_list = [], []\n",
    "    for x, y in loader:\n",
    "        X_list.append(x.cpu())\n",
    "        y_list.append(y.cpu())\n",
    "    return torch.cat(X_list, dim=0), torch.cat(y_list, dim=0)\n",
    "\n",
    "# === Trích xuất X, y từ train và val loader ===\n",
    "X_train_all, y_train_all = extract_from_loader(pruned_train_loader)\n",
    "X_val_all,   y_val_all   = extract_from_loader(pruned_val_loader)\n",
    "\n",
    "# Gộp train + val\n",
    "X_all = torch.cat([X_train_all, X_val_all], dim=0)  # shape: (N, C, T) hoặc (N, T)\n",
    "y_all = torch.cat([y_train_all, y_val_all], dim=0)  # shape: (N,)\n",
    "\n",
    "# === Chuyển về numpy ===\n",
    "X_np = X_all.numpy()\n",
    "y_np = y_all.numpy().reshape(-1, 1)\n",
    "\n",
    "# === Reshape X về (N, D) nếu cần (flatten nếu có chiều phụ) ===\n",
    "if X_np.ndim > 2:\n",
    "    X_np = X_np.reshape(X_np.shape[0], -1)  # (N, D)\n",
    "\n",
    "# ❌ KHÔNG chuẩn hóa, giữ nguyên raw X_np\n",
    "\n",
    "# === Ghép lại X và y ===\n",
    "data_np = np.hstack((X_np, y_np))  # shape: (N, D+1)\n",
    "\n",
    "# === Tạo DataFrame và lưu CSV ===\n",
    "num_features = X_np.shape[1]\n",
    "column_names = [f\"feature_{i}\" for i in range(num_features)] + [\"label\"]\n",
    "df = pd.DataFrame(data_np, columns=column_names)\n",
    "\n",
    "df.to_csv(\"pruned_dataset.csv\", index=False)\n",
    "print(\"✅ Đã lưu dữ liệu gốc (KHÔNG chuẩn hóa) vào 'pruned_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da84abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Number of samples per class:\n",
      "  Class 0: 53407 samples\n",
      "  Class 1: 20446 samples\n",
      "  Class 2: 57828 samples\n",
      "  Class 3: 12388 samples\n",
      "  Class 4: 23252 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"pruned_dataset_78.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "unique, counts = np.unique(y_np, return_counts=True)\n",
    "\n",
    "print(\"\\n[INFO] Number of samples per class:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696781e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-search filter pruning based on information capacity and independence (Stage 2)\n",
    "# === Entropy-based pruning ===\n",
    "def compute_filter_entropy(weight_tensor):\n",
    "    entropy_list = []\n",
    "    for filt in weight_tensor:\n",
    "        filt_flat = filt.view(filt.size(0), -1)\n",
    "        norms = torch.norm(filt_flat, dim=1) + 1e-6\n",
    "        p = norms / norms.sum()\n",
    "        entropy = -torch.sum(p * torch.log2(p))\n",
    "        entropy_list.append(entropy.item())\n",
    "    return entropy_list\n",
    "\n",
    "def prune_model_entropy(model, prune_ratio=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            weight = module.weight.data.detach().cpu()\n",
    "            entropy = compute_filter_entropy(weight)\n",
    "            entropy_tensor = torch.tensor(entropy)\n",
    "            k = int((1 - prune_ratio) * len(entropy))\n",
    "            topk_indices = torch.topk(entropy_tensor, k=k).indices\n",
    "            mask = torch.zeros_like(entropy_tensor)\n",
    "            mask[topk_indices] = 1.0\n",
    "            full_mask = mask[:, None, None].expand_as(weight).to(module.weight.device)\n",
    "            module.weight.data *= full_mask\n",
    "    return model\n",
    "\n",
    "def count_pruned_weights(model):\n",
    "    total, nonzero = 0, 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
    "            w = module.weight.data\n",
    "            total += w.numel()\n",
    "            nonzero += w.nonzero().size(0)\n",
    "    zero = total - nonzero\n",
    "    print(f\"[INFO] Total weights: {total}\")\n",
    "    print(f\"[INFO] Non-zero weights: {nonzero}\")\n",
    "    print(f\"[INFO] Pruned weights: {zero}\")\n",
    "    print(f\"[INFO] Pruned ratio: {100 * zero / total:.2f}%\")\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred, num_classes):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    gmeans = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        recall_c = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        gmeans.append(recall_c)\n",
    "\n",
    "    mgm = np.sqrt(np.prod(gmeans)) if np.all(np.array(gmeans) > 0) else 0.0\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, mf1, mgm, prec, rec, f1s, gmeans, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d4d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.5881 | Train Acc: 0.7674 | Val Loss: 0.5197 | Val Acc: 0.8065\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5273 | Train Acc: 0.7935 | Val Loss: 0.5245 | Val Acc: 0.8017\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5118 | Train Acc: 0.8011 | Val Loss: 0.4812 | Val Acc: 0.8120\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.4980 | Train Acc: 0.8062 | Val Loss: 0.4819 | Val Acc: 0.8143\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.4891 | Train Acc: 0.8086 | Val Loss: 0.5630 | Val Acc: 0.7838\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4821 | Train Acc: 0.8121 | Val Loss: 0.4602 | Val Acc: 0.8210\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4744 | Train Acc: 0.8164 | Val Loss: 0.4610 | Val Acc: 0.8225\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4709 | Train Acc: 0.8176 | Val Loss: 0.4630 | Val Acc: 0.8224\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4656 | Train Acc: 0.8190 | Val Loss: 0.4462 | Val Acc: 0.8290\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4606 | Train Acc: 0.8209 | Val Loss: 0.4578 | Val Acc: 0.8240\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4562 | Train Acc: 0.8245 | Val Loss: 0.4661 | Val Acc: 0.8175\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4513 | Train Acc: 0.8248 | Val Loss: 0.4430 | Val Acc: 0.8303\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4491 | Train Acc: 0.8252 | Val Loss: 0.4560 | Val Acc: 0.8240\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4448 | Train Acc: 0.8282 | Val Loss: 0.4474 | Val Acc: 0.8275\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4409 | Train Acc: 0.8282 | Val Loss: 0.4496 | Val Acc: 0.8278\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4375 | Train Acc: 0.8309 | Val Loss: 0.4377 | Val Acc: 0.8311\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4340 | Train Acc: 0.8312 | Val Loss: 0.4603 | Val Acc: 0.8270\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4303 | Train Acc: 0.8331 | Val Loss: 0.4418 | Val Acc: 0.8294\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4280 | Train Acc: 0.8337 | Val Loss: 0.4390 | Val Acc: 0.8300\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4238 | Train Acc: 0.8364 | Val Loss: 0.4430 | Val Acc: 0.8310\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4202 | Train Acc: 0.8363 | Val Loss: 0.4730 | Val Acc: 0.8146\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4156 | Train Acc: 0.8385 | Val Loss: 0.4527 | Val Acc: 0.8252\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4122 | Train Acc: 0.8391 | Val Loss: 0.4923 | Val Acc: 0.8090\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4108 | Train Acc: 0.8405 | Val Loss: 0.4437 | Val Acc: 0.8339\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4092 | Train Acc: 0.8412 | Val Loss: 0.4566 | Val Acc: 0.8249\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4050 | Train Acc: 0.8422 | Val Loss: 0.4587 | Val Acc: 0.8229\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4012 | Train Acc: 0.8444 | Val Loss: 0.4518 | Val Acc: 0.8306\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.3986 | Train Acc: 0.8459 | Val Loss: 0.4760 | Val Acc: 0.8190\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.3925 | Train Acc: 0.8478 | Val Loss: 0.4413 | Val Acc: 0.8329\n",
      "[Fold 1 | Epoch 30] Train Loss: 0.3916 | Train Acc: 0.8477 | Val Loss: 0.4778 | Val Acc: 0.8243\n",
      "[Fold 1 | Epoch 31] Train Loss: 0.3882 | Train Acc: 0.8486 | Val Loss: 0.4530 | Val Acc: 0.8255\n",
      "[Fold 1 | Epoch 32] Train Loss: 0.3870 | Train Acc: 0.8497 | Val Loss: 0.4485 | Val Acc: 0.8296\n",
      "[Fold 1 | Epoch 33] Train Loss: 0.3850 | Train Acc: 0.8503 | Val Loss: 0.4609 | Val Acc: 0.8287\n",
      "[Fold 1 | Epoch 34] Train Loss: 0.3792 | Train Acc: 0.8530 | Val Loss: 0.4519 | Val Acc: 0.8312\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 34.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 24\n",
      "ACC: 0.8339 | MF1: 0.7890 | G-Mean: 0.8661\n",
      "[Class 0] Prec: 0.9323 | Rec: 0.9297 | F1: 0.9310 | GM: 0.9487\n",
      "[Class 1] Prec: 0.5683 | Rec: 0.4938 | F1: 0.5284 | GM: 0.6843\n",
      "[Class 2] Prec: 0.8579 | Rec: 0.8775 | F1: 0.8676 | GM: 0.9001\n",
      "[Class 3] Prec: 0.8186 | Rec: 0.8922 | F1: 0.8538 | GM: 0.9373\n",
      "[Class 4] Prec: 0.7579 | Rec: 0.7710 | F1: 0.7644 | GM: 0.8602\n",
      "\n",
      "===== Fold 2 =====\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.5905 | Train Acc: 0.7690 | Val Loss: 0.5343 | Val Acc: 0.7874\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5271 | Train Acc: 0.7936 | Val Loss: 0.4929 | Val Acc: 0.8060\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5075 | Train Acc: 0.8026 | Val Loss: 0.4847 | Val Acc: 0.8125\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.4994 | Train Acc: 0.8076 | Val Loss: 0.4833 | Val Acc: 0.8091\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.4880 | Train Acc: 0.8105 | Val Loss: 0.4844 | Val Acc: 0.8117\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4827 | Train Acc: 0.8123 | Val Loss: 0.4593 | Val Acc: 0.8236\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4755 | Train Acc: 0.8150 | Val Loss: 0.4625 | Val Acc: 0.8213\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4706 | Train Acc: 0.8181 | Val Loss: 0.4898 | Val Acc: 0.8109\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4657 | Train Acc: 0.8188 | Val Loss: 0.4597 | Val Acc: 0.8222\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4602 | Train Acc: 0.8220 | Val Loss: 0.4482 | Val Acc: 0.8272\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4568 | Train Acc: 0.8237 | Val Loss: 0.4623 | Val Acc: 0.8214\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4520 | Train Acc: 0.8246 | Val Loss: 0.4475 | Val Acc: 0.8250\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4491 | Train Acc: 0.8262 | Val Loss: 0.4463 | Val Acc: 0.8265\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4469 | Train Acc: 0.8271 | Val Loss: 0.4569 | Val Acc: 0.8228\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4427 | Train Acc: 0.8284 | Val Loss: 0.4552 | Val Acc: 0.8247\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4389 | Train Acc: 0.8298 | Val Loss: 0.4512 | Val Acc: 0.8273\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4362 | Train Acc: 0.8313 | Val Loss: 0.4797 | Val Acc: 0.8197\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4325 | Train Acc: 0.8329 | Val Loss: 0.4454 | Val Acc: 0.8275\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4290 | Train Acc: 0.8339 | Val Loss: 0.4538 | Val Acc: 0.8248\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4277 | Train Acc: 0.8343 | Val Loss: 0.4421 | Val Acc: 0.8266\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4233 | Train Acc: 0.8365 | Val Loss: 0.4619 | Val Acc: 0.8228\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4191 | Train Acc: 0.8386 | Val Loss: 0.4530 | Val Acc: 0.8264\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4166 | Train Acc: 0.8386 | Val Loss: 0.4559 | Val Acc: 0.8231\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4129 | Train Acc: 0.8402 | Val Loss: 0.4608 | Val Acc: 0.8233\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4107 | Train Acc: 0.8416 | Val Loss: 0.4613 | Val Acc: 0.8221\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4086 | Train Acc: 0.8420 | Val Loss: 0.4495 | Val Acc: 0.8289\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4046 | Train Acc: 0.8432 | Val Loss: 0.4452 | Val Acc: 0.8305\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.4031 | Train Acc: 0.8442 | Val Loss: 0.4533 | Val Acc: 0.8265\n",
      "[Fold 2 | Epoch 29] Train Loss: 0.3995 | Train Acc: 0.8450 | Val Loss: 0.4591 | Val Acc: 0.8222\n",
      "[Fold 2 | Epoch 30] Train Loss: 0.3972 | Train Acc: 0.8469 | Val Loss: 0.4534 | Val Acc: 0.8284\n",
      "[Fold 2 | Epoch 31] Train Loss: 0.3929 | Train Acc: 0.8482 | Val Loss: 0.4684 | Val Acc: 0.8189\n",
      "[Fold 2 | Epoch 32] Train Loss: 0.3922 | Train Acc: 0.8475 | Val Loss: 0.4556 | Val Acc: 0.8259\n",
      "[Fold 2 | Epoch 33] Train Loss: 0.3892 | Train Acc: 0.8477 | Val Loss: 0.4642 | Val Acc: 0.8246\n",
      "[Fold 2 | Epoch 34] Train Loss: 0.3857 | Train Acc: 0.8510 | Val Loss: 0.4748 | Val Acc: 0.8205\n",
      "[Fold 2 | Epoch 35] Train Loss: 0.3813 | Train Acc: 0.8525 | Val Loss: 0.4842 | Val Acc: 0.8163\n",
      "[Fold 2 | Epoch 36] Train Loss: 0.3799 | Train Acc: 0.8527 | Val Loss: 0.4525 | Val Acc: 0.8246\n",
      "[Fold 2 | Epoch 37] Train Loss: 0.3775 | Train Acc: 0.8539 | Val Loss: 0.4530 | Val Acc: 0.8284\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 37.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 27\n",
      "ACC: 0.8305 | MF1: 0.7841 | G-Mean: 0.8615\n",
      "[Class 0] Prec: 0.9362 | Rec: 0.9164 | F1: 0.9262 | GM: 0.9431\n",
      "[Class 1] Prec: 0.5801 | Rec: 0.4550 | F1: 0.5100 | GM: 0.6588\n",
      "[Class 2] Prec: 0.8520 | Rec: 0.8852 | F1: 0.8683 | GM: 0.9019\n",
      "[Class 3] Prec: 0.8402 | Rec: 0.8791 | F1: 0.8592 | GM: 0.9313\n",
      "[Class 4] Prec: 0.7172 | Rec: 0.8016 | F1: 0.7570 | GM: 0.8723\n",
      "\n",
      "===== Fold 3 =====\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.5827 | Train Acc: 0.7711 | Val Loss: 0.5296 | Val Acc: 0.7952\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5203 | Train Acc: 0.7969 | Val Loss: 0.5137 | Val Acc: 0.8023\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5076 | Train Acc: 0.8021 | Val Loss: 0.5081 | Val Acc: 0.8043\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.4952 | Train Acc: 0.8073 | Val Loss: 0.4857 | Val Acc: 0.8172\n",
      "[Fold 3 | Epoch 5] Train Loss: 0.4871 | Train Acc: 0.8100 | Val Loss: 0.4864 | Val Acc: 0.8146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3 | Epoch 6] Train Loss: 0.4811 | Train Acc: 0.8127 | Val Loss: 0.4752 | Val Acc: 0.8170\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4758 | Train Acc: 0.8156 | Val Loss: 0.4588 | Val Acc: 0.8237\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4710 | Train Acc: 0.8168 | Val Loss: 0.4709 | Val Acc: 0.8179\n",
      "[Fold 3 | Epoch 9] Train Loss: 0.4649 | Train Acc: 0.8200 | Val Loss: 0.4469 | Val Acc: 0.8283\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4594 | Train Acc: 0.8208 | Val Loss: 0.4725 | Val Acc: 0.8182\n",
      "[Fold 3 | Epoch 11] Train Loss: 0.4552 | Train Acc: 0.8234 | Val Loss: 0.4389 | Val Acc: 0.8285\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4508 | Train Acc: 0.8248 | Val Loss: 0.4535 | Val Acc: 0.8262\n",
      "[Fold 3 | Epoch 13] Train Loss: 0.4476 | Train Acc: 0.8274 | Val Loss: 0.4847 | Val Acc: 0.8085\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4439 | Train Acc: 0.8273 | Val Loss: 0.5504 | Val Acc: 0.7908\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4387 | Train Acc: 0.8296 | Val Loss: 0.4408 | Val Acc: 0.8298\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4371 | Train Acc: 0.8310 | Val Loss: 0.4328 | Val Acc: 0.8347\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4327 | Train Acc: 0.8326 | Val Loss: 0.4452 | Val Acc: 0.8288\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4292 | Train Acc: 0.8334 | Val Loss: 0.4357 | Val Acc: 0.8303\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4254 | Train Acc: 0.8343 | Val Loss: 0.4416 | Val Acc: 0.8292\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4212 | Train Acc: 0.8369 | Val Loss: 0.4428 | Val Acc: 0.8275\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4188 | Train Acc: 0.8374 | Val Loss: 0.4326 | Val Acc: 0.8346\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4129 | Train Acc: 0.8396 | Val Loss: 0.4381 | Val Acc: 0.8302\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4109 | Train Acc: 0.8407 | Val Loss: 0.4445 | Val Acc: 0.8274\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4075 | Train Acc: 0.8424 | Val Loss: 0.4326 | Val Acc: 0.8329\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4060 | Train Acc: 0.8438 | Val Loss: 0.4570 | Val Acc: 0.8267\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4015 | Train Acc: 0.8440 | Val Loss: 0.4412 | Val Acc: 0.8340\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 26.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 16\n",
      "ACC: 0.8347 | MF1: 0.7882 | G-Mean: 0.8597\n",
      "[Class 0] Prec: 0.9237 | Rec: 0.9340 | F1: 0.9288 | GM: 0.9490\n",
      "[Class 1] Prec: 0.5718 | Rec: 0.4744 | F1: 0.5186 | GM: 0.6718\n",
      "[Class 2] Prec: 0.8487 | Rec: 0.8948 | F1: 0.8712 | GM: 0.9052\n",
      "[Class 3] Prec: 0.8697 | Rec: 0.8430 | F1: 0.8561 | GM: 0.9134\n",
      "[Class 4] Prec: 0.7651 | Rec: 0.7678 | F1: 0.7664 | GM: 0.8590\n",
      "\n",
      "===== Fold 4 =====\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.5892 | Train Acc: 0.7687 | Val Loss: 0.5309 | Val Acc: 0.7930\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5289 | Train Acc: 0.7940 | Val Loss: 0.5024 | Val Acc: 0.8088\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5114 | Train Acc: 0.8015 | Val Loss: 0.4873 | Val Acc: 0.8080\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5013 | Train Acc: 0.8050 | Val Loss: 0.4868 | Val Acc: 0.8097\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.4926 | Train Acc: 0.8087 | Val Loss: 0.4610 | Val Acc: 0.8203\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4849 | Train Acc: 0.8113 | Val Loss: 0.4674 | Val Acc: 0.8181\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4788 | Train Acc: 0.8138 | Val Loss: 0.4549 | Val Acc: 0.8236\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4725 | Train Acc: 0.8166 | Val Loss: 0.4647 | Val Acc: 0.8184\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4683 | Train Acc: 0.8190 | Val Loss: 0.4563 | Val Acc: 0.8253\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4619 | Train Acc: 0.8213 | Val Loss: 0.4503 | Val Acc: 0.8250\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4579 | Train Acc: 0.8216 | Val Loss: 0.4801 | Val Acc: 0.8144\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4555 | Train Acc: 0.8233 | Val Loss: 0.4640 | Val Acc: 0.8191\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4508 | Train Acc: 0.8256 | Val Loss: 0.4541 | Val Acc: 0.8251\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4476 | Train Acc: 0.8272 | Val Loss: 0.4451 | Val Acc: 0.8251\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4431 | Train Acc: 0.8286 | Val Loss: 0.4617 | Val Acc: 0.8166\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4379 | Train Acc: 0.8305 | Val Loss: 0.4573 | Val Acc: 0.8180\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4341 | Train Acc: 0.8319 | Val Loss: 0.4502 | Val Acc: 0.8255\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4305 | Train Acc: 0.8331 | Val Loss: 0.4483 | Val Acc: 0.8260\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4284 | Train Acc: 0.8344 | Val Loss: 0.4402 | Val Acc: 0.8284\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4225 | Train Acc: 0.8367 | Val Loss: 0.4434 | Val Acc: 0.8287\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4208 | Train Acc: 0.8373 | Val Loss: 0.4648 | Val Acc: 0.8208\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4168 | Train Acc: 0.8386 | Val Loss: 0.4482 | Val Acc: 0.8275\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4144 | Train Acc: 0.8406 | Val Loss: 0.4538 | Val Acc: 0.8252\n",
      "[Fold 4 | Epoch 24] Train Loss: 0.4104 | Train Acc: 0.8418 | Val Loss: 0.4632 | Val Acc: 0.8210\n",
      "[Fold 4 | Epoch 25] Train Loss: 0.4070 | Train Acc: 0.8421 | Val Loss: 0.4521 | Val Acc: 0.8264\n",
      "[Fold 4 | Epoch 26] Train Loss: 0.4033 | Train Acc: 0.8438 | Val Loss: 0.4448 | Val Acc: 0.8277\n",
      "[Fold 4 | Epoch 27] Train Loss: 0.4010 | Train Acc: 0.8446 | Val Loss: 0.4755 | Val Acc: 0.8185\n",
      "[Fold 4 | Epoch 28] Train Loss: 0.3974 | Train Acc: 0.8458 | Val Loss: 0.4545 | Val Acc: 0.8239\n",
      "[Fold 4 | Epoch 29] Train Loss: 0.3940 | Train Acc: 0.8473 | Val Loss: 0.4592 | Val Acc: 0.8261\n",
      "[Fold 4 | Epoch 30] Train Loss: 0.3913 | Train Acc: 0.8489 | Val Loss: 0.4636 | Val Acc: 0.8209\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 30.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 20\n",
      "ACC: 0.8287 | MF1: 0.7868 | G-Mean: 0.8651\n",
      "[Class 0] Prec: 0.9489 | Rec: 0.9010 | F1: 0.9244 | GM: 0.9383\n",
      "[Class 1] Prec: 0.5397 | Rec: 0.5103 | F1: 0.5246 | GM: 0.6921\n",
      "[Class 2] Prec: 0.8597 | Rec: 0.8815 | F1: 0.8705 | GM: 0.9026\n",
      "[Class 3] Prec: 0.8497 | Rec: 0.8603 | F1: 0.8550 | GM: 0.9219\n",
      "[Class 4] Prec: 0.7262 | Rec: 0.7963 | F1: 0.7596 | GM: 0.8709\n",
      "\n",
      "===== Fold 5 =====\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.5852 | Train Acc: 0.7708 | Val Loss: 0.5184 | Val Acc: 0.8013\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5237 | Train Acc: 0.7963 | Val Loss: 0.4899 | Val Acc: 0.8107\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5055 | Train Acc: 0.8026 | Val Loss: 0.5018 | Val Acc: 0.8078\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.4950 | Train Acc: 0.8083 | Val Loss: 0.4856 | Val Acc: 0.8136\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.4855 | Train Acc: 0.8113 | Val Loss: 0.4888 | Val Acc: 0.8160\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.4796 | Train Acc: 0.8135 | Val Loss: 0.4652 | Val Acc: 0.8219\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4727 | Train Acc: 0.8156 | Val Loss: 0.4546 | Val Acc: 0.8268\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4682 | Train Acc: 0.8181 | Val Loss: 0.4899 | Val Acc: 0.8111\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4631 | Train Acc: 0.8205 | Val Loss: 0.5073 | Val Acc: 0.8083\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4583 | Train Acc: 0.8217 | Val Loss: 0.4528 | Val Acc: 0.8223\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4532 | Train Acc: 0.8243 | Val Loss: 0.4598 | Val Acc: 0.8245\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4483 | Train Acc: 0.8264 | Val Loss: 0.4638 | Val Acc: 0.8249\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4468 | Train Acc: 0.8266 | Val Loss: 0.4699 | Val Acc: 0.8197\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4406 | Train Acc: 0.8289 | Val Loss: 0.4529 | Val Acc: 0.8276\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4372 | Train Acc: 0.8301 | Val Loss: 0.4525 | Val Acc: 0.8245\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4335 | Train Acc: 0.8327 | Val Loss: 0.4539 | Val Acc: 0.8269\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4305 | Train Acc: 0.8329 | Val Loss: 0.4790 | Val Acc: 0.8149\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4262 | Train Acc: 0.8336 | Val Loss: 0.4562 | Val Acc: 0.8257\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.4229 | Train Acc: 0.8358 | Val Loss: 0.4565 | Val Acc: 0.8236\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.4192 | Train Acc: 0.8374 | Val Loss: 0.4495 | Val Acc: 0.8278\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.4159 | Train Acc: 0.8387 | Val Loss: 0.4474 | Val Acc: 0.8285\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.4123 | Train Acc: 0.8393 | Val Loss: 0.4451 | Val Acc: 0.8284\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.4097 | Train Acc: 0.8401 | Val Loss: 0.4659 | Val Acc: 0.8219\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.4043 | Train Acc: 0.8425 | Val Loss: 0.4601 | Val Acc: 0.8238\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.4026 | Train Acc: 0.8428 | Val Loss: 0.4519 | Val Acc: 0.8271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5 | Epoch 26] Train Loss: 0.3996 | Train Acc: 0.8451 | Val Loss: 0.4890 | Val Acc: 0.8121\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.3947 | Train Acc: 0.8461 | Val Loss: 0.4607 | Val Acc: 0.8224\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.3922 | Train Acc: 0.8484 | Val Loss: 0.4549 | Val Acc: 0.8275\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.3892 | Train Acc: 0.8498 | Val Loss: 0.4619 | Val Acc: 0.8260\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.3845 | Train Acc: 0.8505 | Val Loss: 0.4853 | Val Acc: 0.8142\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.3811 | Train Acc: 0.8527 | Val Loss: 0.5063 | Val Acc: 0.8147\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 31.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 21\n",
      "ACC: 0.8285 | MF1: 0.7754 | G-Mean: 0.8532\n",
      "[Class 0] Prec: 0.9360 | Rec: 0.9254 | F1: 0.9307 | GM: 0.9477\n",
      "[Class 1] Prec: 0.5951 | Rec: 0.3882 | F1: 0.4699 | GM: 0.6114\n",
      "[Class 2] Prec: 0.8543 | Rec: 0.8845 | F1: 0.8691 | GM: 0.9022\n",
      "[Class 3] Prec: 0.8690 | Rec: 0.8426 | F1: 0.8556 | GM: 0.9131\n",
      "[Class 4] Prec: 0.6724 | Rec: 0.8516 | F1: 0.7515 | GM: 0.8918\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8313 | MF1: 0.7849 | G-Mean: 0.8613\n",
      "[Class 0] Prec: 0.9353 | Rec: 0.9213 | F1: 0.9282 | GM: 0.9454\n",
      "[Class 1] Prec: 0.5689 | Rec: 0.4642 | F1: 0.5112 | GM: 0.6644\n",
      "[Class 2] Prec: 0.8545 | Rec: 0.8847 | F1: 0.8693 | GM: 0.9024\n",
      "[Class 3] Prec: 0.8489 | Rec: 0.8631 | F1: 0.8559 | GM: 0.9233\n",
      "[Class 4] Prec: 0.7253 | Rec: 0.7974 | F1: 0.7596 | GM: 0.8709\n",
      "Confusion Matrix:\n",
      "[[49203  2809   418    52   925]\n",
      " [ 2591  9491  4674     9  3681]\n",
      " [  265  2181 51161  1823  2398]\n",
      " [   32     0  1645 10692    19]\n",
      " [  516  2202  1975    19 18540]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from model_build import FinalNetwork\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "# Z-score normalization\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Evaluation Metric Function ====\n",
    "def evaluate_metrics(y_true, y_pred, num_classes):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    gmeans = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        tn = np.sum((y_pred != c) & (y_true != c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        gm = np.sqrt((tp / (tp + fn + 1e-6)) * (tn / (tn + fp + 1e-6)))\n",
    "        gmeans.append(gm)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, mf1, np.mean(gmeans), prec, rec, f1s, gmeans, cm\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=47)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = FinalNetwork(C=9, num_classes=num_classes, layers=7, genotype=searched_genotype).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOP counter\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(val_true), np.array(val_pred), num_classes)\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7532feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.5977 | Train Acc: 0.7635 | Val Loss: 0.5472 | Val Acc: 0.7883\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5327 | Train Acc: 0.7911 | Val Loss: 0.4989 | Val Acc: 0.8062\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5154 | Train Acc: 0.7997 | Val Loss: 0.5013 | Val Acc: 0.8058\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.5025 | Train Acc: 0.8038 | Val Loss: 0.4796 | Val Acc: 0.8162\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.4952 | Train Acc: 0.8069 | Val Loss: 0.4587 | Val Acc: 0.8242\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4872 | Train Acc: 0.8101 | Val Loss: 0.5139 | Val Acc: 0.8022\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4823 | Train Acc: 0.8126 | Val Loss: 0.4519 | Val Acc: 0.8275\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4762 | Train Acc: 0.8153 | Val Loss: 0.4690 | Val Acc: 0.8180\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4722 | Train Acc: 0.8155 | Val Loss: 0.4520 | Val Acc: 0.8256\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4667 | Train Acc: 0.8189 | Val Loss: 0.4679 | Val Acc: 0.8175\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4619 | Train Acc: 0.8205 | Val Loss: 0.4469 | Val Acc: 0.8271\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4574 | Train Acc: 0.8230 | Val Loss: 0.4493 | Val Acc: 0.8269\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4544 | Train Acc: 0.8241 | Val Loss: 0.4434 | Val Acc: 0.8266\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4505 | Train Acc: 0.8258 | Val Loss: 0.4415 | Val Acc: 0.8297\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4471 | Train Acc: 0.8265 | Val Loss: 0.4454 | Val Acc: 0.8279\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4444 | Train Acc: 0.8277 | Val Loss: 0.4372 | Val Acc: 0.8321\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4395 | Train Acc: 0.8299 | Val Loss: 0.4395 | Val Acc: 0.8292\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4365 | Train Acc: 0.8309 | Val Loss: 0.4488 | Val Acc: 0.8278\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4335 | Train Acc: 0.8321 | Val Loss: 0.4369 | Val Acc: 0.8335\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4300 | Train Acc: 0.8332 | Val Loss: 0.4463 | Val Acc: 0.8286\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4255 | Train Acc: 0.8364 | Val Loss: 0.4433 | Val Acc: 0.8318\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4229 | Train Acc: 0.8369 | Val Loss: 0.4419 | Val Acc: 0.8313\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4210 | Train Acc: 0.8371 | Val Loss: 0.4578 | Val Acc: 0.8228\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4188 | Train Acc: 0.8371 | Val Loss: 0.4438 | Val Acc: 0.8284\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4144 | Train Acc: 0.8390 | Val Loss: 0.4462 | Val Acc: 0.8282\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4122 | Train Acc: 0.8403 | Val Loss: 0.4436 | Val Acc: 0.8331\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4090 | Train Acc: 0.8414 | Val Loss: 0.4465 | Val Acc: 0.8280\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.4064 | Train Acc: 0.8421 | Val Loss: 0.4582 | Val Acc: 0.8278\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.4035 | Train Acc: 0.8432 | Val Loss: 0.4552 | Val Acc: 0.8251\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 29.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 19\n",
      "ACC: 0.8335 | MF1: 0.7886 | G-Mean: 0.8619\n",
      "[Class 0] Prec: 0.9085 | Rec: 0.9463 | F1: 0.9270 | GM: 0.9506\n",
      "[Class 1] Prec: 0.5527 | Rec: 0.5007 | F1: 0.5254 | GM: 0.6876\n",
      "[Class 2] Prec: 0.8739 | Rec: 0.8708 | F1: 0.8724 | GM: 0.9017\n",
      "[Class 3] Prec: 0.8767 | Rec: 0.8373 | F1: 0.8565 | GM: 0.9109\n",
      "[Class 4] Prec: 0.7552 | Rec: 0.7686 | F1: 0.7619 | GM: 0.8587\n",
      "\n",
      "===== Fold 2 =====\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.5952 | Train Acc: 0.7672 | Val Loss: 0.5754 | Val Acc: 0.7652\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5288 | Train Acc: 0.7940 | Val Loss: 0.5145 | Val Acc: 0.7990\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5119 | Train Acc: 0.8017 | Val Loss: 0.4905 | Val Acc: 0.8063\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.5012 | Train Acc: 0.8049 | Val Loss: 0.5496 | Val Acc: 0.7852\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.4926 | Train Acc: 0.8083 | Val Loss: 0.4723 | Val Acc: 0.8132\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4876 | Train Acc: 0.8119 | Val Loss: 0.4942 | Val Acc: 0.8072\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4791 | Train Acc: 0.8134 | Val Loss: 0.4895 | Val Acc: 0.8079\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4744 | Train Acc: 0.8159 | Val Loss: 0.4641 | Val Acc: 0.8169\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4701 | Train Acc: 0.8184 | Val Loss: 0.4597 | Val Acc: 0.8198\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4676 | Train Acc: 0.8185 | Val Loss: 0.4545 | Val Acc: 0.8190\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4614 | Train Acc: 0.8209 | Val Loss: 0.4555 | Val Acc: 0.8220\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4561 | Train Acc: 0.8232 | Val Loss: 0.4497 | Val Acc: 0.8223\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4526 | Train Acc: 0.8244 | Val Loss: 0.4550 | Val Acc: 0.8206\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4516 | Train Acc: 0.8254 | Val Loss: 0.4485 | Val Acc: 0.8254\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4459 | Train Acc: 0.8277 | Val Loss: 0.4458 | Val Acc: 0.8267\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4434 | Train Acc: 0.8290 | Val Loss: 0.4737 | Val Acc: 0.8157\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4373 | Train Acc: 0.8306 | Val Loss: 0.4790 | Val Acc: 0.8126\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4368 | Train Acc: 0.8319 | Val Loss: 0.4543 | Val Acc: 0.8229\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4330 | Train Acc: 0.8324 | Val Loss: 0.4397 | Val Acc: 0.8289\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4316 | Train Acc: 0.8334 | Val Loss: 0.4676 | Val Acc: 0.8142\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4281 | Train Acc: 0.8345 | Val Loss: 0.4470 | Val Acc: 0.8245\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4227 | Train Acc: 0.8374 | Val Loss: 0.4560 | Val Acc: 0.8187\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4210 | Train Acc: 0.8373 | Val Loss: 0.4672 | Val Acc: 0.8175\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4168 | Train Acc: 0.8386 | Val Loss: 0.4559 | Val Acc: 0.8208\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4152 | Train Acc: 0.8392 | Val Loss: 0.4390 | Val Acc: 0.8301\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4112 | Train Acc: 0.8403 | Val Loss: 0.4448 | Val Acc: 0.8253\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4079 | Train Acc: 0.8413 | Val Loss: 0.4493 | Val Acc: 0.8253\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.4067 | Train Acc: 0.8428 | Val Loss: 0.4566 | Val Acc: 0.8227\n",
      "[Fold 2 | Epoch 29] Train Loss: 0.4038 | Train Acc: 0.8432 | Val Loss: 0.4544 | Val Acc: 0.8236\n",
      "[Fold 2 | Epoch 30] Train Loss: 0.4004 | Train Acc: 0.8455 | Val Loss: 0.4561 | Val Acc: 0.8218\n",
      "[Fold 2 | Epoch 31] Train Loss: 0.3957 | Train Acc: 0.8463 | Val Loss: 0.4636 | Val Acc: 0.8200\n",
      "[Fold 2 | Epoch 32] Train Loss: 0.3964 | Train Acc: 0.8472 | Val Loss: 0.4625 | Val Acc: 0.8218\n",
      "[Fold 2 | Epoch 33] Train Loss: 0.3905 | Train Acc: 0.8485 | Val Loss: 0.4496 | Val Acc: 0.8250\n",
      "[Fold 2 | Epoch 34] Train Loss: 0.3881 | Train Acc: 0.8501 | Val Loss: 0.5058 | Val Acc: 0.8007\n",
      "[Fold 2 | Epoch 35] Train Loss: 0.3857 | Train Acc: 0.8514 | Val Loss: 0.4645 | Val Acc: 0.8203\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 35.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 25\n",
      "ACC: 0.8301 | MF1: 0.7868 | G-Mean: 0.8604\n",
      "[Class 0] Prec: 0.9162 | Rec: 0.9340 | F1: 0.9250 | GM: 0.9468\n",
      "[Class 1] Prec: 0.5500 | Rec: 0.4882 | F1: 0.5173 | GM: 0.6789\n",
      "[Class 2] Prec: 0.8656 | Rec: 0.8713 | F1: 0.8684 | GM: 0.8995\n",
      "[Class 3] Prec: 0.8960 | Rec: 0.8352 | F1: 0.8645 | GM: 0.9104\n",
      "[Class 4] Prec: 0.7325 | Rec: 0.7869 | F1: 0.7587 | GM: 0.8663\n",
      "\n",
      "===== Fold 3 =====\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.5953 | Train Acc: 0.7645 | Val Loss: 0.5404 | Val Acc: 0.7874\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5312 | Train Acc: 0.7920 | Val Loss: 0.5556 | Val Acc: 0.7834\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5173 | Train Acc: 0.7990 | Val Loss: 0.4931 | Val Acc: 0.8090\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.5064 | Train Acc: 0.8028 | Val Loss: 0.5023 | Val Acc: 0.8063\n",
      "[Fold 3 | Epoch 5] Train Loss: 0.4987 | Train Acc: 0.8058 | Val Loss: 0.4663 | Val Acc: 0.8204\n",
      "[Fold 3 | Epoch 6] Train Loss: 0.4896 | Train Acc: 0.8091 | Val Loss: 0.4672 | Val Acc: 0.8226\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4835 | Train Acc: 0.8120 | Val Loss: 0.4523 | Val Acc: 0.8256\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4782 | Train Acc: 0.8141 | Val Loss: 0.4637 | Val Acc: 0.8177\n",
      "[Fold 3 | Epoch 9] Train Loss: 0.4728 | Train Acc: 0.8165 | Val Loss: 0.4682 | Val Acc: 0.8191\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4686 | Train Acc: 0.8175 | Val Loss: 0.4791 | Val Acc: 0.8152\n",
      "[Fold 3 | Epoch 11] Train Loss: 0.4628 | Train Acc: 0.8205 | Val Loss: 0.4582 | Val Acc: 0.8235\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4598 | Train Acc: 0.8216 | Val Loss: 0.4758 | Val Acc: 0.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3 | Epoch 13] Train Loss: 0.4573 | Train Acc: 0.8223 | Val Loss: 0.4610 | Val Acc: 0.8211\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4532 | Train Acc: 0.8230 | Val Loss: 0.4577 | Val Acc: 0.8227\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4489 | Train Acc: 0.8263 | Val Loss: 0.4448 | Val Acc: 0.8287\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4444 | Train Acc: 0.8282 | Val Loss: 0.4511 | Val Acc: 0.8252\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4417 | Train Acc: 0.8289 | Val Loss: 0.4411 | Val Acc: 0.8299\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4373 | Train Acc: 0.8297 | Val Loss: 0.4503 | Val Acc: 0.8232\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4353 | Train Acc: 0.8310 | Val Loss: 0.4423 | Val Acc: 0.8312\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4308 | Train Acc: 0.8328 | Val Loss: 0.4462 | Val Acc: 0.8283\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4275 | Train Acc: 0.8344 | Val Loss: 0.4836 | Val Acc: 0.8074\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4265 | Train Acc: 0.8350 | Val Loss: 0.4892 | Val Acc: 0.8153\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4229 | Train Acc: 0.8366 | Val Loss: 0.4402 | Val Acc: 0.8318\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4187 | Train Acc: 0.8382 | Val Loss: 0.4668 | Val Acc: 0.8195\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4156 | Train Acc: 0.8391 | Val Loss: 0.4447 | Val Acc: 0.8295\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4135 | Train Acc: 0.8399 | Val Loss: 0.4440 | Val Acc: 0.8278\n",
      "[Fold 3 | Epoch 27] Train Loss: 0.4120 | Train Acc: 0.8406 | Val Loss: 0.4419 | Val Acc: 0.8309\n",
      "[Fold 3 | Epoch 28] Train Loss: 0.4101 | Train Acc: 0.8414 | Val Loss: 0.4459 | Val Acc: 0.8304\n",
      "[Fold 3 | Epoch 29] Train Loss: 0.4054 | Train Acc: 0.8418 | Val Loss: 0.4474 | Val Acc: 0.8306\n",
      "[Fold 3 | Epoch 30] Train Loss: 0.4025 | Train Acc: 0.8444 | Val Loss: 0.4577 | Val Acc: 0.8244\n",
      "[Fold 3 | Epoch 31] Train Loss: 0.4014 | Train Acc: 0.8454 | Val Loss: 0.4649 | Val Acc: 0.8229\n",
      "[Fold 3 | Epoch 32] Train Loss: 0.3971 | Train Acc: 0.8465 | Val Loss: 0.4598 | Val Acc: 0.8275\n",
      "[Fold 3 | Epoch 33] Train Loss: 0.3931 | Train Acc: 0.8483 | Val Loss: 0.4510 | Val Acc: 0.8293\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 33.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 23\n",
      "ACC: 0.8318 | MF1: 0.7876 | G-Mean: 0.8610\n",
      "[Class 0] Prec: 0.9119 | Rec: 0.9449 | F1: 0.9281 | GM: 0.9513\n",
      "[Class 1] Prec: 0.5402 | Rec: 0.5107 | F1: 0.5250 | GM: 0.6930\n",
      "[Class 2] Prec: 0.8646 | Rec: 0.8760 | F1: 0.8702 | GM: 0.9014\n",
      "[Class 3] Prec: 0.8685 | Rec: 0.8501 | F1: 0.8592 | GM: 0.9172\n",
      "[Class 4] Prec: 0.7776 | Rec: 0.7345 | F1: 0.7554 | GM: 0.8420\n",
      "\n",
      "===== Fold 4 =====\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.5884 | Train Acc: 0.7687 | Val Loss: 0.5636 | Val Acc: 0.7816\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5278 | Train Acc: 0.7941 | Val Loss: 0.5371 | Val Acc: 0.7893\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5085 | Train Acc: 0.8023 | Val Loss: 0.4940 | Val Acc: 0.7996\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5008 | Train Acc: 0.8058 | Val Loss: 0.5247 | Val Acc: 0.7925\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.4904 | Train Acc: 0.8107 | Val Loss: 0.4920 | Val Acc: 0.8058\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4846 | Train Acc: 0.8125 | Val Loss: 0.5314 | Val Acc: 0.7876\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4770 | Train Acc: 0.8153 | Val Loss: 0.4827 | Val Acc: 0.8082\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4732 | Train Acc: 0.8157 | Val Loss: 0.4783 | Val Acc: 0.8143\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4693 | Train Acc: 0.8188 | Val Loss: 0.4656 | Val Acc: 0.8197\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4638 | Train Acc: 0.8216 | Val Loss: 0.4919 | Val Acc: 0.8109\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4597 | Train Acc: 0.8214 | Val Loss: 0.4508 | Val Acc: 0.8235\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4555 | Train Acc: 0.8234 | Val Loss: 0.4476 | Val Acc: 0.8257\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4522 | Train Acc: 0.8254 | Val Loss: 0.4912 | Val Acc: 0.8040\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4482 | Train Acc: 0.8263 | Val Loss: 0.4446 | Val Acc: 0.8269\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4440 | Train Acc: 0.8283 | Val Loss: 0.4500 | Val Acc: 0.8269\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4414 | Train Acc: 0.8292 | Val Loss: 0.4465 | Val Acc: 0.8252\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4366 | Train Acc: 0.8317 | Val Loss: 0.4650 | Val Acc: 0.8165\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4351 | Train Acc: 0.8320 | Val Loss: 0.4481 | Val Acc: 0.8244\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4313 | Train Acc: 0.8339 | Val Loss: 0.4634 | Val Acc: 0.8219\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4278 | Train Acc: 0.8341 | Val Loss: 0.4559 | Val Acc: 0.8202\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4235 | Train Acc: 0.8363 | Val Loss: 0.4458 | Val Acc: 0.8275\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4206 | Train Acc: 0.8374 | Val Loss: 0.4544 | Val Acc: 0.8264\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4184 | Train Acc: 0.8389 | Val Loss: 0.4678 | Val Acc: 0.8166\n",
      "[Fold 4 | Epoch 24] Train Loss: 0.4150 | Train Acc: 0.8398 | Val Loss: 0.4574 | Val Acc: 0.8220\n",
      "[Fold 4 | Epoch 25] Train Loss: 0.4124 | Train Acc: 0.8407 | Val Loss: 0.4375 | Val Acc: 0.8313\n",
      "[Fold 4 | Epoch 26] Train Loss: 0.4099 | Train Acc: 0.8415 | Val Loss: 0.4514 | Val Acc: 0.8226\n",
      "[Fold 4 | Epoch 27] Train Loss: 0.4078 | Train Acc: 0.8416 | Val Loss: 0.4416 | Val Acc: 0.8318\n",
      "[Fold 4 | Epoch 28] Train Loss: 0.4035 | Train Acc: 0.8439 | Val Loss: 0.4538 | Val Acc: 0.8284\n",
      "[Fold 4 | Epoch 29] Train Loss: 0.4008 | Train Acc: 0.8452 | Val Loss: 0.4437 | Val Acc: 0.8270\n",
      "[Fold 4 | Epoch 30] Train Loss: 0.3992 | Train Acc: 0.8455 | Val Loss: 0.4476 | Val Acc: 0.8260\n",
      "[Fold 4 | Epoch 31] Train Loss: 0.3952 | Train Acc: 0.8480 | Val Loss: 0.4495 | Val Acc: 0.8266\n",
      "[Fold 4 | Epoch 32] Train Loss: 0.3926 | Train Acc: 0.8477 | Val Loss: 0.4550 | Val Acc: 0.8238\n",
      "[Fold 4 | Epoch 33] Train Loss: 0.3897 | Train Acc: 0.8483 | Val Loss: 0.4670 | Val Acc: 0.8203\n",
      "[Fold 4 | Epoch 34] Train Loss: 0.3869 | Train Acc: 0.8514 | Val Loss: 0.4688 | Val Acc: 0.8172\n",
      "[Fold 4 | Epoch 35] Train Loss: 0.3845 | Train Acc: 0.8506 | Val Loss: 0.4626 | Val Acc: 0.8206\n",
      "[Fold 4 | Epoch 36] Train Loss: 0.3823 | Train Acc: 0.8524 | Val Loss: 0.4553 | Val Acc: 0.8270\n",
      "[Fold 4 | Epoch 37] Train Loss: 0.3797 | Train Acc: 0.8526 | Val Loss: 0.4530 | Val Acc: 0.8247\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 37.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 27\n",
      "ACC: 0.8318 | MF1: 0.7819 | G-Mean: 0.8545\n",
      "[Class 0] Prec: 0.9387 | Rec: 0.9209 | F1: 0.9297 | GM: 0.9459\n",
      "[Class 1] Prec: 0.5838 | Rec: 0.4437 | F1: 0.5042 | GM: 0.6511\n",
      "[Class 2] Prec: 0.8387 | Rec: 0.9042 | F1: 0.8702 | GM: 0.9062\n",
      "[Class 3] Prec: 0.8828 | Rec: 0.8086 | F1: 0.8441 | GM: 0.8954\n",
      "[Class 4] Prec: 0.7240 | Rec: 0.8028 | F1: 0.7614 | GM: 0.8740\n",
      "\n",
      "===== Fold 5 =====\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.6026 | Train Acc: 0.7627 | Val Loss: 0.5476 | Val Acc: 0.7839\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5302 | Train Acc: 0.7927 | Val Loss: 0.6596 | Val Acc: 0.7398\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5121 | Train Acc: 0.7999 | Val Loss: 0.5272 | Val Acc: 0.7933\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.5026 | Train Acc: 0.8045 | Val Loss: 0.5109 | Val Acc: 0.8016\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.4931 | Train Acc: 0.8073 | Val Loss: 0.5023 | Val Acc: 0.7994\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.4838 | Train Acc: 0.8119 | Val Loss: 0.4908 | Val Acc: 0.8126\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4814 | Train Acc: 0.8124 | Val Loss: 0.5059 | Val Acc: 0.8019\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4753 | Train Acc: 0.8149 | Val Loss: 0.4786 | Val Acc: 0.8159\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4687 | Train Acc: 0.8172 | Val Loss: 0.4594 | Val Acc: 0.8240\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4658 | Train Acc: 0.8184 | Val Loss: 0.4735 | Val Acc: 0.8180\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4603 | Train Acc: 0.8217 | Val Loss: 0.4719 | Val Acc: 0.8170\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4546 | Train Acc: 0.8241 | Val Loss: 0.4552 | Val Acc: 0.8236\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4538 | Train Acc: 0.8242 | Val Loss: 0.4788 | Val Acc: 0.8150\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4484 | Train Acc: 0.8258 | Val Loss: 0.4636 | Val Acc: 0.8220\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4448 | Train Acc: 0.8265 | Val Loss: 0.4630 | Val Acc: 0.8235\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4413 | Train Acc: 0.8291 | Val Loss: 0.4638 | Val Acc: 0.8206\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4371 | Train Acc: 0.8304 | Val Loss: 0.4544 | Val Acc: 0.8239\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4347 | Train Acc: 0.8325 | Val Loss: 0.4583 | Val Acc: 0.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5 | Epoch 19] Train Loss: 0.4307 | Train Acc: 0.8329 | Val Loss: 0.4765 | Val Acc: 0.8187\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 19.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 9\n",
      "ACC: 0.8240 | MF1: 0.7806 | G-Mean: 0.8558\n",
      "[Class 0] Prec: 0.9452 | Rec: 0.9102 | F1: 0.9273 | GM: 0.9422\n",
      "[Class 1] Prec: 0.5258 | Rec: 0.5145 | F1: 0.5201 | GM: 0.6934\n",
      "[Class 2] Prec: 0.8463 | Rec: 0.8845 | F1: 0.8650 | GM: 0.8997\n",
      "[Class 3] Prec: 0.8999 | Rec: 0.7941 | F1: 0.8437 | GM: 0.8879\n",
      "[Class 4] Prec: 0.7270 | Rec: 0.7679 | F1: 0.7469 | GM: 0.8559\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8302 | MF1: 0.7852 | G-Mean: 0.8588\n",
      "[Class 0] Prec: 0.9236 | Rec: 0.9313 | F1: 0.9274 | GM: 0.9474\n",
      "[Class 1] Prec: 0.5490 | Rec: 0.4914 | F1: 0.5186 | GM: 0.6810\n",
      "[Class 2] Prec: 0.8575 | Rec: 0.8813 | F1: 0.8692 | GM: 0.9017\n",
      "[Class 3] Prec: 0.8845 | Rec: 0.8249 | F1: 0.8536 | GM: 0.9043\n",
      "[Class 4] Prec: 0.7425 | Rec: 0.7719 | F1: 0.7569 | GM: 0.8594\n",
      "Confusion Matrix:\n",
      "[[49737  2613   408    43   606]\n",
      " [ 2932 10048  4241     6  3219]\n",
      " [  342  2873 50966  1267  2380]\n",
      " [   35     1  2112 10219    21]\n",
      " [  804  2769  1712    19 17948]]\n"
     ]
    }
   ],
   "source": [
    "#9_1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from model_build import FinalNetwork\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "# Z-score normalization\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Evaluation Metric Function ====\n",
    "def evaluate_metrics(y_true, y_pred, num_classes):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    gmeans = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        tn = np.sum((y_pred != c) & (y_true != c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        gm = np.sqrt((tp / (tp + fn + 1e-6)) * (tn / (tn + fp + 1e-6)))\n",
    "        gmeans.append(gm)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, mf1, np.mean(gmeans), prec, rec, f1s, gmeans, cm\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=47)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = FinalNetwork(C=9, num_classes=num_classes, layers=9, genotype=searched_genotype).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOP counter\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(val_true), np.array(val_pred), num_classes)\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21bce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[INFO] Total parameters BEFORE pruning: 84,812\n",
      "[INFO] Non-zero parameters AFTER pruning: 42,608\n",
      "[INFO] Pruned parameters: 42,204 (49.76%)\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.6056 | Train Acc: 0.7606 | Val Loss: 0.6019 | Val Acc: 0.7646\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5375 | Train Acc: 0.7893 | Val Loss: 0.5465 | Val Acc: 0.7903\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5211 | Train Acc: 0.7964 | Val Loss: 0.5361 | Val Acc: 0.7921\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.5086 | Train Acc: 0.8015 | Val Loss: 0.4933 | Val Acc: 0.8106\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.5000 | Train Acc: 0.8047 | Val Loss: 0.4856 | Val Acc: 0.8128\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4940 | Train Acc: 0.8077 | Val Loss: 0.5566 | Val Acc: 0.7845\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4888 | Train Acc: 0.8105 | Val Loss: 0.5159 | Val Acc: 0.7986\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4829 | Train Acc: 0.8120 | Val Loss: 0.4700 | Val Acc: 0.8184\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4801 | Train Acc: 0.8130 | Val Loss: 0.4630 | Val Acc: 0.8215\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4743 | Train Acc: 0.8145 | Val Loss: 0.4921 | Val Acc: 0.8084\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4715 | Train Acc: 0.8159 | Val Loss: 0.4909 | Val Acc: 0.8099\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4669 | Train Acc: 0.8179 | Val Loss: 0.4605 | Val Acc: 0.8225\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4644 | Train Acc: 0.8184 | Val Loss: 0.4631 | Val Acc: 0.8178\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4597 | Train Acc: 0.8223 | Val Loss: 0.4923 | Val Acc: 0.8058\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4596 | Train Acc: 0.8216 | Val Loss: 0.4575 | Val Acc: 0.8218\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4552 | Train Acc: 0.8241 | Val Loss: 0.4664 | Val Acc: 0.8187\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4527 | Train Acc: 0.8254 | Val Loss: 0.4625 | Val Acc: 0.8172\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4489 | Train Acc: 0.8252 | Val Loss: 0.4944 | Val Acc: 0.8088\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4457 | Train Acc: 0.8272 | Val Loss: 0.4663 | Val Acc: 0.8188\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4441 | Train Acc: 0.8267 | Val Loss: 0.4496 | Val Acc: 0.8265\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4413 | Train Acc: 0.8281 | Val Loss: 0.5101 | Val Acc: 0.7990\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4407 | Train Acc: 0.8288 | Val Loss: 0.4559 | Val Acc: 0.8235\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4378 | Train Acc: 0.8305 | Val Loss: 0.4514 | Val Acc: 0.8250\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4353 | Train Acc: 0.8308 | Val Loss: 0.4657 | Val Acc: 0.8147\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4341 | Train Acc: 0.8315 | Val Loss: 0.4780 | Val Acc: 0.8154\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4318 | Train Acc: 0.8323 | Val Loss: 0.4669 | Val Acc: 0.8163\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4300 | Train Acc: 0.8335 | Val Loss: 0.4646 | Val Acc: 0.8186\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.4278 | Train Acc: 0.8344 | Val Loss: 0.4520 | Val Acc: 0.8252\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.4256 | Train Acc: 0.8352 | Val Loss: 0.4504 | Val Acc: 0.8281\n",
      "[Fold 1 | Epoch 30] Train Loss: 0.4228 | Train Acc: 0.8358 | Val Loss: 0.4577 | Val Acc: 0.8248\n",
      "[Fold 1 | Epoch 31] Train Loss: 0.4224 | Train Acc: 0.8355 | Val Loss: 0.4540 | Val Acc: 0.8276\n",
      "[Fold 1 | Epoch 32] Train Loss: 0.4198 | Train Acc: 0.8370 | Val Loss: 0.4734 | Val Acc: 0.8173\n",
      "[Fold 1 | Epoch 33] Train Loss: 0.4184 | Train Acc: 0.8369 | Val Loss: 0.4654 | Val Acc: 0.8213\n",
      "[Fold 1 | Epoch 34] Train Loss: 0.4162 | Train Acc: 0.8385 | Val Loss: 0.4750 | Val Acc: 0.8193\n",
      "[Fold 1 | Epoch 35] Train Loss: 0.4137 | Train Acc: 0.8384 | Val Loss: 0.4594 | Val Acc: 0.8190\n",
      "[Fold 1 | Epoch 36] Train Loss: 0.4118 | Train Acc: 0.8402 | Val Loss: 0.4607 | Val Acc: 0.8208\n",
      "[Fold 1 | Epoch 37] Train Loss: 0.4110 | Train Acc: 0.8405 | Val Loss: 0.4576 | Val Acc: 0.8215\n",
      "[Fold 1 | Epoch 38] Train Loss: 0.4086 | Train Acc: 0.8410 | Val Loss: 0.4535 | Val Acc: 0.8252\n",
      "[Fold 1 | Epoch 39] Train Loss: 0.4063 | Train Acc: 0.8415 | Val Loss: 0.4541 | Val Acc: 0.8243\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 39.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 29\n",
      "ACC: 0.8281 | MF1: 0.7807 | G-Mean: 0.8514\n",
      "[Class 0] Prec: 0.9197 | Rec: 0.9333 | F1: 0.9264 | GM: 0.9472\n",
      "[Class 1] Prec: 0.5340 | Rec: 0.5027 | F1: 0.5179 | GM: 0.6874\n",
      "[Class 2] Prec: 0.8365 | Rec: 0.9000 | F1: 0.8671 | GM: 0.9038\n",
      "[Class 3] Prec: 0.8983 | Rec: 0.8096 | F1: 0.8517 | GM: 0.8964\n",
      "[Class 4] Prec: 0.7906 | Rec: 0.6962 | F1: 0.7404 | GM: 0.8221\n",
      "\n",
      "===== Fold 2 =====\n",
      "[INFO] Total parameters BEFORE pruning: 84,812\n",
      "[INFO] Non-zero parameters AFTER pruning: 42,608\n",
      "[INFO] Pruned parameters: 42,204 (49.76%)\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.6220 | Train Acc: 0.7541 | Val Loss: 0.6174 | Val Acc: 0.7569\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5414 | Train Acc: 0.7889 | Val Loss: 0.5612 | Val Acc: 0.7839\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5219 | Train Acc: 0.7960 | Val Loss: 0.5016 | Val Acc: 0.8070\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.5075 | Train Acc: 0.8031 | Val Loss: 0.4896 | Val Acc: 0.8090\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.5006 | Train Acc: 0.8044 | Val Loss: 0.5179 | Val Acc: 0.7984\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4920 | Train Acc: 0.8081 | Val Loss: 0.4742 | Val Acc: 0.8183\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4867 | Train Acc: 0.8121 | Val Loss: 0.5063 | Val Acc: 0.8006\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4808 | Train Acc: 0.8133 | Val Loss: 0.4747 | Val Acc: 0.8186\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4774 | Train Acc: 0.8146 | Val Loss: 0.5829 | Val Acc: 0.7614\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4728 | Train Acc: 0.8160 | Val Loss: 0.4807 | Val Acc: 0.8170\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4684 | Train Acc: 0.8185 | Val Loss: 0.4901 | Val Acc: 0.8135\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4661 | Train Acc: 0.8197 | Val Loss: 0.4720 | Val Acc: 0.8171\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4619 | Train Acc: 0.8214 | Val Loss: 0.4743 | Val Acc: 0.8163\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4600 | Train Acc: 0.8215 | Val Loss: 0.4725 | Val Acc: 0.8113\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4563 | Train Acc: 0.8225 | Val Loss: 0.4642 | Val Acc: 0.8182\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4529 | Train Acc: 0.8238 | Val Loss: 0.4884 | Val Acc: 0.8104\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4518 | Train Acc: 0.8250 | Val Loss: 0.4624 | Val Acc: 0.8206\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4465 | Train Acc: 0.8273 | Val Loss: 0.4558 | Val Acc: 0.8232\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4453 | Train Acc: 0.8275 | Val Loss: 0.4656 | Val Acc: 0.8200\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4425 | Train Acc: 0.8278 | Val Loss: 0.5215 | Val Acc: 0.7985\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4427 | Train Acc: 0.8274 | Val Loss: 0.4613 | Val Acc: 0.8202\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4377 | Train Acc: 0.8288 | Val Loss: 0.4968 | Val Acc: 0.8060\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4367 | Train Acc: 0.8306 | Val Loss: 0.4663 | Val Acc: 0.8207\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4333 | Train Acc: 0.8312 | Val Loss: 0.4825 | Val Acc: 0.8102\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4329 | Train Acc: 0.8315 | Val Loss: 0.4902 | Val Acc: 0.8064\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4300 | Train Acc: 0.8336 | Val Loss: 0.4657 | Val Acc: 0.8182\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4275 | Train Acc: 0.8347 | Val Loss: 0.4557 | Val Acc: 0.8203\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.4252 | Train Acc: 0.8350 | Val Loss: 0.4609 | Val Acc: 0.8203\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 28.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 18\n",
      "ACC: 0.8232 | MF1: 0.7743 | G-Mean: 0.8464\n",
      "[Class 0] Prec: 0.9435 | Rec: 0.9049 | F1: 0.9238 | GM: 0.9392\n",
      "[Class 1] Prec: 0.5347 | Rec: 0.4807 | F1: 0.5062 | GM: 0.6725\n",
      "[Class 2] Prec: 0.8304 | Rec: 0.9057 | F1: 0.8664 | GM: 0.9041\n",
      "[Class 3] Prec: 0.9317 | Rec: 0.7225 | F1: 0.8139 | GM: 0.8482\n",
      "[Class 4] Prec: 0.7341 | Rec: 0.7900 | F1: 0.7610 | GM: 0.8679\n",
      "\n",
      "===== Fold 3 =====\n",
      "[INFO] Total parameters BEFORE pruning: 84,812\n",
      "[INFO] Non-zero parameters AFTER pruning: 42,608\n",
      "[INFO] Pruned parameters: 42,204 (49.76%)\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.6086 | Train Acc: 0.7581 | Val Loss: 0.5420 | Val Acc: 0.7906\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5430 | Train Acc: 0.7880 | Val Loss: 0.5520 | Val Acc: 0.7809\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5251 | Train Acc: 0.7940 | Val Loss: 0.5061 | Val Acc: 0.8032\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.5141 | Train Acc: 0.7996 | Val Loss: 0.4838 | Val Acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3 | Epoch 5] Train Loss: 0.5075 | Train Acc: 0.8020 | Val Loss: 0.5398 | Val Acc: 0.7815\n",
      "[Fold 3 | Epoch 6] Train Loss: 0.5011 | Train Acc: 0.8040 | Val Loss: 0.4940 | Val Acc: 0.8084\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4924 | Train Acc: 0.8095 | Val Loss: 0.4782 | Val Acc: 0.8142\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4876 | Train Acc: 0.8108 | Val Loss: 0.4792 | Val Acc: 0.8145\n",
      "[Fold 3 | Epoch 9] Train Loss: 0.4828 | Train Acc: 0.8116 | Val Loss: 0.4848 | Val Acc: 0.8099\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4793 | Train Acc: 0.8135 | Val Loss: 0.4873 | Val Acc: 0.8114\n",
      "[Fold 3 | Epoch 11] Train Loss: 0.4723 | Train Acc: 0.8170 | Val Loss: 0.4723 | Val Acc: 0.8145\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4693 | Train Acc: 0.8175 | Val Loss: 0.4531 | Val Acc: 0.8237\n",
      "[Fold 3 | Epoch 13] Train Loss: 0.4667 | Train Acc: 0.8176 | Val Loss: 0.4518 | Val Acc: 0.8252\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4635 | Train Acc: 0.8208 | Val Loss: 0.4635 | Val Acc: 0.8189\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4603 | Train Acc: 0.8217 | Val Loss: 0.4489 | Val Acc: 0.8259\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4565 | Train Acc: 0.8233 | Val Loss: 0.4434 | Val Acc: 0.8313\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4539 | Train Acc: 0.8239 | Val Loss: 0.4401 | Val Acc: 0.8325\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4532 | Train Acc: 0.8234 | Val Loss: 0.4527 | Val Acc: 0.8253\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4506 | Train Acc: 0.8242 | Val Loss: 0.4679 | Val Acc: 0.8190\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4471 | Train Acc: 0.8259 | Val Loss: 0.4487 | Val Acc: 0.8272\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4449 | Train Acc: 0.8272 | Val Loss: 0.4552 | Val Acc: 0.8248\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4433 | Train Acc: 0.8273 | Val Loss: 0.4530 | Val Acc: 0.8248\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4399 | Train Acc: 0.8295 | Val Loss: 0.4434 | Val Acc: 0.8269\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4379 | Train Acc: 0.8300 | Val Loss: 0.4489 | Val Acc: 0.8257\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4369 | Train Acc: 0.8305 | Val Loss: 0.4472 | Val Acc: 0.8290\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4323 | Train Acc: 0.8318 | Val Loss: 0.4505 | Val Acc: 0.8263\n",
      "[Fold 3 | Epoch 27] Train Loss: 0.4323 | Train Acc: 0.8326 | Val Loss: 0.4418 | Val Acc: 0.8301\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 27.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 17\n",
      "ACC: 0.8325 | MF1: 0.7846 | G-Mean: 0.8550\n",
      "[Class 0] Prec: 0.9266 | Rec: 0.9303 | F1: 0.9285 | GM: 0.9477\n",
      "[Class 1] Prec: 0.5590 | Rec: 0.4660 | F1: 0.5083 | GM: 0.6650\n",
      "[Class 2] Prec: 0.8353 | Rec: 0.9101 | F1: 0.8711 | GM: 0.9074\n",
      "[Class 3] Prec: 0.8758 | Rec: 0.8417 | F1: 0.8584 | GM: 0.9131\n",
      "[Class 4] Prec: 0.7834 | Rec: 0.7321 | F1: 0.7568 | GM: 0.8416\n",
      "\n",
      "===== Fold 4 =====\n",
      "[INFO] Total parameters BEFORE pruning: 84,812\n",
      "[INFO] Non-zero parameters AFTER pruning: 42,608\n",
      "[INFO] Pruned parameters: 42,204 (49.76%)\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.6090 | Train Acc: 0.7603 | Val Loss: 0.5588 | Val Acc: 0.7777\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5422 | Train Acc: 0.7877 | Val Loss: 0.5468 | Val Acc: 0.7848\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5250 | Train Acc: 0.7964 | Val Loss: 0.5049 | Val Acc: 0.8004\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5131 | Train Acc: 0.8007 | Val Loss: 0.4820 | Val Acc: 0.8152\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.5024 | Train Acc: 0.8053 | Val Loss: 0.4890 | Val Acc: 0.8106\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4955 | Train Acc: 0.8068 | Val Loss: 0.4684 | Val Acc: 0.8167\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4893 | Train Acc: 0.8096 | Val Loss: 0.5071 | Val Acc: 0.7997\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4840 | Train Acc: 0.8109 | Val Loss: 0.5004 | Val Acc: 0.8051\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4792 | Train Acc: 0.8150 | Val Loss: 0.4620 | Val Acc: 0.8186\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4740 | Train Acc: 0.8160 | Val Loss: 0.4670 | Val Acc: 0.8161\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4707 | Train Acc: 0.8170 | Val Loss: 0.4899 | Val Acc: 0.8026\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4659 | Train Acc: 0.8185 | Val Loss: 0.4685 | Val Acc: 0.8147\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4623 | Train Acc: 0.8211 | Val Loss: 0.4526 | Val Acc: 0.8220\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4605 | Train Acc: 0.8217 | Val Loss: 0.4470 | Val Acc: 0.8256\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4562 | Train Acc: 0.8237 | Val Loss: 0.4504 | Val Acc: 0.8220\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4526 | Train Acc: 0.8246 | Val Loss: 0.4421 | Val Acc: 0.8278\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4492 | Train Acc: 0.8259 | Val Loss: 0.4621 | Val Acc: 0.8235\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4485 | Train Acc: 0.8273 | Val Loss: 0.4417 | Val Acc: 0.8283\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4451 | Train Acc: 0.8266 | Val Loss: 0.4507 | Val Acc: 0.8229\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4419 | Train Acc: 0.8281 | Val Loss: 0.4453 | Val Acc: 0.8237\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4401 | Train Acc: 0.8303 | Val Loss: 0.5223 | Val Acc: 0.7937\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4377 | Train Acc: 0.8310 | Val Loss: 0.4402 | Val Acc: 0.8272\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4367 | Train Acc: 0.8307 | Val Loss: 0.4359 | Val Acc: 0.8301\n",
      "[Fold 4 | Epoch 24] Train Loss: 0.4345 | Train Acc: 0.8318 | Val Loss: 0.4646 | Val Acc: 0.8145\n",
      "[Fold 4 | Epoch 25] Train Loss: 0.4331 | Train Acc: 0.8330 | Val Loss: 0.4326 | Val Acc: 0.8301\n",
      "[Fold 4 | Epoch 26] Train Loss: 0.4305 | Train Acc: 0.8338 | Val Loss: 0.4429 | Val Acc: 0.8281\n",
      "[Fold 4 | Epoch 27] Train Loss: 0.4285 | Train Acc: 0.8353 | Val Loss: 0.4462 | Val Acc: 0.8263\n",
      "[Fold 4 | Epoch 28] Train Loss: 0.4267 | Train Acc: 0.8350 | Val Loss: 0.4798 | Val Acc: 0.8113\n",
      "[Fold 4 | Epoch 29] Train Loss: 0.4260 | Train Acc: 0.8352 | Val Loss: 0.4418 | Val Acc: 0.8260\n",
      "[Fold 4 | Epoch 30] Train Loss: 0.4237 | Train Acc: 0.8363 | Val Loss: 0.4302 | Val Acc: 0.8313\n",
      "[Fold 4 | Epoch 31] Train Loss: 0.4202 | Train Acc: 0.8373 | Val Loss: 0.4366 | Val Acc: 0.8312\n",
      "[Fold 4 | Epoch 32] Train Loss: 0.4203 | Train Acc: 0.8378 | Val Loss: 0.4425 | Val Acc: 0.8255\n",
      "[Fold 4 | Epoch 33] Train Loss: 0.4170 | Train Acc: 0.8368 | Val Loss: 0.4574 | Val Acc: 0.8259\n",
      "[Fold 4 | Epoch 34] Train Loss: 0.4156 | Train Acc: 0.8394 | Val Loss: 0.4421 | Val Acc: 0.8264\n",
      "[Fold 4 | Epoch 35] Train Loss: 0.4135 | Train Acc: 0.8406 | Val Loss: 0.4622 | Val Acc: 0.8214\n",
      "[Fold 4 | Epoch 36] Train Loss: 0.4124 | Train Acc: 0.8415 | Val Loss: 0.4625 | Val Acc: 0.8147\n",
      "[Fold 4 | Epoch 37] Train Loss: 0.4106 | Train Acc: 0.8402 | Val Loss: 0.4556 | Val Acc: 0.8186\n",
      "[Fold 4 | Epoch 38] Train Loss: 0.4086 | Train Acc: 0.8412 | Val Loss: 0.4354 | Val Acc: 0.8299\n",
      "[Fold 4 | Epoch 39] Train Loss: 0.4070 | Train Acc: 0.8420 | Val Loss: 0.4478 | Val Acc: 0.8276\n",
      "[Fold 4 | Epoch 40] Train Loss: 0.4067 | Train Acc: 0.8423 | Val Loss: 0.4510 | Val Acc: 0.8247\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 40.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 30\n",
      "ACC: 0.8313 | MF1: 0.7814 | G-Mean: 0.8512\n",
      "[Class 0] Prec: 0.9232 | Rec: 0.9300 | F1: 0.9266 | GM: 0.9468\n",
      "[Class 1] Prec: 0.5717 | Rec: 0.4323 | F1: 0.4923 | GM: 0.6424\n",
      "[Class 2] Prec: 0.8363 | Rec: 0.9085 | F1: 0.8709 | GM: 0.9076\n",
      "[Class 3] Prec: 0.9032 | Rec: 0.8028 | F1: 0.8501 | GM: 0.8929\n",
      "[Class 4] Prec: 0.7511 | Rec: 0.7841 | F1: 0.7672 | GM: 0.8663\n",
      "\n",
      "===== Fold 5 =====\n",
      "[INFO] Total parameters BEFORE pruning: 84,812\n",
      "[INFO] Non-zero parameters AFTER pruning: 42,608\n",
      "[INFO] Pruned parameters: 42,204 (49.76%)\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.6123 | Train Acc: 0.7583 | Val Loss: 0.5334 | Val Acc: 0.7935\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5380 | Train Acc: 0.7903 | Val Loss: 0.5015 | Val Acc: 0.8064\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5218 | Train Acc: 0.7966 | Val Loss: 0.5115 | Val Acc: 0.8021\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.5130 | Train Acc: 0.8004 | Val Loss: 0.4895 | Val Acc: 0.8135\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.5018 | Train Acc: 0.8052 | Val Loss: 0.4738 | Val Acc: 0.8175\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.4943 | Train Acc: 0.8072 | Val Loss: 0.4707 | Val Acc: 0.8161\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4904 | Train Acc: 0.8097 | Val Loss: 0.5287 | Val Acc: 0.7970\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4813 | Train Acc: 0.8126 | Val Loss: 0.4625 | Val Acc: 0.8196\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4785 | Train Acc: 0.8137 | Val Loss: 0.4682 | Val Acc: 0.8183\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4756 | Train Acc: 0.8154 | Val Loss: 0.5134 | Val Acc: 0.7999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5 | Epoch 11] Train Loss: 0.4717 | Train Acc: 0.8173 | Val Loss: 0.4544 | Val Acc: 0.8223\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4678 | Train Acc: 0.8183 | Val Loss: 0.4626 | Val Acc: 0.8196\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4642 | Train Acc: 0.8204 | Val Loss: 0.4628 | Val Acc: 0.8218\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4604 | Train Acc: 0.8218 | Val Loss: 0.4841 | Val Acc: 0.8151\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4584 | Train Acc: 0.8227 | Val Loss: 0.4484 | Val Acc: 0.8273\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4539 | Train Acc: 0.8236 | Val Loss: 0.4733 | Val Acc: 0.8124\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4512 | Train Acc: 0.8246 | Val Loss: 0.4714 | Val Acc: 0.8128\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4477 | Train Acc: 0.8267 | Val Loss: 0.4744 | Val Acc: 0.8125\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.4465 | Train Acc: 0.8276 | Val Loss: 0.4618 | Val Acc: 0.8211\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.4424 | Train Acc: 0.8283 | Val Loss: 0.4533 | Val Acc: 0.8251\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.4409 | Train Acc: 0.8286 | Val Loss: 0.4478 | Val Acc: 0.8283\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.4364 | Train Acc: 0.8305 | Val Loss: 0.4482 | Val Acc: 0.8242\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.4362 | Train Acc: 0.8309 | Val Loss: 0.4384 | Val Acc: 0.8312\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.4343 | Train Acc: 0.8322 | Val Loss: 0.4484 | Val Acc: 0.8257\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.4311 | Train Acc: 0.8318 | Val Loss: 0.4549 | Val Acc: 0.8244\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.4296 | Train Acc: 0.8332 | Val Loss: 0.4458 | Val Acc: 0.8304\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.4266 | Train Acc: 0.8346 | Val Loss: 0.4535 | Val Acc: 0.8245\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.4237 | Train Acc: 0.8348 | Val Loss: 0.4441 | Val Acc: 0.8282\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.4229 | Train Acc: 0.8352 | Val Loss: 0.4630 | Val Acc: 0.8206\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.4217 | Train Acc: 0.8361 | Val Loss: 0.4481 | Val Acc: 0.8264\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.4205 | Train Acc: 0.8370 | Val Loss: 0.4592 | Val Acc: 0.8259\n",
      "[Fold 5 | Epoch 32] Train Loss: 0.4171 | Train Acc: 0.8389 | Val Loss: 0.4549 | Val Acc: 0.8258\n",
      "[Fold 5 | Epoch 33] Train Loss: 0.4155 | Train Acc: 0.8390 | Val Loss: 0.4344 | Val Acc: 0.8312\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 33.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 23\n",
      "ACC: 0.8312 | MF1: 0.7858 | G-Mean: 0.8571\n",
      "[Class 0] Prec: 0.9295 | Rec: 0.9277 | F1: 0.9286 | GM: 0.9471\n",
      "[Class 1] Prec: 0.5548 | Rec: 0.4980 | F1: 0.5249 | GM: 0.6859\n",
      "[Class 2] Prec: 0.8322 | Rec: 0.9021 | F1: 0.8657 | GM: 0.9026\n",
      "[Class 3] Prec: 0.8587 | Rec: 0.8559 | F1: 0.8573 | GM: 0.9199\n",
      "[Class 4] Prec: 0.8014 | Rec: 0.7089 | F1: 0.7523 | GM: 0.8301\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8293 | MF1: 0.7816 | G-Mean: 0.8525\n",
      "[Class 0] Prec: 0.9283 | Rec: 0.9253 | F1: 0.9268 | GM: 0.9457\n",
      "[Class 1] Prec: 0.5499 | Rec: 0.4758 | F1: 0.5102 | GM: 0.6708\n",
      "[Class 2] Prec: 0.8341 | Rec: 0.9053 | F1: 0.8682 | GM: 0.9051\n",
      "[Class 3] Prec: 0.8915 | Rec: 0.8066 | F1: 0.8469 | GM: 0.8946\n",
      "[Class 4] Prec: 0.7698 | Rec: 0.7426 | F1: 0.7560 | GM: 0.8462\n",
      "Confusion Matrix:\n",
      "[[49415  2773   534    53   632]\n",
      " [ 2816  9728  5094     8  2800]\n",
      " [  303  2318 52349  1139  1719]\n",
      " [   20     2  2360  9992    14]\n",
      " [  677  2869  2422    16 17268]]\n"
     ]
    }
   ],
   "source": [
    "#p9_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    # === Build and Prune Model ===\n",
    "    model_unpruned = FinalNetwork(C=9, num_classes=num_classes, layers=9, genotype=searched_genotype).to(device)\n",
    "    total_params_before = sum(p.numel() for p in model_unpruned.parameters())\n",
    "    print(f\"[INFO] Total parameters BEFORE pruning: {total_params_before:,}\")\n",
    "\n",
    "    model = prune_model_entropy(model_unpruned, prune_ratio=0.5)\n",
    "    nonzero_params_after = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    zero_params = total_params_before - nonzero_params_after\n",
    "    pruned_ratio = 100 * zero_params / total_params_before\n",
    "    print(f\"[INFO] Non-zero parameters AFTER pruning: {nonzero_params_after:,}\")\n",
    "    print(f\"[INFO] Pruned parameters: {zero_params:,} ({pruned_ratio:.2f}%)\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dataloader\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOPPING COUNTER\n",
    "\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(\n",
    "                np.array(val_true), np.array(val_pred), num_classes\n",
    "            )\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    # Gộp toàn bộ val_true và val_pred để đánh giá toàn bộ tập sau K-Fold\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd53db05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.5998 | Train Acc: 0.7633 | Val Loss: 0.5385 | Val Acc: 0.7907\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5339 | Train Acc: 0.7907 | Val Loss: 0.5080 | Val Acc: 0.8035\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5132 | Train Acc: 0.8004 | Val Loss: 0.4999 | Val Acc: 0.8055\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.5005 | Train Acc: 0.8058 | Val Loss: 0.4916 | Val Acc: 0.8084\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.4942 | Train Acc: 0.8069 | Val Loss: 0.4859 | Val Acc: 0.8122\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4855 | Train Acc: 0.8105 | Val Loss: 0.4795 | Val Acc: 0.8137\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4807 | Train Acc: 0.8128 | Val Loss: 0.4551 | Val Acc: 0.8248\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4739 | Train Acc: 0.8159 | Val Loss: 0.4588 | Val Acc: 0.8234\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4680 | Train Acc: 0.8183 | Val Loss: 0.4664 | Val Acc: 0.8188\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4635 | Train Acc: 0.8190 | Val Loss: 0.4700 | Val Acc: 0.8179\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4595 | Train Acc: 0.8214 | Val Loss: 0.4530 | Val Acc: 0.8204\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4561 | Train Acc: 0.8227 | Val Loss: 0.4552 | Val Acc: 0.8248\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4512 | Train Acc: 0.8249 | Val Loss: 0.4489 | Val Acc: 0.8284\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4488 | Train Acc: 0.8250 | Val Loss: 0.4506 | Val Acc: 0.8230\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4439 | Train Acc: 0.8286 | Val Loss: 0.4793 | Val Acc: 0.8169\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4401 | Train Acc: 0.8300 | Val Loss: 0.4427 | Val Acc: 0.8281\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4360 | Train Acc: 0.8301 | Val Loss: 0.4427 | Val Acc: 0.8319\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4338 | Train Acc: 0.8318 | Val Loss: 0.4425 | Val Acc: 0.8291\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4299 | Train Acc: 0.8332 | Val Loss: 0.4677 | Val Acc: 0.8212\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4282 | Train Acc: 0.8337 | Val Loss: 0.4408 | Val Acc: 0.8278\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4231 | Train Acc: 0.8354 | Val Loss: 0.4395 | Val Acc: 0.8313\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4220 | Train Acc: 0.8358 | Val Loss: 0.4450 | Val Acc: 0.8295\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4182 | Train Acc: 0.8381 | Val Loss: 0.4493 | Val Acc: 0.8264\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4141 | Train Acc: 0.8400 | Val Loss: 0.4813 | Val Acc: 0.8144\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4111 | Train Acc: 0.8405 | Val Loss: 0.4433 | Val Acc: 0.8290\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4089 | Train Acc: 0.8421 | Val Loss: 0.4388 | Val Acc: 0.8343\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4056 | Train Acc: 0.8430 | Val Loss: 0.4445 | Val Acc: 0.8299\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.4022 | Train Acc: 0.8434 | Val Loss: 0.4440 | Val Acc: 0.8298\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.3984 | Train Acc: 0.8455 | Val Loss: 0.4588 | Val Acc: 0.8246\n",
      "[Fold 1 | Epoch 30] Train Loss: 0.3966 | Train Acc: 0.8466 | Val Loss: 0.4496 | Val Acc: 0.8293\n",
      "[Fold 1 | Epoch 31] Train Loss: 0.3927 | Train Acc: 0.8482 | Val Loss: 0.4387 | Val Acc: 0.8326\n",
      "[Fold 1 | Epoch 32] Train Loss: 0.3903 | Train Acc: 0.8485 | Val Loss: 0.4641 | Val Acc: 0.8272\n",
      "[Fold 1 | Epoch 33] Train Loss: 0.3861 | Train Acc: 0.8512 | Val Loss: 0.4479 | Val Acc: 0.8265\n",
      "[Fold 1 | Epoch 34] Train Loss: 0.3829 | Train Acc: 0.8515 | Val Loss: 0.4753 | Val Acc: 0.8200\n",
      "[Fold 1 | Epoch 35] Train Loss: 0.3819 | Train Acc: 0.8525 | Val Loss: 0.4438 | Val Acc: 0.8309\n",
      "[Fold 1 | Epoch 36] Train Loss: 0.3775 | Train Acc: 0.8542 | Val Loss: 0.4515 | Val Acc: 0.8297\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 36.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 26\n",
      "ACC: 0.8343 | MF1: 0.7862 | G-Mean: 0.8551\n",
      "[Class 0] Prec: 0.9323 | Rec: 0.9308 | F1: 0.9316 | GM: 0.9492\n",
      "[Class 1] Prec: 0.5577 | Rec: 0.4852 | F1: 0.5189 | GM: 0.6778\n",
      "[Class 2] Prec: 0.8424 | Rec: 0.9047 | F1: 0.8725 | GM: 0.9076\n",
      "[Class 3] Prec: 0.9151 | Rec: 0.7807 | F1: 0.8426 | GM: 0.8811\n",
      "[Class 4] Prec: 0.7620 | Rec: 0.7690 | F1: 0.7655 | GM: 0.8596\n",
      "\n",
      "===== Fold 2 =====\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.5980 | Train Acc: 0.7656 | Val Loss: 0.5846 | Val Acc: 0.7778\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5321 | Train Acc: 0.7934 | Val Loss: 0.4950 | Val Acc: 0.8112\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5144 | Train Acc: 0.8008 | Val Loss: 0.5009 | Val Acc: 0.8112\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.5022 | Train Acc: 0.8048 | Val Loss: 0.4790 | Val Acc: 0.8111\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.4921 | Train Acc: 0.8085 | Val Loss: 0.4758 | Val Acc: 0.8161\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4839 | Train Acc: 0.8119 | Val Loss: 0.4669 | Val Acc: 0.8178\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4787 | Train Acc: 0.8134 | Val Loss: 0.4683 | Val Acc: 0.8188\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4732 | Train Acc: 0.8155 | Val Loss: 0.4758 | Val Acc: 0.8143\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4648 | Train Acc: 0.8196 | Val Loss: 0.4535 | Val Acc: 0.8224\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4604 | Train Acc: 0.8206 | Val Loss: 0.4409 | Val Acc: 0.8298\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4571 | Train Acc: 0.8235 | Val Loss: 0.4654 | Val Acc: 0.8159\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4530 | Train Acc: 0.8242 | Val Loss: 0.4537 | Val Acc: 0.8252\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4482 | Train Acc: 0.8255 | Val Loss: 0.4601 | Val Acc: 0.8208\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4456 | Train Acc: 0.8279 | Val Loss: 0.4723 | Val Acc: 0.8184\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4410 | Train Acc: 0.8293 | Val Loss: 0.4610 | Val Acc: 0.8214\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4380 | Train Acc: 0.8305 | Val Loss: 0.4417 | Val Acc: 0.8298\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4340 | Train Acc: 0.8315 | Val Loss: 0.4733 | Val Acc: 0.8057\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4303 | Train Acc: 0.8331 | Val Loss: 0.4419 | Val Acc: 0.8289\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4295 | Train Acc: 0.8334 | Val Loss: 0.4462 | Val Acc: 0.8266\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4243 | Train Acc: 0.8354 | Val Loss: 0.4406 | Val Acc: 0.8324\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4214 | Train Acc: 0.8372 | Val Loss: 0.4551 | Val Acc: 0.8274\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4188 | Train Acc: 0.8378 | Val Loss: 0.4366 | Val Acc: 0.8290\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4149 | Train Acc: 0.8396 | Val Loss: 0.4400 | Val Acc: 0.8308\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4130 | Train Acc: 0.8403 | Val Loss: 0.4611 | Val Acc: 0.8218\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4084 | Train Acc: 0.8411 | Val Loss: 0.4376 | Val Acc: 0.8310\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4056 | Train Acc: 0.8428 | Val Loss: 0.4534 | Val Acc: 0.8245\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4013 | Train Acc: 0.8449 | Val Loss: 0.4511 | Val Acc: 0.8221\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.3992 | Train Acc: 0.8457 | Val Loss: 0.4509 | Val Acc: 0.8297\n",
      "[Fold 2 | Epoch 29] Train Loss: 0.3952 | Train Acc: 0.8463 | Val Loss: 0.4891 | Val Acc: 0.8134\n",
      "[Fold 2 | Epoch 30] Train Loss: 0.3938 | Train Acc: 0.8461 | Val Loss: 0.4520 | Val Acc: 0.8235\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 30.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 20\n",
      "ACC: 0.8324 | MF1: 0.7849 | G-Mean: 0.8620\n",
      "[Class 0] Prec: 0.9239 | Rec: 0.9289 | F1: 0.9264 | GM: 0.9462\n",
      "[Class 1] Prec: 0.5864 | Rec: 0.4386 | F1: 0.5019 | GM: 0.6478\n",
      "[Class 2] Prec: 0.8563 | Rec: 0.8791 | F1: 0.8675 | GM: 0.9004\n",
      "[Class 3] Prec: 0.8277 | Rec: 0.8912 | F1: 0.8583 | GM: 0.9371\n",
      "[Class 4] Prec: 0.7343 | Rec: 0.8100 | F1: 0.7703 | GM: 0.8786\n",
      "\n",
      "===== Fold 3 =====\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.5977 | Train Acc: 0.7651 | Val Loss: 0.5155 | Val Acc: 0.8033\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5338 | Train Acc: 0.7925 | Val Loss: 0.5260 | Val Acc: 0.7928\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5167 | Train Acc: 0.7981 | Val Loss: 0.4965 | Val Acc: 0.8053\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.5036 | Train Acc: 0.8043 | Val Loss: 0.4960 | Val Acc: 0.8095\n",
      "[Fold 3 | Epoch 5] Train Loss: 0.4946 | Train Acc: 0.8070 | Val Loss: 0.4742 | Val Acc: 0.8148\n",
      "[Fold 3 | Epoch 6] Train Loss: 0.4894 | Train Acc: 0.8091 | Val Loss: 0.4885 | Val Acc: 0.8152\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4832 | Train Acc: 0.8129 | Val Loss: 0.5038 | Val Acc: 0.8036\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4765 | Train Acc: 0.8141 | Val Loss: 0.4661 | Val Acc: 0.8226\n",
      "[Fold 3 | Epoch 9] Train Loss: 0.4707 | Train Acc: 0.8184 | Val Loss: 0.4565 | Val Acc: 0.8264\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4684 | Train Acc: 0.8185 | Val Loss: 0.4968 | Val Acc: 0.8116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3 | Epoch 11] Train Loss: 0.4641 | Train Acc: 0.8205 | Val Loss: 0.4538 | Val Acc: 0.8250\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4580 | Train Acc: 0.8223 | Val Loss: 0.4577 | Val Acc: 0.8222\n",
      "[Fold 3 | Epoch 13] Train Loss: 0.4533 | Train Acc: 0.8237 | Val Loss: 0.4757 | Val Acc: 0.8148\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4514 | Train Acc: 0.8258 | Val Loss: 0.4663 | Val Acc: 0.8221\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4460 | Train Acc: 0.8260 | Val Loss: 0.4442 | Val Acc: 0.8273\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4431 | Train Acc: 0.8281 | Val Loss: 0.4534 | Val Acc: 0.8294\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4398 | Train Acc: 0.8300 | Val Loss: 0.4538 | Val Acc: 0.8238\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4368 | Train Acc: 0.8304 | Val Loss: 0.4570 | Val Acc: 0.8241\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4330 | Train Acc: 0.8327 | Val Loss: 0.4442 | Val Acc: 0.8286\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4298 | Train Acc: 0.8332 | Val Loss: 0.4398 | Val Acc: 0.8309\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4264 | Train Acc: 0.8342 | Val Loss: 0.4409 | Val Acc: 0.8318\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4233 | Train Acc: 0.8356 | Val Loss: 0.4364 | Val Acc: 0.8307\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4214 | Train Acc: 0.8372 | Val Loss: 0.4486 | Val Acc: 0.8272\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4179 | Train Acc: 0.8380 | Val Loss: 0.4428 | Val Acc: 0.8286\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4148 | Train Acc: 0.8393 | Val Loss: 0.4485 | Val Acc: 0.8259\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4112 | Train Acc: 0.8407 | Val Loss: 0.4747 | Val Acc: 0.8186\n",
      "[Fold 3 | Epoch 27] Train Loss: 0.4083 | Train Acc: 0.8413 | Val Loss: 0.4819 | Val Acc: 0.8126\n",
      "[Fold 3 | Epoch 28] Train Loss: 0.4046 | Train Acc: 0.8436 | Val Loss: 0.4522 | Val Acc: 0.8277\n",
      "[Fold 3 | Epoch 29] Train Loss: 0.4006 | Train Acc: 0.8440 | Val Loss: 0.4495 | Val Acc: 0.8313\n",
      "[Fold 3 | Epoch 30] Train Loss: 0.3992 | Train Acc: 0.8449 | Val Loss: 0.4503 | Val Acc: 0.8262\n",
      "[Fold 3 | Epoch 31] Train Loss: 0.3956 | Train Acc: 0.8461 | Val Loss: 0.4484 | Val Acc: 0.8286\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 31.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 21\n",
      "ACC: 0.8318 | MF1: 0.7755 | G-Mean: 0.8429\n",
      "[Class 0] Prec: 0.9259 | Rec: 0.9332 | F1: 0.9296 | GM: 0.9492\n",
      "[Class 1] Prec: 0.5875 | Rec: 0.4065 | F1: 0.4805 | GM: 0.6250\n",
      "[Class 2] Prec: 0.8227 | Rec: 0.9228 | F1: 0.8699 | GM: 0.9088\n",
      "[Class 3] Prec: 0.9283 | Rec: 0.7481 | F1: 0.8285 | GM: 0.8629\n",
      "[Class 4] Prec: 0.7510 | Rec: 0.7884 | F1: 0.7692 | GM: 0.8685\n",
      "\n",
      "===== Fold 4 =====\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.5976 | Train Acc: 0.7639 | Val Loss: 0.6414 | Val Acc: 0.7519\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5321 | Train Acc: 0.7927 | Val Loss: 0.5230 | Val Acc: 0.7957\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5173 | Train Acc: 0.7987 | Val Loss: 0.4869 | Val Acc: 0.8057\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5048 | Train Acc: 0.8036 | Val Loss: 0.5157 | Val Acc: 0.7943\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.4953 | Train Acc: 0.8079 | Val Loss: 0.4645 | Val Acc: 0.8180\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4886 | Train Acc: 0.8107 | Val Loss: 0.4616 | Val Acc: 0.8177\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4802 | Train Acc: 0.8139 | Val Loss: 0.4616 | Val Acc: 0.8228\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4749 | Train Acc: 0.8164 | Val Loss: 0.4713 | Val Acc: 0.8166\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4689 | Train Acc: 0.8184 | Val Loss: 0.4743 | Val Acc: 0.8119\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4629 | Train Acc: 0.8202 | Val Loss: 0.4486 | Val Acc: 0.8264\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4600 | Train Acc: 0.8228 | Val Loss: 0.4456 | Val Acc: 0.8268\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4545 | Train Acc: 0.8234 | Val Loss: 0.4453 | Val Acc: 0.8271\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4510 | Train Acc: 0.8246 | Val Loss: 0.4430 | Val Acc: 0.8301\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4483 | Train Acc: 0.8259 | Val Loss: 0.4705 | Val Acc: 0.8180\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4424 | Train Acc: 0.8286 | Val Loss: 0.4733 | Val Acc: 0.8155\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4387 | Train Acc: 0.8301 | Val Loss: 0.4572 | Val Acc: 0.8223\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4349 | Train Acc: 0.8322 | Val Loss: 0.4543 | Val Acc: 0.8228\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4314 | Train Acc: 0.8339 | Val Loss: 0.4454 | Val Acc: 0.8289\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4294 | Train Acc: 0.8333 | Val Loss: 0.4620 | Val Acc: 0.8223\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4245 | Train Acc: 0.8354 | Val Loss: 0.4433 | Val Acc: 0.8291\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4216 | Train Acc: 0.8371 | Val Loss: 0.4535 | Val Acc: 0.8254\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4163 | Train Acc: 0.8386 | Val Loss: 0.4663 | Val Acc: 0.8186\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4144 | Train Acc: 0.8399 | Val Loss: 0.4641 | Val Acc: 0.8242\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 23.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 13\n",
      "ACC: 0.8301 | MF1: 0.7814 | G-Mean: 0.8568\n",
      "[Class 0] Prec: 0.9138 | Rec: 0.9357 | F1: 0.9247 | GM: 0.9470\n",
      "[Class 1] Prec: 0.5689 | Rec: 0.4437 | F1: 0.4986 | GM: 0.6502\n",
      "[Class 2] Prec: 0.8603 | Rec: 0.8811 | F1: 0.8706 | GM: 0.9025\n",
      "[Class 3] Prec: 0.8642 | Rec: 0.8450 | F1: 0.8545 | GM: 0.9143\n",
      "[Class 4] Prec: 0.7264 | Rec: 0.7941 | F1: 0.7587 | GM: 0.8697\n",
      "\n",
      "===== Fold 5 =====\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.6033 | Train Acc: 0.7615 | Val Loss: 0.5355 | Val Acc: 0.7939\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5334 | Train Acc: 0.7925 | Val Loss: 0.5308 | Val Acc: 0.7923\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5153 | Train Acc: 0.7996 | Val Loss: 0.5466 | Val Acc: 0.7929\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.5033 | Train Acc: 0.8043 | Val Loss: 0.5035 | Val Acc: 0.8040\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.4933 | Train Acc: 0.8074 | Val Loss: 0.5023 | Val Acc: 0.8102\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.4849 | Train Acc: 0.8113 | Val Loss: 0.4874 | Val Acc: 0.8124\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4784 | Train Acc: 0.8141 | Val Loss: 0.4839 | Val Acc: 0.8145\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4731 | Train Acc: 0.8171 | Val Loss: 0.5070 | Val Acc: 0.8027\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4677 | Train Acc: 0.8184 | Val Loss: 0.4762 | Val Acc: 0.8142\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4640 | Train Acc: 0.8193 | Val Loss: 0.4594 | Val Acc: 0.8240\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4570 | Train Acc: 0.8227 | Val Loss: 0.4888 | Val Acc: 0.8092\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4530 | Train Acc: 0.8240 | Val Loss: 0.4717 | Val Acc: 0.8170\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4502 | Train Acc: 0.8250 | Val Loss: 0.4722 | Val Acc: 0.8203\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4476 | Train Acc: 0.8264 | Val Loss: 0.4666 | Val Acc: 0.8210\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4417 | Train Acc: 0.8292 | Val Loss: 0.4691 | Val Acc: 0.8205\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4368 | Train Acc: 0.8307 | Val Loss: 0.4615 | Val Acc: 0.8264\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4335 | Train Acc: 0.8326 | Val Loss: 0.4799 | Val Acc: 0.8174\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4315 | Train Acc: 0.8321 | Val Loss: 0.4653 | Val Acc: 0.8246\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.4274 | Train Acc: 0.8340 | Val Loss: 0.4631 | Val Acc: 0.8216\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.4248 | Train Acc: 0.8354 | Val Loss: 0.4727 | Val Acc: 0.8145\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.4204 | Train Acc: 0.8371 | Val Loss: 0.4658 | Val Acc: 0.8187\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.4182 | Train Acc: 0.8379 | Val Loss: 0.4562 | Val Acc: 0.8240\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.4155 | Train Acc: 0.8391 | Val Loss: 0.4542 | Val Acc: 0.8249\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.4123 | Train Acc: 0.8409 | Val Loss: 0.4608 | Val Acc: 0.8217\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.4084 | Train Acc: 0.8412 | Val Loss: 0.4554 | Val Acc: 0.8220\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.4043 | Train Acc: 0.8426 | Val Loss: 0.4836 | Val Acc: 0.8217\n",
      "[Early Stopping] No improvement in 10 epochs. Stopping at epoch 26.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 16\n",
      "ACC: 0.8264 | MF1: 0.7823 | G-Mean: 0.8643\n",
      "[Class 0] Prec: 0.9491 | Rec: 0.9093 | F1: 0.9288 | GM: 0.9427\n",
      "[Class 1] Prec: 0.5591 | Rec: 0.4683 | F1: 0.5097 | GM: 0.6663\n",
      "[Class 2] Prec: 0.8634 | Rec: 0.8654 | F1: 0.8644 | GM: 0.8960\n",
      "[Class 3] Prec: 0.8287 | Rec: 0.8915 | F1: 0.8590 | GM: 0.9370\n",
      "[Class 4] Prec: 0.6884 | Rec: 0.8225 | F1: 0.7495 | GM: 0.8796\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8310 | MF1: 0.7823 | G-Mean: 0.8565\n",
      "[Class 0] Prec: 0.9287 | Rec: 0.9276 | F1: 0.9282 | GM: 0.9469\n",
      "[Class 1] Prec: 0.5709 | Rec: 0.4485 | F1: 0.5024 | GM: 0.6538\n",
      "[Class 2] Prec: 0.8484 | Rec: 0.8906 | F1: 0.8690 | GM: 0.9032\n",
      "[Class 3] Prec: 0.8676 | Rec: 0.8316 | F1: 0.8492 | GM: 0.9073\n",
      "[Class 4] Prec: 0.7312 | Rec: 0.7967 | F1: 0.7626 | GM: 0.8712\n",
      "Confusion Matrix:\n",
      "[[49541  2560   446    50   810]\n",
      " [ 2859  9171  4813    13  3590]\n",
      " [  319  2125 51504  1490  2390]\n",
      " [   29     0  2039 10302    18]\n",
      " [  595  2208  1906    19 18524]]\n"
     ]
    }
   ],
   "source": [
    "#11_1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from model_build import FinalNetwork\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "# Z-score normalization\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Evaluation Metric Function ====\n",
    "def evaluate_metrics(y_true, y_pred, num_classes):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    gmeans = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        tn = np.sum((y_pred != c) & (y_true != c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        gm = np.sqrt((tp / (tp + fn + 1e-6)) * (tn / (tn + fp + 1e-6)))\n",
    "        gmeans.append(gm)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, mf1, np.mean(gmeans), prec, rec, f1s, gmeans, cm\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=47)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = FinalNetwork(C=9, num_classes=num_classes, layers=11, genotype=searched_genotype).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOP counter\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(val_true), np.array(val_pred), num_classes)\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91c671f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[INFO] Total parameters BEFORE pruning: 117,212\n",
      "[INFO] Non-zero parameters AFTER pruning: 58,808\n",
      "[INFO] Pruned parameters: 58,404 (49.83%)\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.6085 | Train Acc: 0.7594 | Val Loss: 0.5731 | Val Acc: 0.7762\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5432 | Train Acc: 0.7874 | Val Loss: 0.5784 | Val Acc: 0.7668\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5252 | Train Acc: 0.7950 | Val Loss: 0.5975 | Val Acc: 0.7693\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.5143 | Train Acc: 0.8005 | Val Loss: 0.5166 | Val Acc: 0.7965\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.5050 | Train Acc: 0.8036 | Val Loss: 0.4844 | Val Acc: 0.8147\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4968 | Train Acc: 0.8058 | Val Loss: 0.4950 | Val Acc: 0.8128\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4907 | Train Acc: 0.8080 | Val Loss: 0.5055 | Val Acc: 0.7989\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4861 | Train Acc: 0.8114 | Val Loss: 0.4712 | Val Acc: 0.8169\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4798 | Train Acc: 0.8140 | Val Loss: 0.4649 | Val Acc: 0.8216\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4744 | Train Acc: 0.8153 | Val Loss: 0.5113 | Val Acc: 0.8007\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4693 | Train Acc: 0.8170 | Val Loss: 0.4586 | Val Acc: 0.8232\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4663 | Train Acc: 0.8185 | Val Loss: 0.4773 | Val Acc: 0.8133\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4620 | Train Acc: 0.8206 | Val Loss: 0.4691 | Val Acc: 0.8195\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4585 | Train Acc: 0.8216 | Val Loss: 0.4602 | Val Acc: 0.8233\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4538 | Train Acc: 0.8233 | Val Loss: 0.4606 | Val Acc: 0.8230\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4515 | Train Acc: 0.8251 | Val Loss: 0.4556 | Val Acc: 0.8231\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4480 | Train Acc: 0.8259 | Val Loss: 0.4817 | Val Acc: 0.8161\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4458 | Train Acc: 0.8275 | Val Loss: 0.4710 | Val Acc: 0.8167\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4432 | Train Acc: 0.8286 | Val Loss: 0.4541 | Val Acc: 0.8258\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4397 | Train Acc: 0.8292 | Val Loss: 0.4500 | Val Acc: 0.8270\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4384 | Train Acc: 0.8299 | Val Loss: 0.4775 | Val Acc: 0.8124\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4360 | Train Acc: 0.8304 | Val Loss: 0.4530 | Val Acc: 0.8249\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4323 | Train Acc: 0.8321 | Val Loss: 0.4756 | Val Acc: 0.8150\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4304 | Train Acc: 0.8327 | Val Loss: 0.4668 | Val Acc: 0.8182\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4289 | Train Acc: 0.8341 | Val Loss: 0.4496 | Val Acc: 0.8279\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4253 | Train Acc: 0.8340 | Val Loss: 0.4599 | Val Acc: 0.8255\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4230 | Train Acc: 0.8351 | Val Loss: 0.4633 | Val Acc: 0.8223\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.4216 | Train Acc: 0.8366 | Val Loss: 0.4503 | Val Acc: 0.8264\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.4194 | Train Acc: 0.8370 | Val Loss: 0.4580 | Val Acc: 0.8255\n",
      "[Fold 1 | Epoch 30] Train Loss: 0.4172 | Train Acc: 0.8381 | Val Loss: 0.4534 | Val Acc: 0.8251\n",
      "[Fold 1 | Epoch 31] Train Loss: 0.4146 | Train Acc: 0.8386 | Val Loss: 0.4520 | Val Acc: 0.8259\n",
      "[Fold 1 | Epoch 32] Train Loss: 0.4129 | Train Acc: 0.8393 | Val Loss: 0.4437 | Val Acc: 0.8304\n",
      "[Fold 1 | Epoch 33] Train Loss: 0.4116 | Train Acc: 0.8392 | Val Loss: 0.4624 | Val Acc: 0.8228\n",
      "[Fold 1 | Epoch 34] Train Loss: 0.4091 | Train Acc: 0.8417 | Val Loss: 0.4761 | Val Acc: 0.8190\n",
      "[Fold 1 | Epoch 35] Train Loss: 0.4068 | Train Acc: 0.8427 | Val Loss: 0.4659 | Val Acc: 0.8234\n",
      "[Fold 1 | Epoch 36] Train Loss: 0.4057 | Train Acc: 0.8429 | Val Loss: 0.4813 | Val Acc: 0.8181\n",
      "[Fold 1 | Epoch 37] Train Loss: 0.4034 | Train Acc: 0.8430 | Val Loss: 0.4624 | Val Acc: 0.8224\n",
      "[Fold 1 | Epoch 38] Train Loss: 0.4006 | Train Acc: 0.8437 | Val Loss: 0.4565 | Val Acc: 0.8247\n",
      "[Fold 1 | Epoch 39] Train Loss: 0.3982 | Train Acc: 0.8448 | Val Loss: 0.4873 | Val Acc: 0.8164\n",
      "[Fold 1 | Epoch 40] Train Loss: 0.3964 | Train Acc: 0.8453 | Val Loss: 0.4639 | Val Acc: 0.8226\n",
      "[Fold 1 | Epoch 41] Train Loss: 0.3950 | Train Acc: 0.8466 | Val Loss: 0.4677 | Val Acc: 0.8217\n",
      "[Fold 1 | Epoch 42] Train Loss: 0.3922 | Train Acc: 0.8476 | Val Loss: 0.4717 | Val Acc: 0.8153\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 42.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 32\n",
      "ACC: 0.8304 | MF1: 0.7795 | G-Mean: 0.8515\n",
      "[Class 0] Prec: 0.9147 | Rec: 0.9366 | F1: 0.9255 | GM: 0.9475\n",
      "[Class 1] Prec: 0.5656 | Rec: 0.4382 | F1: 0.4938 | GM: 0.6465\n",
      "[Class 2] Prec: 0.8478 | Rec: 0.8942 | F1: 0.8704 | GM: 0.9049\n",
      "[Class 3] Prec: 0.9015 | Rec: 0.8132 | F1: 0.8551 | GM: 0.8985\n",
      "[Class 4] Prec: 0.7322 | Rec: 0.7745 | F1: 0.7527 | GM: 0.8600\n",
      "\n",
      "===== Fold 2 =====\n",
      "[INFO] Total parameters BEFORE pruning: 117,212\n",
      "[INFO] Non-zero parameters AFTER pruning: 58,808\n",
      "[INFO] Pruned parameters: 58,404 (49.83%)\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.6120 | Train Acc: 0.7582 | Val Loss: 0.6339 | Val Acc: 0.7392\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5405 | Train Acc: 0.7884 | Val Loss: 0.4985 | Val Acc: 0.8076\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5237 | Train Acc: 0.7969 | Val Loss: 0.5194 | Val Acc: 0.7963\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.5129 | Train Acc: 0.8001 | Val Loss: 0.4930 | Val Acc: 0.8112\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.5007 | Train Acc: 0.8054 | Val Loss: 0.5109 | Val Acc: 0.8054\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4971 | Train Acc: 0.8069 | Val Loss: 0.4815 | Val Acc: 0.8147\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4887 | Train Acc: 0.8102 | Val Loss: 0.4778 | Val Acc: 0.8151\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4828 | Train Acc: 0.8134 | Val Loss: 0.4785 | Val Acc: 0.8155\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4789 | Train Acc: 0.8149 | Val Loss: 0.4588 | Val Acc: 0.8231\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4748 | Train Acc: 0.8159 | Val Loss: 0.4758 | Val Acc: 0.8133\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4695 | Train Acc: 0.8174 | Val Loss: 0.4833 | Val Acc: 0.8139\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4667 | Train Acc: 0.8191 | Val Loss: 0.4706 | Val Acc: 0.8193\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4628 | Train Acc: 0.8209 | Val Loss: 0.4713 | Val Acc: 0.8167\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4581 | Train Acc: 0.8227 | Val Loss: 0.4630 | Val Acc: 0.8223\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4553 | Train Acc: 0.8238 | Val Loss: 0.4608 | Val Acc: 0.8216\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4515 | Train Acc: 0.8259 | Val Loss: 0.4934 | Val Acc: 0.8067\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4501 | Train Acc: 0.8250 | Val Loss: 0.4607 | Val Acc: 0.8262\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4460 | Train Acc: 0.8283 | Val Loss: 0.4677 | Val Acc: 0.8184\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4433 | Train Acc: 0.8277 | Val Loss: 0.4684 | Val Acc: 0.8196\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4419 | Train Acc: 0.8296 | Val Loss: 0.4640 | Val Acc: 0.8226\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4391 | Train Acc: 0.8293 | Val Loss: 0.4582 | Val Acc: 0.8229\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4378 | Train Acc: 0.8305 | Val Loss: 0.4607 | Val Acc: 0.8230\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4339 | Train Acc: 0.8318 | Val Loss: 0.4550 | Val Acc: 0.8271\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4322 | Train Acc: 0.8332 | Val Loss: 0.4541 | Val Acc: 0.8266\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4288 | Train Acc: 0.8340 | Val Loss: 0.4568 | Val Acc: 0.8215\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4270 | Train Acc: 0.8356 | Val Loss: 0.4540 | Val Acc: 0.8257\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4255 | Train Acc: 0.8358 | Val Loss: 0.4684 | Val Acc: 0.8234\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.4228 | Train Acc: 0.8369 | Val Loss: 0.4517 | Val Acc: 0.8246\n",
      "[Fold 2 | Epoch 29] Train Loss: 0.4211 | Train Acc: 0.8367 | Val Loss: 0.4522 | Val Acc: 0.8237\n",
      "[Fold 2 | Epoch 30] Train Loss: 0.4185 | Train Acc: 0.8382 | Val Loss: 0.4650 | Val Acc: 0.8226\n",
      "[Fold 2 | Epoch 31] Train Loss: 0.4172 | Train Acc: 0.8386 | Val Loss: 0.4528 | Val Acc: 0.8267\n",
      "[Fold 2 | Epoch 32] Train Loss: 0.4148 | Train Acc: 0.8394 | Val Loss: 0.4621 | Val Acc: 0.8197\n",
      "[Fold 2 | Epoch 33] Train Loss: 0.4133 | Train Acc: 0.8408 | Val Loss: 0.4559 | Val Acc: 0.8234\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 33.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 23\n",
      "ACC: 0.8271 | MF1: 0.7824 | G-Mean: 0.8573\n",
      "[Class 0] Prec: 0.9346 | Rec: 0.9216 | F1: 0.9280 | GM: 0.9454\n",
      "[Class 1] Prec: 0.5452 | Rec: 0.4964 | F1: 0.5197 | GM: 0.6836\n",
      "[Class 2] Prec: 0.8599 | Rec: 0.8764 | F1: 0.8681 | GM: 0.9002\n",
      "[Class 3] Prec: 0.9034 | Rec: 0.7885 | F1: 0.8420 | GM: 0.8850\n",
      "[Class 4] Prec: 0.7110 | Rec: 0.8035 | F1: 0.7544 | GM: 0.8722\n",
      "\n",
      "===== Fold 3 =====\n",
      "[INFO] Total parameters BEFORE pruning: 117,212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Non-zero parameters AFTER pruning: 58,808\n",
      "[INFO] Pruned parameters: 58,404 (49.83%)\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.6090 | Train Acc: 0.7580 | Val Loss: 0.5551 | Val Acc: 0.7799\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5434 | Train Acc: 0.7873 | Val Loss: 0.5481 | Val Acc: 0.7872\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5230 | Train Acc: 0.7961 | Val Loss: 0.5418 | Val Acc: 0.7820\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.5119 | Train Acc: 0.8001 | Val Loss: 0.4794 | Val Acc: 0.8137\n",
      "[Fold 3 | Epoch 5] Train Loss: 0.5034 | Train Acc: 0.8033 | Val Loss: 0.4800 | Val Acc: 0.8139\n",
      "[Fold 3 | Epoch 6] Train Loss: 0.4962 | Train Acc: 0.8062 | Val Loss: 0.4862 | Val Acc: 0.8111\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4915 | Train Acc: 0.8085 | Val Loss: 0.4706 | Val Acc: 0.8177\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4861 | Train Acc: 0.8111 | Val Loss: 0.5034 | Val Acc: 0.8047\n",
      "[Fold 3 | Epoch 9] Train Loss: 0.4822 | Train Acc: 0.8129 | Val Loss: 0.4885 | Val Acc: 0.8105\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4755 | Train Acc: 0.8154 | Val Loss: 0.4723 | Val Acc: 0.8156\n",
      "[Fold 3 | Epoch 11] Train Loss: 0.4741 | Train Acc: 0.8162 | Val Loss: 0.4706 | Val Acc: 0.8199\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4694 | Train Acc: 0.8173 | Val Loss: 0.4488 | Val Acc: 0.8276\n",
      "[Fold 3 | Epoch 13] Train Loss: 0.4655 | Train Acc: 0.8192 | Val Loss: 0.4460 | Val Acc: 0.8277\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4609 | Train Acc: 0.8202 | Val Loss: 0.4668 | Val Acc: 0.8199\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4570 | Train Acc: 0.8217 | Val Loss: 0.4444 | Val Acc: 0.8277\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4562 | Train Acc: 0.8229 | Val Loss: 0.4440 | Val Acc: 0.8265\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4509 | Train Acc: 0.8250 | Val Loss: 0.4426 | Val Acc: 0.8272\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4503 | Train Acc: 0.8251 | Val Loss: 0.4920 | Val Acc: 0.8110\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4453 | Train Acc: 0.8269 | Val Loss: 0.4561 | Val Acc: 0.8228\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4430 | Train Acc: 0.8274 | Val Loss: 0.4430 | Val Acc: 0.8295\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4402 | Train Acc: 0.8295 | Val Loss: 0.4446 | Val Acc: 0.8273\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4382 | Train Acc: 0.8300 | Val Loss: 0.4495 | Val Acc: 0.8230\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4364 | Train Acc: 0.8294 | Val Loss: 0.4566 | Val Acc: 0.8226\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4338 | Train Acc: 0.8314 | Val Loss: 0.4562 | Val Acc: 0.8223\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4322 | Train Acc: 0.8324 | Val Loss: 0.4659 | Val Acc: 0.8186\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4285 | Train Acc: 0.8335 | Val Loss: 0.4362 | Val Acc: 0.8313\n",
      "[Fold 3 | Epoch 27] Train Loss: 0.4263 | Train Acc: 0.8346 | Val Loss: 0.4421 | Val Acc: 0.8287\n",
      "[Fold 3 | Epoch 28] Train Loss: 0.4237 | Train Acc: 0.8359 | Val Loss: 0.4620 | Val Acc: 0.8212\n",
      "[Fold 3 | Epoch 29] Train Loss: 0.4211 | Train Acc: 0.8374 | Val Loss: 0.4453 | Val Acc: 0.8281\n",
      "[Fold 3 | Epoch 30] Train Loss: 0.4196 | Train Acc: 0.8380 | Val Loss: 0.4727 | Val Acc: 0.8157\n",
      "[Fold 3 | Epoch 31] Train Loss: 0.4172 | Train Acc: 0.8371 | Val Loss: 0.4706 | Val Acc: 0.8201\n",
      "[Fold 3 | Epoch 32] Train Loss: 0.4167 | Train Acc: 0.8387 | Val Loss: 0.4545 | Val Acc: 0.8254\n",
      "[Fold 3 | Epoch 33] Train Loss: 0.4154 | Train Acc: 0.8393 | Val Loss: 0.4496 | Val Acc: 0.8265\n",
      "[Fold 3 | Epoch 34] Train Loss: 0.4130 | Train Acc: 0.8396 | Val Loss: 0.4442 | Val Acc: 0.8282\n",
      "[Fold 3 | Epoch 35] Train Loss: 0.4093 | Train Acc: 0.8408 | Val Loss: 0.4492 | Val Acc: 0.8268\n",
      "[Fold 3 | Epoch 36] Train Loss: 0.4079 | Train Acc: 0.8418 | Val Loss: 0.4451 | Val Acc: 0.8303\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 36.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 26\n",
      "ACC: 0.8313 | MF1: 0.7871 | G-Mean: 0.8601\n",
      "[Class 0] Prec: 0.9422 | Rec: 0.9138 | F1: 0.9278 | GM: 0.9433\n",
      "[Class 1] Prec: 0.5437 | Rec: 0.4995 | F1: 0.5207 | GM: 0.6858\n",
      "[Class 2] Prec: 0.8393 | Rec: 0.9039 | F1: 0.8704 | GM: 0.9060\n",
      "[Class 3] Prec: 0.8632 | Rec: 0.8555 | F1: 0.8594 | GM: 0.9199\n",
      "[Class 4] Prec: 0.7759 | Rec: 0.7398 | F1: 0.7574 | GM: 0.8453\n",
      "\n",
      "===== Fold 4 =====\n",
      "[INFO] Total parameters BEFORE pruning: 117,212\n",
      "[INFO] Non-zero parameters AFTER pruning: 58,808\n",
      "[INFO] Pruned parameters: 58,404 (49.83%)\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.6222 | Train Acc: 0.7536 | Val Loss: 0.5579 | Val Acc: 0.7842\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5471 | Train Acc: 0.7866 | Val Loss: 0.5124 | Val Acc: 0.8017\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5270 | Train Acc: 0.7942 | Val Loss: 0.5249 | Val Acc: 0.7930\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5162 | Train Acc: 0.7985 | Val Loss: 0.5298 | Val Acc: 0.7976\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.5080 | Train Acc: 0.8005 | Val Loss: 0.4911 | Val Acc: 0.8105\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4988 | Train Acc: 0.8062 | Val Loss: 0.5571 | Val Acc: 0.7777\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4935 | Train Acc: 0.8078 | Val Loss: 0.4923 | Val Acc: 0.8108\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4856 | Train Acc: 0.8116 | Val Loss: 0.4625 | Val Acc: 0.8184\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4803 | Train Acc: 0.8131 | Val Loss: 0.4870 | Val Acc: 0.8117\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4754 | Train Acc: 0.8148 | Val Loss: 0.4674 | Val Acc: 0.8209\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4707 | Train Acc: 0.8181 | Val Loss: 0.4574 | Val Acc: 0.8209\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4676 | Train Acc: 0.8190 | Val Loss: 0.4496 | Val Acc: 0.8259\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4635 | Train Acc: 0.8202 | Val Loss: 0.4670 | Val Acc: 0.8138\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4586 | Train Acc: 0.8216 | Val Loss: 0.4878 | Val Acc: 0.8073\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4559 | Train Acc: 0.8230 | Val Loss: 0.4615 | Val Acc: 0.8164\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4536 | Train Acc: 0.8237 | Val Loss: 0.4415 | Val Acc: 0.8273\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4524 | Train Acc: 0.8246 | Val Loss: 0.4796 | Val Acc: 0.8125\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4459 | Train Acc: 0.8277 | Val Loss: 0.4452 | Val Acc: 0.8265\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4457 | Train Acc: 0.8272 | Val Loss: 0.4451 | Val Acc: 0.8275\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4420 | Train Acc: 0.8288 | Val Loss: 0.4478 | Val Acc: 0.8249\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4404 | Train Acc: 0.8287 | Val Loss: 0.4419 | Val Acc: 0.8249\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4372 | Train Acc: 0.8301 | Val Loss: 0.4442 | Val Acc: 0.8281\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4341 | Train Acc: 0.8320 | Val Loss: 0.4758 | Val Acc: 0.8137\n",
      "[Fold 4 | Epoch 24] Train Loss: 0.4335 | Train Acc: 0.8317 | Val Loss: 0.4462 | Val Acc: 0.8259\n",
      "[Fold 4 | Epoch 25] Train Loss: 0.4303 | Train Acc: 0.8340 | Val Loss: 0.4553 | Val Acc: 0.8252\n",
      "[Fold 4 | Epoch 26] Train Loss: 0.4273 | Train Acc: 0.8345 | Val Loss: 0.4480 | Val Acc: 0.8265\n",
      "[Fold 4 | Epoch 27] Train Loss: 0.4252 | Train Acc: 0.8360 | Val Loss: 0.4360 | Val Acc: 0.8307\n",
      "[Fold 4 | Epoch 28] Train Loss: 0.4229 | Train Acc: 0.8364 | Val Loss: 0.4412 | Val Acc: 0.8272\n",
      "[Fold 4 | Epoch 29] Train Loss: 0.4216 | Train Acc: 0.8366 | Val Loss: 0.4428 | Val Acc: 0.8252\n",
      "[Fold 4 | Epoch 30] Train Loss: 0.4201 | Train Acc: 0.8374 | Val Loss: 0.4573 | Val Acc: 0.8214\n",
      "[Fold 4 | Epoch 31] Train Loss: 0.4175 | Train Acc: 0.8380 | Val Loss: 0.4468 | Val Acc: 0.8287\n",
      "[Fold 4 | Epoch 32] Train Loss: 0.4150 | Train Acc: 0.8390 | Val Loss: 0.4518 | Val Acc: 0.8243\n",
      "[Fold 4 | Epoch 33] Train Loss: 0.4137 | Train Acc: 0.8399 | Val Loss: 0.4573 | Val Acc: 0.8228\n",
      "[Fold 4 | Epoch 34] Train Loss: 0.4100 | Train Acc: 0.8415 | Val Loss: 0.4545 | Val Acc: 0.8232\n",
      "[Fold 4 | Epoch 35] Train Loss: 0.4090 | Train Acc: 0.8421 | Val Loss: 0.4422 | Val Acc: 0.8279\n",
      "[Fold 4 | Epoch 36] Train Loss: 0.4085 | Train Acc: 0.8429 | Val Loss: 0.4749 | Val Acc: 0.8171\n",
      "[Fold 4 | Epoch 37] Train Loss: 0.4053 | Train Acc: 0.8433 | Val Loss: 0.4499 | Val Acc: 0.8213\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 37.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 27\n",
      "ACC: 0.8307 | MF1: 0.7809 | G-Mean: 0.8493\n",
      "[Class 0] Prec: 0.9251 | Rec: 0.9294 | F1: 0.9273 | GM: 0.9470\n",
      "[Class 1] Prec: 0.5697 | Rec: 0.4330 | F1: 0.4921 | GM: 0.6428\n",
      "[Class 2] Prec: 0.8270 | Rec: 0.9147 | F1: 0.8686 | GM: 0.9071\n",
      "[Class 3] Prec: 0.9100 | Rec: 0.7964 | F1: 0.8494 | GM: 0.8896\n",
      "[Class 4] Prec: 0.7649 | Rec: 0.7693 | F1: 0.7671 | GM: 0.8599\n",
      "\n",
      "===== Fold 5 =====\n",
      "[INFO] Total parameters BEFORE pruning: 117,212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Non-zero parameters AFTER pruning: 58,808\n",
      "[INFO] Pruned parameters: 58,404 (49.83%)\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.6227 | Train Acc: 0.7542 | Val Loss: 0.5237 | Val Acc: 0.7950\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5463 | Train Acc: 0.7858 | Val Loss: 0.6075 | Val Acc: 0.7661\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5296 | Train Acc: 0.7939 | Val Loss: 0.5225 | Val Acc: 0.7946\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.5179 | Train Acc: 0.7981 | Val Loss: 0.4960 | Val Acc: 0.8093\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.5116 | Train Acc: 0.8010 | Val Loss: 0.4941 | Val Acc: 0.8110\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.5023 | Train Acc: 0.8038 | Val Loss: 0.5004 | Val Acc: 0.8047\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4961 | Train Acc: 0.8064 | Val Loss: 0.5267 | Val Acc: 0.7960\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4901 | Train Acc: 0.8081 | Val Loss: 0.4728 | Val Acc: 0.8157\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4849 | Train Acc: 0.8109 | Val Loss: 0.4712 | Val Acc: 0.8188\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4802 | Train Acc: 0.8130 | Val Loss: 0.4745 | Val Acc: 0.8121\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4761 | Train Acc: 0.8151 | Val Loss: 0.4605 | Val Acc: 0.8234\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4723 | Train Acc: 0.8169 | Val Loss: 0.4732 | Val Acc: 0.8116\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4679 | Train Acc: 0.8186 | Val Loss: 0.4586 | Val Acc: 0.8261\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4644 | Train Acc: 0.8198 | Val Loss: 0.4650 | Val Acc: 0.8231\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4600 | Train Acc: 0.8220 | Val Loss: 0.4477 | Val Acc: 0.8295\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4573 | Train Acc: 0.8221 | Val Loss: 0.4499 | Val Acc: 0.8257\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4526 | Train Acc: 0.8241 | Val Loss: 0.4441 | Val Acc: 0.8293\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4498 | Train Acc: 0.8256 | Val Loss: 0.4502 | Val Acc: 0.8292\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.4496 | Train Acc: 0.8249 | Val Loss: 0.4634 | Val Acc: 0.8223\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.4447 | Train Acc: 0.8270 | Val Loss: 0.4385 | Val Acc: 0.8314\n",
      "[Fold 5 | Epoch 21] Train Loss: 0.4439 | Train Acc: 0.8273 | Val Loss: 0.4560 | Val Acc: 0.8234\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.4408 | Train Acc: 0.8294 | Val Loss: 0.4652 | Val Acc: 0.8241\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.4374 | Train Acc: 0.8298 | Val Loss: 0.4469 | Val Acc: 0.8243\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.4351 | Train Acc: 0.8305 | Val Loss: 0.4643 | Val Acc: 0.8225\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.4320 | Train Acc: 0.8327 | Val Loss: 0.4507 | Val Acc: 0.8292\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.4316 | Train Acc: 0.8326 | Val Loss: 0.4660 | Val Acc: 0.8248\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.4298 | Train Acc: 0.8329 | Val Loss: 0.4394 | Val Acc: 0.8315\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.4258 | Train Acc: 0.8356 | Val Loss: 0.4449 | Val Acc: 0.8266\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.4258 | Train Acc: 0.8342 | Val Loss: 0.4446 | Val Acc: 0.8301\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.4225 | Train Acc: 0.8350 | Val Loss: 0.4685 | Val Acc: 0.8223\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.4212 | Train Acc: 0.8363 | Val Loss: 0.4536 | Val Acc: 0.8224\n",
      "[Fold 5 | Epoch 32] Train Loss: 0.4189 | Train Acc: 0.8384 | Val Loss: 0.4478 | Val Acc: 0.8275\n",
      "[Fold 5 | Epoch 33] Train Loss: 0.4163 | Train Acc: 0.8380 | Val Loss: 0.4436 | Val Acc: 0.8283\n",
      "[Fold 5 | Epoch 34] Train Loss: 0.4149 | Train Acc: 0.8389 | Val Loss: 0.4483 | Val Acc: 0.8243\n",
      "[Fold 5 | Epoch 35] Train Loss: 0.4139 | Train Acc: 0.8392 | Val Loss: 0.4395 | Val Acc: 0.8308\n",
      "[Fold 5 | Epoch 36] Train Loss: 0.4104 | Train Acc: 0.8407 | Val Loss: 0.4508 | Val Acc: 0.8312\n",
      "[Fold 5 | Epoch 37] Train Loss: 0.4084 | Train Acc: 0.8406 | Val Loss: 0.4482 | Val Acc: 0.8300\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 37.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 27\n",
      "ACC: 0.8315 | MF1: 0.7840 | G-Mean: 0.8598\n",
      "[Class 0] Prec: 0.9184 | Rec: 0.9407 | F1: 0.9294 | GM: 0.9506\n",
      "[Class 1] Prec: 0.5672 | Rec: 0.4764 | F1: 0.5179 | GM: 0.6726\n",
      "[Class 2] Prec: 0.8590 | Rec: 0.8769 | F1: 0.8679 | GM: 0.8998\n",
      "[Class 3] Prec: 0.8433 | Rec: 0.8607 | F1: 0.8519 | GM: 0.9218\n",
      "[Class 4] Prec: 0.7451 | Rec: 0.7610 | F1: 0.7530 | GM: 0.8540\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8302 | MF1: 0.7829 | G-Mean: 0.8558\n",
      "[Class 0] Prec: 0.9268 | Rec: 0.9284 | F1: 0.9276 | GM: 0.9468\n",
      "[Class 1] Prec: 0.5574 | Rec: 0.4688 | F1: 0.5093 | GM: 0.6667\n",
      "[Class 2] Prec: 0.8462 | Rec: 0.8932 | F1: 0.8691 | GM: 0.9037\n",
      "[Class 3] Prec: 0.8827 | Rec: 0.8228 | F1: 0.8517 | GM: 0.9031\n",
      "[Class 4] Prec: 0.7446 | Rec: 0.7697 | F1: 0.7569 | GM: 0.8584\n",
      "Confusion Matrix:\n",
      "[[49584  2667   424    38   694]\n",
      " [ 2887  9585  4684     9  3281]\n",
      " [  334  2405 51652  1286  2151]\n",
      " [   31     2  2148 10193    14]\n",
      " [  667  2536  2130    22 17897]]\n"
     ]
    }
   ],
   "source": [
    "#p11_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    # === Build and Prune Model ===\n",
    "    model_unpruned = FinalNetwork(C=9, num_classes=num_classes, layers=11, genotype=searched_genotype).to(device)\n",
    "    total_params_before = sum(p.numel() for p in model_unpruned.parameters())\n",
    "    print(f\"[INFO] Total parameters BEFORE pruning: {total_params_before:,}\")\n",
    "\n",
    "    model = prune_model_entropy(model_unpruned, prune_ratio=0.5)\n",
    "    nonzero_params_after = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    zero_params = total_params_before - nonzero_params_after\n",
    "    pruned_ratio = 100 * zero_params / total_params_before\n",
    "    print(f\"[INFO] Non-zero parameters AFTER pruning: {nonzero_params_after:,}\")\n",
    "    print(f\"[INFO] Pruned parameters: {zero_params:,} ({pruned_ratio:.2f}%)\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dataloader\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOPPING COUNTER\n",
    "\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(\n",
    "                np.array(val_true), np.array(val_pred), num_classes\n",
    "            )\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    # Gộp toàn bộ val_true và val_pred để đánh giá toàn bộ tập sau K-Fold\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f14ac3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "[INFO] Total parameters BEFORE pruning: 75,902\n",
      "[INFO] Non-zero parameters AFTER pruning: 38,254\n",
      "[INFO] Pruned parameters: 37,648 (49.60%)\n",
      "[Fold 1 | Epoch 1] Train Loss: 0.5965 | Train Acc: 0.7646 | Val Loss: 0.7113 | Val Acc: 0.7129\n",
      "[Fold 1 | Epoch 2] Train Loss: 0.5316 | Train Acc: 0.7918 | Val Loss: 0.5037 | Val Acc: 0.8022\n",
      "[Fold 1 | Epoch 3] Train Loss: 0.5160 | Train Acc: 0.7985 | Val Loss: 0.4847 | Val Acc: 0.8124\n",
      "[Fold 1 | Epoch 4] Train Loss: 0.5058 | Train Acc: 0.8029 | Val Loss: 0.5024 | Val Acc: 0.8027\n",
      "[Fold 1 | Epoch 5] Train Loss: 0.4965 | Train Acc: 0.8064 | Val Loss: 0.4893 | Val Acc: 0.8076\n",
      "[Fold 1 | Epoch 6] Train Loss: 0.4872 | Train Acc: 0.8105 | Val Loss: 0.4818 | Val Acc: 0.8131\n",
      "[Fold 1 | Epoch 7] Train Loss: 0.4813 | Train Acc: 0.8127 | Val Loss: 0.4598 | Val Acc: 0.8233\n",
      "[Fold 1 | Epoch 8] Train Loss: 0.4748 | Train Acc: 0.8150 | Val Loss: 0.4613 | Val Acc: 0.8207\n",
      "[Fold 1 | Epoch 9] Train Loss: 0.4702 | Train Acc: 0.8171 | Val Loss: 0.4640 | Val Acc: 0.8206\n",
      "[Fold 1 | Epoch 10] Train Loss: 0.4678 | Train Acc: 0.8185 | Val Loss: 0.4664 | Val Acc: 0.8199\n",
      "[Fold 1 | Epoch 11] Train Loss: 0.4628 | Train Acc: 0.8201 | Val Loss: 0.4901 | Val Acc: 0.8140\n",
      "[Fold 1 | Epoch 12] Train Loss: 0.4607 | Train Acc: 0.8215 | Val Loss: 0.4509 | Val Acc: 0.8264\n",
      "[Fold 1 | Epoch 13] Train Loss: 0.4571 | Train Acc: 0.8215 | Val Loss: 0.4512 | Val Acc: 0.8265\n",
      "[Fold 1 | Epoch 14] Train Loss: 0.4535 | Train Acc: 0.8243 | Val Loss: 0.4536 | Val Acc: 0.8266\n",
      "[Fold 1 | Epoch 15] Train Loss: 0.4481 | Train Acc: 0.8266 | Val Loss: 0.4481 | Val Acc: 0.8268\n",
      "[Fold 1 | Epoch 16] Train Loss: 0.4485 | Train Acc: 0.8261 | Val Loss: 0.4443 | Val Acc: 0.8263\n",
      "[Fold 1 | Epoch 17] Train Loss: 0.4444 | Train Acc: 0.8281 | Val Loss: 0.4552 | Val Acc: 0.8249\n",
      "[Fold 1 | Epoch 18] Train Loss: 0.4409 | Train Acc: 0.8293 | Val Loss: 0.4776 | Val Acc: 0.8135\n",
      "[Fold 1 | Epoch 19] Train Loss: 0.4396 | Train Acc: 0.8294 | Val Loss: 0.4484 | Val Acc: 0.8240\n",
      "[Fold 1 | Epoch 20] Train Loss: 0.4363 | Train Acc: 0.8303 | Val Loss: 0.4572 | Val Acc: 0.8214\n",
      "[Fold 1 | Epoch 21] Train Loss: 0.4340 | Train Acc: 0.8327 | Val Loss: 0.4545 | Val Acc: 0.8249\n",
      "[Fold 1 | Epoch 22] Train Loss: 0.4322 | Train Acc: 0.8325 | Val Loss: 0.4513 | Val Acc: 0.8261\n",
      "[Fold 1 | Epoch 23] Train Loss: 0.4295 | Train Acc: 0.8337 | Val Loss: 0.4499 | Val Acc: 0.8271\n",
      "[Fold 1 | Epoch 24] Train Loss: 0.4277 | Train Acc: 0.8341 | Val Loss: 0.4595 | Val Acc: 0.8226\n",
      "[Fold 1 | Epoch 25] Train Loss: 0.4242 | Train Acc: 0.8355 | Val Loss: 0.4476 | Val Acc: 0.8275\n",
      "[Fold 1 | Epoch 26] Train Loss: 0.4232 | Train Acc: 0.8360 | Val Loss: 0.4534 | Val Acc: 0.8214\n",
      "[Fold 1 | Epoch 27] Train Loss: 0.4209 | Train Acc: 0.8365 | Val Loss: 0.4532 | Val Acc: 0.8215\n",
      "[Fold 1 | Epoch 28] Train Loss: 0.4194 | Train Acc: 0.8378 | Val Loss: 0.4504 | Val Acc: 0.8260\n",
      "[Fold 1 | Epoch 29] Train Loss: 0.4159 | Train Acc: 0.8379 | Val Loss: 0.4563 | Val Acc: 0.8225\n",
      "[Fold 1 | Epoch 30] Train Loss: 0.4150 | Train Acc: 0.8395 | Val Loss: 0.4482 | Val Acc: 0.8264\n",
      "[Fold 1 | Epoch 31] Train Loss: 0.4145 | Train Acc: 0.8392 | Val Loss: 0.4596 | Val Acc: 0.8244\n",
      "[Fold 1 | Epoch 32] Train Loss: 0.4105 | Train Acc: 0.8414 | Val Loss: 0.5035 | Val Acc: 0.8078\n",
      "[Fold 1 | Epoch 33] Train Loss: 0.4104 | Train Acc: 0.8407 | Val Loss: 0.4701 | Val Acc: 0.8215\n",
      "[Fold 1 | Epoch 34] Train Loss: 0.4070 | Train Acc: 0.8420 | Val Loss: 0.4670 | Val Acc: 0.8200\n",
      "[Fold 1 | Epoch 35] Train Loss: 0.4051 | Train Acc: 0.8434 | Val Loss: 0.4612 | Val Acc: 0.8211\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 35.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 1 =====\n",
      "Best Epoch: 25\n",
      "ACC: 0.8275 | MF1: 0.7832 | G-Mean: 0.8576\n",
      "[Class 0] Prec: 0.9302 | Rec: 0.9188 | F1: 0.9245 | GM: 0.9427\n",
      "[Class 1] Prec: 0.5302 | Rec: 0.5306 | F1: 0.5304 | GM: 0.7046\n",
      "[Class 2] Prec: 0.8404 | Rec: 0.8934 | F1: 0.8661 | GM: 0.9021\n",
      "[Class 3] Prec: 0.8543 | Rec: 0.8499 | F1: 0.8521 | GM: 0.9164\n",
      "[Class 4] Prec: 0.7970 | Rec: 0.6956 | F1: 0.7428 | GM: 0.8222\n",
      "\n",
      "===== Fold 2 =====\n",
      "[INFO] Total parameters BEFORE pruning: 75,902\n",
      "[INFO] Non-zero parameters AFTER pruning: 38,254\n",
      "[INFO] Pruned parameters: 37,648 (49.60%)\n",
      "[Fold 2 | Epoch 1] Train Loss: 0.5980 | Train Acc: 0.7637 | Val Loss: 0.5795 | Val Acc: 0.7694\n",
      "[Fold 2 | Epoch 2] Train Loss: 0.5280 | Train Acc: 0.7947 | Val Loss: 0.6052 | Val Acc: 0.7640\n",
      "[Fold 2 | Epoch 3] Train Loss: 0.5129 | Train Acc: 0.7996 | Val Loss: 0.6033 | Val Acc: 0.7769\n",
      "[Fold 2 | Epoch 4] Train Loss: 0.5035 | Train Acc: 0.8037 | Val Loss: 0.5428 | Val Acc: 0.7862\n",
      "[Fold 2 | Epoch 5] Train Loss: 0.4940 | Train Acc: 0.8084 | Val Loss: 0.4957 | Val Acc: 0.8070\n",
      "[Fold 2 | Epoch 6] Train Loss: 0.4882 | Train Acc: 0.8095 | Val Loss: 0.4667 | Val Acc: 0.8194\n",
      "[Fold 2 | Epoch 7] Train Loss: 0.4829 | Train Acc: 0.8122 | Val Loss: 0.4847 | Val Acc: 0.8102\n",
      "[Fold 2 | Epoch 8] Train Loss: 0.4765 | Train Acc: 0.8153 | Val Loss: 0.4991 | Val Acc: 0.8013\n",
      "[Fold 2 | Epoch 9] Train Loss: 0.4733 | Train Acc: 0.8165 | Val Loss: 0.4647 | Val Acc: 0.8200\n",
      "[Fold 2 | Epoch 10] Train Loss: 0.4682 | Train Acc: 0.8196 | Val Loss: 0.4801 | Val Acc: 0.8175\n",
      "[Fold 2 | Epoch 11] Train Loss: 0.4659 | Train Acc: 0.8202 | Val Loss: 0.4628 | Val Acc: 0.8190\n",
      "[Fold 2 | Epoch 12] Train Loss: 0.4601 | Train Acc: 0.8219 | Val Loss: 0.4673 | Val Acc: 0.8177\n",
      "[Fold 2 | Epoch 13] Train Loss: 0.4572 | Train Acc: 0.8225 | Val Loss: 0.5088 | Val Acc: 0.8024\n",
      "[Fold 2 | Epoch 14] Train Loss: 0.4544 | Train Acc: 0.8238 | Val Loss: 0.4462 | Val Acc: 0.8293\n",
      "[Fold 2 | Epoch 15] Train Loss: 0.4492 | Train Acc: 0.8259 | Val Loss: 0.4764 | Val Acc: 0.8157\n",
      "[Fold 2 | Epoch 16] Train Loss: 0.4471 | Train Acc: 0.8264 | Val Loss: 0.4504 | Val Acc: 0.8268\n",
      "[Fold 2 | Epoch 17] Train Loss: 0.4448 | Train Acc: 0.8279 | Val Loss: 0.4838 | Val Acc: 0.8109\n",
      "[Fold 2 | Epoch 18] Train Loss: 0.4408 | Train Acc: 0.8293 | Val Loss: 0.4448 | Val Acc: 0.8301\n",
      "[Fold 2 | Epoch 19] Train Loss: 0.4392 | Train Acc: 0.8299 | Val Loss: 0.4652 | Val Acc: 0.8198\n",
      "[Fold 2 | Epoch 20] Train Loss: 0.4360 | Train Acc: 0.8311 | Val Loss: 0.4522 | Val Acc: 0.8251\n",
      "[Fold 2 | Epoch 21] Train Loss: 0.4357 | Train Acc: 0.8310 | Val Loss: 0.4520 | Val Acc: 0.8249\n",
      "[Fold 2 | Epoch 22] Train Loss: 0.4325 | Train Acc: 0.8332 | Val Loss: 0.4512 | Val Acc: 0.8278\n",
      "[Fold 2 | Epoch 23] Train Loss: 0.4288 | Train Acc: 0.8329 | Val Loss: 0.4440 | Val Acc: 0.8298\n",
      "[Fold 2 | Epoch 24] Train Loss: 0.4266 | Train Acc: 0.8350 | Val Loss: 0.4511 | Val Acc: 0.8239\n",
      "[Fold 2 | Epoch 25] Train Loss: 0.4247 | Train Acc: 0.8356 | Val Loss: 0.4414 | Val Acc: 0.8285\n",
      "[Fold 2 | Epoch 26] Train Loss: 0.4221 | Train Acc: 0.8363 | Val Loss: 0.4498 | Val Acc: 0.8280\n",
      "[Fold 2 | Epoch 27] Train Loss: 0.4202 | Train Acc: 0.8376 | Val Loss: 0.4478 | Val Acc: 0.8252\n",
      "[Fold 2 | Epoch 28] Train Loss: 0.4182 | Train Acc: 0.8385 | Val Loss: 0.4580 | Val Acc: 0.8226\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 28.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 2 =====\n",
      "Best Epoch: 18\n",
      "ACC: 0.8301 | MF1: 0.7815 | G-Mean: 0.8550\n",
      "[Class 0] Prec: 0.9188 | Rec: 0.9377 | F1: 0.9282 | GM: 0.9495\n",
      "[Class 1] Prec: 0.5665 | Rec: 0.4464 | F1: 0.4993 | GM: 0.6518\n",
      "[Class 2] Prec: 0.8550 | Rec: 0.8864 | F1: 0.8704 | GM: 0.9034\n",
      "[Class 3] Prec: 0.8843 | Rec: 0.8219 | F1: 0.8520 | GM: 0.9027\n",
      "[Class 4] Prec: 0.7273 | Rec: 0.7904 | F1: 0.7575 | GM: 0.8673\n",
      "\n",
      "===== Fold 3 =====\n",
      "[INFO] Total parameters BEFORE pruning: 75,902\n",
      "[INFO] Non-zero parameters AFTER pruning: 38,254\n",
      "[INFO] Pruned parameters: 37,648 (49.60%)\n",
      "[Fold 3 | Epoch 1] Train Loss: 0.5969 | Train Acc: 0.7663 | Val Loss: 0.5495 | Val Acc: 0.7854\n",
      "[Fold 3 | Epoch 2] Train Loss: 0.5343 | Train Acc: 0.7924 | Val Loss: 0.5093 | Val Acc: 0.8031\n",
      "[Fold 3 | Epoch 3] Train Loss: 0.5176 | Train Acc: 0.7983 | Val Loss: 0.5060 | Val Acc: 0.8006\n",
      "[Fold 3 | Epoch 4] Train Loss: 0.5095 | Train Acc: 0.8018 | Val Loss: 0.4958 | Val Acc: 0.8111\n",
      "[Fold 3 | Epoch 5] Train Loss: 0.4960 | Train Acc: 0.8068 | Val Loss: 0.4827 | Val Acc: 0.8124\n",
      "[Fold 3 | Epoch 6] Train Loss: 0.4923 | Train Acc: 0.8093 | Val Loss: 0.4613 | Val Acc: 0.8232\n",
      "[Fold 3 | Epoch 7] Train Loss: 0.4858 | Train Acc: 0.8122 | Val Loss: 0.4730 | Val Acc: 0.8086\n",
      "[Fold 3 | Epoch 8] Train Loss: 0.4800 | Train Acc: 0.8129 | Val Loss: 0.4550 | Val Acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3 | Epoch 9] Train Loss: 0.4766 | Train Acc: 0.8141 | Val Loss: 0.4862 | Val Acc: 0.8099\n",
      "[Fold 3 | Epoch 10] Train Loss: 0.4738 | Train Acc: 0.8159 | Val Loss: 0.4735 | Val Acc: 0.8188\n",
      "[Fold 3 | Epoch 11] Train Loss: 0.4687 | Train Acc: 0.8174 | Val Loss: 0.4732 | Val Acc: 0.8116\n",
      "[Fold 3 | Epoch 12] Train Loss: 0.4649 | Train Acc: 0.8205 | Val Loss: 0.4821 | Val Acc: 0.8116\n",
      "[Fold 3 | Epoch 13] Train Loss: 0.4623 | Train Acc: 0.8209 | Val Loss: 0.4651 | Val Acc: 0.8234\n",
      "[Fold 3 | Epoch 14] Train Loss: 0.4592 | Train Acc: 0.8225 | Val Loss: 0.4528 | Val Acc: 0.8228\n",
      "[Fold 3 | Epoch 15] Train Loss: 0.4567 | Train Acc: 0.8228 | Val Loss: 0.4620 | Val Acc: 0.8185\n",
      "[Fold 3 | Epoch 16] Train Loss: 0.4530 | Train Acc: 0.8250 | Val Loss: 0.4597 | Val Acc: 0.8250\n",
      "[Fold 3 | Epoch 17] Train Loss: 0.4512 | Train Acc: 0.8248 | Val Loss: 0.4438 | Val Acc: 0.8278\n",
      "[Fold 3 | Epoch 18] Train Loss: 0.4463 | Train Acc: 0.8276 | Val Loss: 0.4536 | Val Acc: 0.8243\n",
      "[Fold 3 | Epoch 19] Train Loss: 0.4444 | Train Acc: 0.8275 | Val Loss: 0.4515 | Val Acc: 0.8262\n",
      "[Fold 3 | Epoch 20] Train Loss: 0.4415 | Train Acc: 0.8289 | Val Loss: 0.4619 | Val Acc: 0.8213\n",
      "[Fold 3 | Epoch 21] Train Loss: 0.4408 | Train Acc: 0.8294 | Val Loss: 0.4541 | Val Acc: 0.8236\n",
      "[Fold 3 | Epoch 22] Train Loss: 0.4370 | Train Acc: 0.8308 | Val Loss: 0.4415 | Val Acc: 0.8309\n",
      "[Fold 3 | Epoch 23] Train Loss: 0.4359 | Train Acc: 0.8309 | Val Loss: 0.4597 | Val Acc: 0.8257\n",
      "[Fold 3 | Epoch 24] Train Loss: 0.4330 | Train Acc: 0.8319 | Val Loss: 0.4533 | Val Acc: 0.8246\n",
      "[Fold 3 | Epoch 25] Train Loss: 0.4301 | Train Acc: 0.8338 | Val Loss: 0.4503 | Val Acc: 0.8257\n",
      "[Fold 3 | Epoch 26] Train Loss: 0.4291 | Train Acc: 0.8339 | Val Loss: 0.4508 | Val Acc: 0.8264\n",
      "[Fold 3 | Epoch 27] Train Loss: 0.4263 | Train Acc: 0.8355 | Val Loss: 0.4484 | Val Acc: 0.8284\n",
      "[Fold 3 | Epoch 28] Train Loss: 0.4251 | Train Acc: 0.8357 | Val Loss: 0.4468 | Val Acc: 0.8251\n",
      "[Fold 3 | Epoch 29] Train Loss: 0.4220 | Train Acc: 0.8366 | Val Loss: 0.4569 | Val Acc: 0.8251\n",
      "[Fold 3 | Epoch 30] Train Loss: 0.4187 | Train Acc: 0.8386 | Val Loss: 0.4537 | Val Acc: 0.8246\n",
      "[Fold 3 | Epoch 31] Train Loss: 0.4178 | Train Acc: 0.8375 | Val Loss: 0.4511 | Val Acc: 0.8274\n",
      "[Fold 3 | Epoch 32] Train Loss: 0.4153 | Train Acc: 0.8394 | Val Loss: 0.4583 | Val Acc: 0.8246\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 32.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 3 =====\n",
      "Best Epoch: 22\n",
      "ACC: 0.8309 | MF1: 0.7780 | G-Mean: 0.8483\n",
      "[Class 0] Prec: 0.9430 | Rec: 0.9187 | F1: 0.9307 | GM: 0.9459\n",
      "[Class 1] Prec: 0.5787 | Rec: 0.4086 | F1: 0.4790 | GM: 0.6259\n",
      "[Class 2] Prec: 0.8259 | Rec: 0.9181 | F1: 0.8696 | GM: 0.9076\n",
      "[Class 3] Prec: 0.9114 | Rec: 0.7934 | F1: 0.8483 | GM: 0.8880\n",
      "[Class 4] Prec: 0.7248 | Rec: 0.8036 | F1: 0.7622 | GM: 0.8742\n",
      "\n",
      "===== Fold 4 =====\n",
      "[INFO] Total parameters BEFORE pruning: 75,902\n",
      "[INFO] Non-zero parameters AFTER pruning: 38,254\n",
      "[INFO] Pruned parameters: 37,648 (49.60%)\n",
      "[Fold 4 | Epoch 1] Train Loss: 0.6059 | Train Acc: 0.7599 | Val Loss: 0.5724 | Val Acc: 0.7786\n",
      "[Fold 4 | Epoch 2] Train Loss: 0.5356 | Train Acc: 0.7915 | Val Loss: 0.6102 | Val Acc: 0.7557\n",
      "[Fold 4 | Epoch 3] Train Loss: 0.5175 | Train Acc: 0.7989 | Val Loss: 0.5104 | Val Acc: 0.8032\n",
      "[Fold 4 | Epoch 4] Train Loss: 0.5068 | Train Acc: 0.8033 | Val Loss: 0.5258 | Val Acc: 0.7933\n",
      "[Fold 4 | Epoch 5] Train Loss: 0.5013 | Train Acc: 0.8060 | Val Loss: 0.5005 | Val Acc: 0.8042\n",
      "[Fold 4 | Epoch 6] Train Loss: 0.4930 | Train Acc: 0.8093 | Val Loss: 0.4666 | Val Acc: 0.8211\n",
      "[Fold 4 | Epoch 7] Train Loss: 0.4860 | Train Acc: 0.8106 | Val Loss: 0.4671 | Val Acc: 0.8199\n",
      "[Fold 4 | Epoch 8] Train Loss: 0.4811 | Train Acc: 0.8126 | Val Loss: 0.4609 | Val Acc: 0.8184\n",
      "[Fold 4 | Epoch 9] Train Loss: 0.4768 | Train Acc: 0.8147 | Val Loss: 0.4516 | Val Acc: 0.8253\n",
      "[Fold 4 | Epoch 10] Train Loss: 0.4714 | Train Acc: 0.8164 | Val Loss: 0.4841 | Val Acc: 0.8051\n",
      "[Fold 4 | Epoch 11] Train Loss: 0.4695 | Train Acc: 0.8172 | Val Loss: 0.4736 | Val Acc: 0.8169\n",
      "[Fold 4 | Epoch 12] Train Loss: 0.4662 | Train Acc: 0.8200 | Val Loss: 0.4443 | Val Acc: 0.8285\n",
      "[Fold 4 | Epoch 13] Train Loss: 0.4609 | Train Acc: 0.8203 | Val Loss: 0.4656 | Val Acc: 0.8188\n",
      "[Fold 4 | Epoch 14] Train Loss: 0.4601 | Train Acc: 0.8222 | Val Loss: 0.4477 | Val Acc: 0.8263\n",
      "[Fold 4 | Epoch 15] Train Loss: 0.4548 | Train Acc: 0.8240 | Val Loss: 0.4445 | Val Acc: 0.8279\n",
      "[Fold 4 | Epoch 16] Train Loss: 0.4513 | Train Acc: 0.8253 | Val Loss: 0.4660 | Val Acc: 0.8188\n",
      "[Fold 4 | Epoch 17] Train Loss: 0.4505 | Train Acc: 0.8254 | Val Loss: 0.4596 | Val Acc: 0.8193\n",
      "[Fold 4 | Epoch 18] Train Loss: 0.4474 | Train Acc: 0.8262 | Val Loss: 0.4449 | Val Acc: 0.8281\n",
      "[Fold 4 | Epoch 19] Train Loss: 0.4468 | Train Acc: 0.8267 | Val Loss: 0.4387 | Val Acc: 0.8304\n",
      "[Fold 4 | Epoch 20] Train Loss: 0.4430 | Train Acc: 0.8285 | Val Loss: 0.4435 | Val Acc: 0.8292\n",
      "[Fold 4 | Epoch 21] Train Loss: 0.4404 | Train Acc: 0.8290 | Val Loss: 0.4493 | Val Acc: 0.8232\n",
      "[Fold 4 | Epoch 22] Train Loss: 0.4386 | Train Acc: 0.8306 | Val Loss: 0.4595 | Val Acc: 0.8202\n",
      "[Fold 4 | Epoch 23] Train Loss: 0.4378 | Train Acc: 0.8301 | Val Loss: 0.4423 | Val Acc: 0.8275\n",
      "[Fold 4 | Epoch 24] Train Loss: 0.4336 | Train Acc: 0.8323 | Val Loss: 0.4584 | Val Acc: 0.8217\n",
      "[Fold 4 | Epoch 25] Train Loss: 0.4318 | Train Acc: 0.8323 | Val Loss: 0.4529 | Val Acc: 0.8248\n",
      "[Fold 4 | Epoch 26] Train Loss: 0.4309 | Train Acc: 0.8341 | Val Loss: 0.4467 | Val Acc: 0.8230\n",
      "[Fold 4 | Epoch 27] Train Loss: 0.4261 | Train Acc: 0.8347 | Val Loss: 0.4762 | Val Acc: 0.8174\n",
      "[Fold 4 | Epoch 28] Train Loss: 0.4258 | Train Acc: 0.8349 | Val Loss: 0.4468 | Val Acc: 0.8272\n",
      "[Fold 4 | Epoch 29] Train Loss: 0.4241 | Train Acc: 0.8355 | Val Loss: 0.4395 | Val Acc: 0.8297\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 29.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 4 =====\n",
      "Best Epoch: 19\n",
      "ACC: 0.8304 | MF1: 0.7856 | G-Mean: 0.8560\n",
      "[Class 0] Prec: 0.9198 | Rec: 0.9340 | F1: 0.9268 | GM: 0.9480\n",
      "[Class 1] Prec: 0.5489 | Rec: 0.5010 | F1: 0.5238 | GM: 0.6870\n",
      "[Class 2] Prec: 0.8528 | Rec: 0.8886 | F1: 0.8703 | GM: 0.9039\n",
      "[Class 3] Prec: 0.9164 | Rec: 0.7842 | F1: 0.8452 | GM: 0.8830\n",
      "[Class 4] Prec: 0.7564 | Rec: 0.7676 | F1: 0.7620 | GM: 0.8581\n",
      "\n",
      "===== Fold 5 =====\n",
      "[INFO] Total parameters BEFORE pruning: 75,902\n",
      "[INFO] Non-zero parameters AFTER pruning: 38,254\n",
      "[INFO] Pruned parameters: 37,648 (49.60%)\n",
      "[Fold 5 | Epoch 1] Train Loss: 0.5991 | Train Acc: 0.7636 | Val Loss: 0.5461 | Val Acc: 0.7834\n",
      "[Fold 5 | Epoch 2] Train Loss: 0.5323 | Train Acc: 0.7931 | Val Loss: 0.5295 | Val Acc: 0.7930\n",
      "[Fold 5 | Epoch 3] Train Loss: 0.5195 | Train Acc: 0.7983 | Val Loss: 0.5057 | Val Acc: 0.8020\n",
      "[Fold 5 | Epoch 4] Train Loss: 0.5073 | Train Acc: 0.8035 | Val Loss: 0.4985 | Val Acc: 0.8033\n",
      "[Fold 5 | Epoch 5] Train Loss: 0.5000 | Train Acc: 0.8046 | Val Loss: 0.4854 | Val Acc: 0.8133\n",
      "[Fold 5 | Epoch 6] Train Loss: 0.4939 | Train Acc: 0.8088 | Val Loss: 0.4804 | Val Acc: 0.8124\n",
      "[Fold 5 | Epoch 7] Train Loss: 0.4865 | Train Acc: 0.8120 | Val Loss: 0.5055 | Val Acc: 0.8018\n",
      "[Fold 5 | Epoch 8] Train Loss: 0.4796 | Train Acc: 0.8145 | Val Loss: 0.4621 | Val Acc: 0.8213\n",
      "[Fold 5 | Epoch 9] Train Loss: 0.4758 | Train Acc: 0.8155 | Val Loss: 0.4624 | Val Acc: 0.8199\n",
      "[Fold 5 | Epoch 10] Train Loss: 0.4716 | Train Acc: 0.8159 | Val Loss: 0.5204 | Val Acc: 0.8027\n",
      "[Fold 5 | Epoch 11] Train Loss: 0.4671 | Train Acc: 0.8195 | Val Loss: 0.5022 | Val Acc: 0.8056\n",
      "[Fold 5 | Epoch 12] Train Loss: 0.4648 | Train Acc: 0.8190 | Val Loss: 0.4553 | Val Acc: 0.8232\n",
      "[Fold 5 | Epoch 13] Train Loss: 0.4613 | Train Acc: 0.8215 | Val Loss: 0.4631 | Val Acc: 0.8201\n",
      "[Fold 5 | Epoch 14] Train Loss: 0.4584 | Train Acc: 0.8222 | Val Loss: 0.4621 | Val Acc: 0.8182\n",
      "[Fold 5 | Epoch 15] Train Loss: 0.4536 | Train Acc: 0.8238 | Val Loss: 0.4460 | Val Acc: 0.8272\n",
      "[Fold 5 | Epoch 16] Train Loss: 0.4521 | Train Acc: 0.8252 | Val Loss: 0.4534 | Val Acc: 0.8292\n",
      "[Fold 5 | Epoch 17] Train Loss: 0.4473 | Train Acc: 0.8270 | Val Loss: 0.4546 | Val Acc: 0.8276\n",
      "[Fold 5 | Epoch 18] Train Loss: 0.4440 | Train Acc: 0.8277 | Val Loss: 0.4436 | Val Acc: 0.8272\n",
      "[Fold 5 | Epoch 19] Train Loss: 0.4427 | Train Acc: 0.8276 | Val Loss: 0.4638 | Val Acc: 0.8176\n",
      "[Fold 5 | Epoch 20] Train Loss: 0.4387 | Train Acc: 0.8299 | Val Loss: 0.4435 | Val Acc: 0.8291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5 | Epoch 21] Train Loss: 0.4363 | Train Acc: 0.8310 | Val Loss: 0.4827 | Val Acc: 0.8126\n",
      "[Fold 5 | Epoch 22] Train Loss: 0.4351 | Train Acc: 0.8322 | Val Loss: 0.4371 | Val Acc: 0.8310\n",
      "[Fold 5 | Epoch 23] Train Loss: 0.4320 | Train Acc: 0.8334 | Val Loss: 0.4400 | Val Acc: 0.8295\n",
      "[Fold 5 | Epoch 24] Train Loss: 0.4307 | Train Acc: 0.8319 | Val Loss: 0.4552 | Val Acc: 0.8239\n",
      "[Fold 5 | Epoch 25] Train Loss: 0.4288 | Train Acc: 0.8338 | Val Loss: 0.4489 | Val Acc: 0.8290\n",
      "[Fold 5 | Epoch 26] Train Loss: 0.4253 | Train Acc: 0.8350 | Val Loss: 0.4450 | Val Acc: 0.8291\n",
      "[Fold 5 | Epoch 27] Train Loss: 0.4224 | Train Acc: 0.8357 | Val Loss: 0.4559 | Val Acc: 0.8223\n",
      "[Fold 5 | Epoch 28] Train Loss: 0.4217 | Train Acc: 0.8365 | Val Loss: 0.4492 | Val Acc: 0.8257\n",
      "[Fold 5 | Epoch 29] Train Loss: 0.4203 | Train Acc: 0.8368 | Val Loss: 0.4512 | Val Acc: 0.8232\n",
      "[Fold 5 | Epoch 30] Train Loss: 0.4168 | Train Acc: 0.8386 | Val Loss: 0.4614 | Val Acc: 0.8266\n",
      "[Fold 5 | Epoch 31] Train Loss: 0.4151 | Train Acc: 0.8401 | Val Loss: 0.4531 | Val Acc: 0.8263\n",
      "[Fold 5 | Epoch 32] Train Loss: 0.4131 | Train Acc: 0.8408 | Val Loss: 0.4674 | Val Acc: 0.8201\n",
      "[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch 32.\n",
      "\n",
      "===== BEST RESULT FOR FOLD 5 =====\n",
      "Best Epoch: 22\n",
      "ACC: 0.8310 | MF1: 0.7901 | G-Mean: 0.8629\n",
      "[Class 0] Prec: 0.9368 | Rec: 0.9246 | F1: 0.9306 | GM: 0.9473\n",
      "[Class 1] Prec: 0.5229 | Rec: 0.5908 | F1: 0.5548 | GM: 0.7394\n",
      "[Class 2] Prec: 0.8594 | Rec: 0.8828 | F1: 0.8709 | GM: 0.9027\n",
      "[Class 3] Prec: 0.8896 | Rec: 0.8149 | F1: 0.8506 | GM: 0.8991\n",
      "[Class 4] Prec: 0.7884 | Rec: 0.7033 | F1: 0.7434 | GM: 0.8259\n",
      "\n",
      "===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\n",
      "ACC: 0.8300 | MF1: 0.7841 | G-Mean: 0.8564\n",
      "[Class 0] Prec: 0.9296 | Rec: 0.9267 | F1: 0.9282 | GM: 0.9467\n",
      "[Class 1] Prec: 0.5461 | Rec: 0.4951 | F1: 0.5194 | GM: 0.6832\n",
      "[Class 2] Prec: 0.8464 | Rec: 0.8939 | F1: 0.8695 | GM: 0.9040\n",
      "[Class 3] Prec: 0.8898 | Rec: 0.8130 | F1: 0.8497 | GM: 0.8981\n",
      "[Class 4] Prec: 0.7557 | Rec: 0.7524 | F1: 0.7540 | GM: 0.8502\n",
      "Confusion Matrix:\n",
      "[[49494  2685   486    46   696]\n",
      " [ 2766 10123  4669     7  2881]\n",
      " [  282  2630 51690  1171  2055]\n",
      " [   23     1  2269 10072    23]\n",
      " [  678  3097  1960    23 17494]]\n"
     ]
    }
   ],
   "source": [
    "#p11_2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ==== Load and Normalize Data ====\n",
    "df = pd.read_csv(\"pruned_dataset.csv\")\n",
    "X_np = df.drop(columns=[\"label\"]).values\n",
    "y_np = df[\"label\"].values\n",
    "\n",
    "X_mean = X_np.mean(axis=0)\n",
    "X_std = np.where(X_np.std(axis=0) == 0, 1, X_np.std(axis=0))\n",
    "X_z = (X_np - X_mean) / X_std\n",
    "X_z = np.clip(X_z, -3, 3) / 3.0\n",
    "\n",
    "X_all = torch.tensor(X_z, dtype=torch.float32).unsqueeze(1)\n",
    "y_all = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y_np))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== K-Fold Training ====\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_val_true, all_val_pred = [], []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all)):\n",
    "    print(f\"\\n===== Fold {fold_idx + 1} =====\")\n",
    "\n",
    "    # === Build and Prune Model ===\n",
    "    model_unpruned = FinalNetwork(C=9, num_classes=num_classes, layers=7, genotype=searched_genotype).to(device)\n",
    "    total_params_before = sum(p.numel() for p in model_unpruned.parameters())\n",
    "    print(f\"[INFO] Total parameters BEFORE pruning: {total_params_before:,}\")\n",
    "\n",
    "    model = prune_model_entropy(model_unpruned, prune_ratio=0.5)\n",
    "    nonzero_params_after = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    zero_params = total_params_before - nonzero_params_after\n",
    "    pruned_ratio = 100 * zero_params / total_params_before\n",
    "    print(f\"[INFO] Non-zero parameters AFTER pruning: {nonzero_params_after:,}\")\n",
    "    print(f\"[INFO] Pruned parameters: {zero_params:,} ({pruned_ratio:.2f}%)\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dataloader\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_result = {}\n",
    "    no_improve_counter = 0  # ← EARLY STOPPING COUNTER\n",
    "\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true, train_pred = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_true.extend(y.cpu().numpy())\n",
    "            train_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                val_true.extend(y.cpu().numpy())\n",
    "                val_pred.extend(output.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_true, train_pred)\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        print(f\"[Fold {fold_idx+1} | Epoch {epoch+1}] Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improve_counter = 0  # reset counter\n",
    "            acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(\n",
    "                np.array(val_true), np.array(val_pred), num_classes\n",
    "            )\n",
    "            best_result = {\n",
    "                'epoch': epoch + 1,\n",
    "                'acc': acc,\n",
    "                'mf1': mf1,\n",
    "                'gmean': mgm,\n",
    "                'prec': prec,\n",
    "                'rec': rec,\n",
    "                'f1': f1s,\n",
    "                'gmean_class': gmeans,\n",
    "                'cm': cm,\n",
    "                'val_true': val_true,\n",
    "                'val_pred': val_pred\n",
    "            }\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            if no_improve_counter >= 10:\n",
    "                print(f\"[Early Stopping] No improvement in 10 consecutive epochs. Stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    # === Print Best Result for Fold ===\n",
    "    print(f\"\\n===== BEST RESULT FOR FOLD {fold_idx+1} =====\")\n",
    "    print(f\"Best Epoch: {best_result['epoch']}\")\n",
    "    print(f\"ACC: {best_result['acc']:.4f} | MF1: {best_result['mf1']:.4f} | G-Mean: {best_result['gmean']:.4f}\")\n",
    "    for i in range(num_classes):\n",
    "        print(f\"[Class {i}] Prec: {best_result['prec'][i]:.4f} | Rec: {best_result['rec'][i]:.4f} \"\n",
    "              f\"| F1: {best_result['f1'][i]:.4f} | GM: {best_result['gmean_class'][i]:.4f}\")\n",
    "\n",
    "    # Gộp toàn bộ val_true và val_pred để đánh giá toàn bộ tập sau K-Fold\n",
    "    all_val_true.extend(best_result['val_true'])\n",
    "    all_val_pred.extend(best_result['val_pred'])\n",
    "\n",
    "# ==== FINAL EVALUATION ON MERGED VAL SET ====\n",
    "acc, mf1, mgm, prec, rec, f1s, gmeans, cm = evaluate_metrics(np.array(all_val_true), np.array(all_val_pred), num_classes)\n",
    "\n",
    "print(\"\\n===== FINAL EVALUATION ON MERGED TEST SET (AFTER K-FOLD) =====\")\n",
    "print(f\"ACC: {acc:.4f} | MF1: {mf1:.4f} | G-Mean: {mgm:.4f}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"[Class {i}] Prec: {prec[i]:.4f} | Rec: {rec[i]:.4f} | F1: {f1s[i]:.4f} | GM: {gmeans[i]:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d303c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
