{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239ebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader_darts import get_dataloaders_simple\n",
    "from darts_search_bdp import train_darts_search_bdp\n",
    "from model_build import FinalNetwork\n",
    "from cell_plot import plot_cell\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c286838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_final_model(model, train_loader, val_loader, device, epochs=25):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_acc = 0\n",
    "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x.squeeze(-1)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_final_model.pt\")\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"[Final Train Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_list, label='Train Loss')\n",
    "    plt.plot(val_loss_list, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve')\n",
    "    plt.savefig('final_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc_list, label='Train Acc')\n",
    "    plt.plot(val_acc_list, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.savefig('final_accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device).squeeze(-1)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            y_true.extend(y.numpy())\n",
    "            y_pred.extend(pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix on Validation:\")\n",
    "    print(cm)\n",
    "    pd.DataFrame(cm).to_csv(\"confusion_matrix.csv\", index=False)\n",
    "    print(\"[\\u2713] Saved confusion matrix to confusion_matrix.csv\")\n",
    "\n",
    "    pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}).to_csv(\"val_predictions.csv\", index=False)\n",
    "    print(\"[\\u2713] Saved predictions to val_predictions.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48d0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading 50/50 split data...\n",
      "[DEBUG] Loaded ./PSG/SC4001E0.npz → 841 samples\n",
      "[DEBUG] Loaded ./PSG/SC4002E0.npz → 1127 samples\n",
      "[DEBUG] Loaded ./PSG/SC4011E0.npz → 1103 samples\n",
      "[DEBUG] Loaded ./PSG/SC4012E0.npz → 1186 samples\n",
      "[DEBUG] Loaded ./PSG/SC4021E0.npz → 1025 samples\n",
      "[DEBUG] Loaded ./PSG/SC4022E0.npz → 1009 samples\n",
      "[DEBUG] Loaded ./PSG/SC4031E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4032E0.npz → 911 samples\n",
      "[DEBUG] Loaded ./PSG/SC4041E0.npz → 1235 samples\n",
      "[DEBUG] Loaded ./PSG/SC4042E0.npz → 1200 samples\n",
      "[DEBUG] Loaded ./PSG/SC4051E0.npz → 672 samples\n",
      "[DEBUG] Loaded ./PSG/SC4052E0.npz → 1246 samples\n",
      "[DEBUG] Loaded ./PSG/SC4061E0.npz → 843 samples\n",
      "[DEBUG] Loaded ./PSG/SC4062E0.npz → 1016 samples\n",
      "[DEBUG] Loaded ./PSG/SC4071E0.npz → 976 samples\n",
      "[DEBUG] Loaded ./PSG/SC4072E0.npz → 1273 samples\n",
      "[DEBUG] Loaded ./PSG/SC4081E0.npz → 1134 samples\n",
      "[DEBUG] Loaded ./PSG/SC4082E0.npz → 1054 samples\n",
      "[DEBUG] Loaded ./PSG/SC4091E0.npz → 1132 samples\n",
      "[DEBUG] Loaded ./PSG/SC4092E0.npz → 1105 samples\n",
      "[DEBUG] Loaded ./PSG/SC4101E0.npz → 1104 samples\n",
      "[DEBUG] Loaded ./PSG/SC4102E0.npz → 1092 samples\n",
      "[DEBUG] Loaded ./PSG/SC4111E0.npz → 928 samples\n",
      "[DEBUG] Loaded ./PSG/SC4112E0.npz → 802 samples\n",
      "[DEBUG] Loaded ./PSG/SC4121E0.npz → 1052 samples\n",
      "[DEBUG] Loaded ./PSG/SC4122E0.npz → 977 samples\n",
      "[DEBUG] Loaded ./PSG/SC4131E0.npz → 1028 samples\n",
      "[DEBUG] Loaded ./PSG/SC4141E0.npz → 1004 samples\n",
      "[DEBUG] Loaded ./PSG/SC4142E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4151E0.npz → 952 samples\n",
      "[DEBUG] Loaded ./PSG/SC4152E0.npz → 1762 samples\n",
      "[DEBUG] Loaded ./PSG/SC4161E0.npz → 1144 samples\n",
      "[DEBUG] Loaded ./PSG/SC4162E0.npz → 1003 samples\n",
      "[DEBUG] Loaded ./PSG/SC4171E0.npz → 1002 samples\n",
      "[DEBUG] Loaded ./PSG/SC4172E0.npz → 1773 samples\n",
      "[DEBUG] Loaded ./PSG/SC4181E0.npz → 964 samples\n",
      "[DEBUG] Loaded ./PSG/SC4182E0.npz → 920 samples\n",
      "[DEBUG] Loaded ./PSG/SC4191E0.npz → 1535 samples\n",
      "[DEBUG] Loaded ./PSG/SC4192E0.npz → 1274 samples\n",
      "[INFO] DARTS will run on 31731 train samples and 10577 val samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Set random seed\n",
    "set_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. Load data\n",
    "print(\"[INFO] Loading 50/50 split data...\")\n",
    "train_loader, val_loader, num_classes = get_dataloaders_simple(batch_size=32)\n",
    "print(f\"[INFO] DARTS will run on {len(train_loader.dataset.y)} train samples and {len(val_loader.dataset.y)} val samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d3954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running DARTS search with BDP...\n",
      "\n",
      "[Epoch 1/27] Starting...\n",
      "  [Step 000] Loss: 1.5110 | Acc: 0.4688\n",
      "  [Step 001] Loss: 1.6911 | Acc: 0.1250\n",
      "  [Step 002] Loss: 1.6090 | Acc: 0.2812\n",
      "  [Step 003] Loss: 1.4410 | Acc: 0.6250\n",
      "  [Step 004] Loss: 1.5858 | Acc: 0.2500\n",
      "  [Step 005] Loss: 1.4449 | Acc: 0.3750\n",
      "  [Step 006] Loss: 1.3907 | Acc: 0.3750\n",
      "  [Step 007] Loss: 1.3827 | Acc: 0.3125\n",
      "  [Step 008] Loss: 1.2448 | Acc: 0.6250\n",
      "  [Step 009] Loss: 1.3367 | Acc: 0.4688\n",
      "  [Step 010] Loss: 1.2459 | Acc: 0.5938\n",
      "  [Step 011] Loss: 1.1620 | Acc: 0.5625\n",
      "  [Step 012] Loss: 1.1318 | Acc: 0.5625\n",
      "  [Step 013] Loss: 1.5355 | Acc: 0.3750\n",
      "  [Step 014] Loss: 1.2681 | Acc: 0.6562\n",
      "  [Step 015] Loss: 1.2638 | Acc: 0.3750\n",
      "  [Step 016] Loss: 1.4126 | Acc: 0.5000\n",
      "  [Step 017] Loss: 1.3874 | Acc: 0.3125\n",
      "  [Step 018] Loss: 1.0820 | Acc: 0.6250\n",
      "  [Step 019] Loss: 1.4483 | Acc: 0.3438\n",
      "  [Step 020] Loss: 1.2594 | Acc: 0.4062\n",
      "  [Step 021] Loss: 1.3662 | Acc: 0.4375\n",
      "  [Step 022] Loss: 1.1266 | Acc: 0.6562\n",
      "  [Step 023] Loss: 1.0434 | Acc: 0.6250\n",
      "  [Step 024] Loss: 1.5273 | Acc: 0.3125\n",
      "  [Step 025] Loss: 1.6826 | Acc: 0.3125\n",
      "  [Step 026] Loss: 1.4294 | Acc: 0.4375\n",
      "  [Step 027] Loss: 1.6811 | Acc: 0.2500\n",
      "  [Step 028] Loss: 1.0429 | Acc: 0.5625\n",
      "  [Step 029] Loss: 1.1423 | Acc: 0.5312\n",
      "  [Step 030] Loss: 1.4033 | Acc: 0.5625\n",
      "  [Step 031] Loss: 1.0521 | Acc: 0.5625\n",
      "  [Step 032] Loss: 1.4494 | Acc: 0.4062\n",
      "  [Step 033] Loss: 1.0571 | Acc: 0.6250\n",
      "  [Step 034] Loss: 1.1747 | Acc: 0.3750\n",
      "  [Step 035] Loss: 1.3438 | Acc: 0.4062\n",
      "  [Step 036] Loss: 0.9986 | Acc: 0.6562\n",
      "  [Step 037] Loss: 1.1068 | Acc: 0.5625\n",
      "  [Step 038] Loss: 1.2101 | Acc: 0.5000\n",
      "  [Step 039] Loss: 1.2656 | Acc: 0.4375\n",
      "  [Step 040] Loss: 1.2338 | Acc: 0.5312\n",
      "  [Step 041] Loss: 1.5186 | Acc: 0.3750\n",
      "  [Step 042] Loss: 1.2639 | Acc: 0.4062\n",
      "  [Step 043] Loss: 1.2486 | Acc: 0.4688\n",
      "  [Step 044] Loss: 1.3606 | Acc: 0.4062\n",
      "  [Step 045] Loss: 1.2147 | Acc: 0.5000\n",
      "  [Step 046] Loss: 1.2248 | Acc: 0.4375\n",
      "  [Step 047] Loss: 1.1481 | Acc: 0.5000\n",
      "  [Step 048] Loss: 1.3991 | Acc: 0.3125\n",
      "  [Step 049] Loss: 1.1666 | Acc: 0.4375\n",
      "  [Step 050] Loss: 1.3120 | Acc: 0.3438\n",
      "  [Step 051] Loss: 1.5119 | Acc: 0.5312\n",
      "  [Step 052] Loss: 0.9932 | Acc: 0.6875\n",
      "  [Step 053] Loss: 1.0310 | Acc: 0.6250\n",
      "  [Step 054] Loss: 1.4955 | Acc: 0.5312\n",
      "  [Step 055] Loss: 0.9970 | Acc: 0.5938\n",
      "  [Step 056] Loss: 1.3702 | Acc: 0.4375\n",
      "  [Step 057] Loss: 1.3429 | Acc: 0.4375\n",
      "  [Step 058] Loss: 1.2548 | Acc: 0.4688\n",
      "  [Step 059] Loss: 1.3059 | Acc: 0.3750\n",
      "  [Step 060] Loss: 1.0756 | Acc: 0.5312\n",
      "  [Step 061] Loss: 1.1117 | Acc: 0.5625\n",
      "  [Step 062] Loss: 1.2735 | Acc: 0.4062\n",
      "  [Step 063] Loss: 1.2284 | Acc: 0.5000\n",
      "  [Step 064] Loss: 1.3714 | Acc: 0.4062\n",
      "  [Step 065] Loss: 0.9733 | Acc: 0.5938\n",
      "  [Step 066] Loss: 1.1049 | Acc: 0.4688\n",
      "  [Step 067] Loss: 1.3715 | Acc: 0.3438\n",
      "  [Step 068] Loss: 1.2531 | Acc: 0.3750\n",
      "  [Step 069] Loss: 1.4626 | Acc: 0.3438\n",
      "  [Step 070] Loss: 1.0388 | Acc: 0.6562\n",
      "  [Step 071] Loss: 1.2069 | Acc: 0.4062\n",
      "  [Step 072] Loss: 1.3470 | Acc: 0.4375\n",
      "  [Step 073] Loss: 1.5095 | Acc: 0.3750\n",
      "  [Step 074] Loss: 1.3820 | Acc: 0.5000\n",
      "  [Step 075] Loss: 1.2185 | Acc: 0.4688\n",
      "  [Step 076] Loss: 1.6584 | Acc: 0.3125\n",
      "  [Step 077] Loss: 1.9823 | Acc: 0.2188\n",
      "  [Step 078] Loss: 1.4838 | Acc: 0.4375\n",
      "  [Step 079] Loss: 1.3182 | Acc: 0.3750\n",
      "  [Step 080] Loss: 1.1643 | Acc: 0.5625\n",
      "  [Step 081] Loss: 1.3123 | Acc: 0.5625\n",
      "  [Step 082] Loss: 1.2171 | Acc: 0.5938\n",
      "  [Step 083] Loss: 1.0980 | Acc: 0.5000\n",
      "  [Step 084] Loss: 1.3004 | Acc: 0.4688\n",
      "  [Step 085] Loss: 1.6888 | Acc: 0.3125\n",
      "  [Step 086] Loss: 1.4535 | Acc: 0.4062\n",
      "  [Step 087] Loss: 1.3274 | Acc: 0.5000\n",
      "  [Step 088] Loss: 1.3842 | Acc: 0.4688\n",
      "  [Step 089] Loss: 1.3609 | Acc: 0.4375\n",
      "  [Step 090] Loss: 1.5606 | Acc: 0.3750\n",
      "  [Step 091] Loss: 1.2881 | Acc: 0.4375\n",
      "  [Step 092] Loss: 1.2106 | Acc: 0.4688\n",
      "  [Step 093] Loss: 1.3851 | Acc: 0.4375\n",
      "  [Step 094] Loss: 1.4070 | Acc: 0.5000\n",
      "  [Step 095] Loss: 1.1900 | Acc: 0.5312\n",
      "  [Step 096] Loss: 1.2011 | Acc: 0.5625\n",
      "  [Step 097] Loss: 1.3156 | Acc: 0.2812\n",
      "  [Step 098] Loss: 1.3142 | Acc: 0.4062\n",
      "  [Step 099] Loss: 1.2439 | Acc: 0.4688\n",
      "  [Step 100] Loss: 1.2703 | Acc: 0.3750\n",
      "  [Step 101] Loss: 1.1756 | Acc: 0.5312\n",
      "  [Step 102] Loss: 1.1585 | Acc: 0.5938\n",
      "  [Step 103] Loss: 1.2420 | Acc: 0.4688\n",
      "  [Step 104] Loss: 1.0572 | Acc: 0.7188\n",
      "  [Step 105] Loss: 1.0493 | Acc: 0.5312\n",
      "  [Step 106] Loss: 1.2443 | Acc: 0.5938\n",
      "  [Step 107] Loss: 1.1811 | Acc: 0.5938\n",
      "  [Step 108] Loss: 1.0281 | Acc: 0.6875\n",
      "  [Step 109] Loss: 1.0489 | Acc: 0.5625\n",
      "  [Step 110] Loss: 1.1778 | Acc: 0.5312\n",
      "  [Step 111] Loss: 1.4724 | Acc: 0.3750\n",
      "  [Step 112] Loss: 1.3911 | Acc: 0.4062\n",
      "  [Step 113] Loss: 1.0729 | Acc: 0.5625\n",
      "  [Step 114] Loss: 1.0166 | Acc: 0.7188\n",
      "  [Step 115] Loss: 1.1017 | Acc: 0.5938\n",
      "  [Step 116] Loss: 1.2577 | Acc: 0.4688\n",
      "  [Step 117] Loss: 1.1274 | Acc: 0.5625\n",
      "  [Step 118] Loss: 1.0848 | Acc: 0.5625\n",
      "  [Step 119] Loss: 1.0715 | Acc: 0.6562\n",
      "  [Step 120] Loss: 1.0032 | Acc: 0.6562\n",
      "  [Step 121] Loss: 0.8747 | Acc: 0.6875\n",
      "  [Step 122] Loss: 0.9168 | Acc: 0.6250\n",
      "  [Step 123] Loss: 1.5184 | Acc: 0.4688\n",
      "  [Step 124] Loss: 0.8208 | Acc: 0.7500\n",
      "  [Step 125] Loss: 1.0140 | Acc: 0.5312\n",
      "  [Step 126] Loss: 1.0918 | Acc: 0.4688\n",
      "  [Step 127] Loss: 0.9818 | Acc: 0.5312\n",
      "  [Step 128] Loss: 1.1915 | Acc: 0.4375\n",
      "  [Step 129] Loss: 0.9238 | Acc: 0.6562\n",
      "  [Step 130] Loss: 1.9052 | Acc: 0.3125\n",
      "  [Step 131] Loss: 0.9468 | Acc: 0.7188\n",
      "  [Step 132] Loss: 1.0869 | Acc: 0.5312\n",
      "  [Step 133] Loss: 1.1749 | Acc: 0.6875\n",
      "  [Step 134] Loss: 1.1592 | Acc: 0.5938\n",
      "  [Step 135] Loss: 1.1559 | Acc: 0.4688\n",
      "  [Step 136] Loss: 1.2981 | Acc: 0.5000\n",
      "  [Step 137] Loss: 1.1563 | Acc: 0.5625\n",
      "  [Step 138] Loss: 1.1847 | Acc: 0.4688\n",
      "  [Step 139] Loss: 1.1471 | Acc: 0.4688\n",
      "  [Step 140] Loss: 1.5841 | Acc: 0.3438\n",
      "  [Step 141] Loss: 1.3458 | Acc: 0.5312\n",
      "  [Step 142] Loss: 1.1586 | Acc: 0.4688\n",
      "  [Step 143] Loss: 1.2190 | Acc: 0.5312\n",
      "  [Step 144] Loss: 1.4416 | Acc: 0.3438\n",
      "  [Step 145] Loss: 1.2958 | Acc: 0.4375\n",
      "  [Step 146] Loss: 1.0885 | Acc: 0.5625\n",
      "  [Step 147] Loss: 1.1561 | Acc: 0.5000\n",
      "  [Step 148] Loss: 1.1757 | Acc: 0.5625\n",
      "  [Step 149] Loss: 1.1515 | Acc: 0.4688\n",
      "  [Step 150] Loss: 1.2894 | Acc: 0.3750\n",
      "  [Step 151] Loss: 1.2175 | Acc: 0.4062\n",
      "  [Step 152] Loss: 1.1627 | Acc: 0.5000\n",
      "  [Step 153] Loss: 0.9975 | Acc: 0.7500\n",
      "  [Step 154] Loss: 1.0149 | Acc: 0.7188\n",
      "  [Step 155] Loss: 1.3060 | Acc: 0.5000\n",
      "  [Step 156] Loss: 1.0653 | Acc: 0.5938\n",
      "  [Step 157] Loss: 1.0137 | Acc: 0.5938\n",
      "  [Step 158] Loss: 1.3825 | Acc: 0.4062\n",
      "  [Step 159] Loss: 1.1835 | Acc: 0.5938\n",
      "  [Step 160] Loss: 1.2517 | Acc: 0.4062\n",
      "  [Step 161] Loss: 0.9863 | Acc: 0.6875\n",
      "  [Step 162] Loss: 1.1099 | Acc: 0.4688\n",
      "  [Step 163] Loss: 1.0659 | Acc: 0.6250\n",
      "  [Step 164] Loss: 0.8759 | Acc: 0.6250\n",
      "  [Step 165] Loss: 1.0852 | Acc: 0.5625\n",
      "  [Step 166] Loss: 1.3391 | Acc: 0.5000\n",
      "  [Step 167] Loss: 1.0863 | Acc: 0.5938\n",
      "  [Step 168] Loss: 1.1399 | Acc: 0.5312\n",
      "  [Step 169] Loss: 0.9755 | Acc: 0.6562\n",
      "  [Step 170] Loss: 0.9258 | Acc: 0.6875\n",
      "  [Step 171] Loss: 0.9771 | Acc: 0.5312\n",
      "  [Step 172] Loss: 1.0697 | Acc: 0.5625\n",
      "  [Step 173] Loss: 1.0325 | Acc: 0.5000\n",
      "  [Step 174] Loss: 1.1618 | Acc: 0.6250\n",
      "  [Step 175] Loss: 0.8423 | Acc: 0.7188\n",
      "  [Step 176] Loss: 0.9677 | Acc: 0.4688\n",
      "  [Step 177] Loss: 1.3259 | Acc: 0.3750\n",
      "  [Step 178] Loss: 1.4600 | Acc: 0.4375\n",
      "  [Step 179] Loss: 1.0251 | Acc: 0.6875\n",
      "  [Step 180] Loss: 0.8176 | Acc: 0.6562\n",
      "  [Step 181] Loss: 1.2144 | Acc: 0.5312\n",
      "  [Step 182] Loss: 1.4831 | Acc: 0.4062\n",
      "  [Step 183] Loss: 1.0931 | Acc: 0.5000\n",
      "  [Step 184] Loss: 1.0523 | Acc: 0.5000\n",
      "  [Step 185] Loss: 1.2790 | Acc: 0.5625\n",
      "  [Step 186] Loss: 1.2480 | Acc: 0.5000\n",
      "  [Step 187] Loss: 1.3141 | Acc: 0.4688\n",
      "  [Step 188] Loss: 0.8366 | Acc: 0.6562\n",
      "  [Step 189] Loss: 1.6458 | Acc: 0.5625\n",
      "  [Step 190] Loss: 0.7921 | Acc: 0.7188\n",
      "  [Step 191] Loss: 1.3297 | Acc: 0.4062\n",
      "  [Step 192] Loss: 0.9825 | Acc: 0.6562\n",
      "  [Step 193] Loss: 1.2605 | Acc: 0.5000\n",
      "  [Step 194] Loss: 0.9044 | Acc: 0.6250\n",
      "  [Step 195] Loss: 0.9914 | Acc: 0.5625\n",
      "  [Step 196] Loss: 1.1002 | Acc: 0.5625\n",
      "  [Step 197] Loss: 0.9654 | Acc: 0.5938\n",
      "  [Step 198] Loss: 1.1517 | Acc: 0.4688\n",
      "  [Step 199] Loss: 0.9709 | Acc: 0.5938\n",
      "  [Step 200] Loss: 1.6878 | Acc: 0.4375\n",
      "  [Step 201] Loss: 1.1772 | Acc: 0.5312\n",
      "  [Step 202] Loss: 1.0539 | Acc: 0.5938\n",
      "  [Step 203] Loss: 1.2448 | Acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 204] Loss: 0.9853 | Acc: 0.5625\n",
      "  [Step 205] Loss: 0.9651 | Acc: 0.6562\n",
      "  [Step 206] Loss: 0.9857 | Acc: 0.6250\n",
      "  [Step 207] Loss: 1.2731 | Acc: 0.5000\n",
      "  [Step 208] Loss: 1.1585 | Acc: 0.5000\n",
      "  [Step 209] Loss: 1.0533 | Acc: 0.5625\n",
      "  [Step 210] Loss: 1.0168 | Acc: 0.5625\n",
      "  [Step 211] Loss: 1.1847 | Acc: 0.3125\n",
      "  [Step 212] Loss: 0.8887 | Acc: 0.7188\n",
      "  [Step 213] Loss: 1.0430 | Acc: 0.6562\n",
      "  [Step 214] Loss: 1.0698 | Acc: 0.5625\n",
      "  [Step 215] Loss: 1.3146 | Acc: 0.4375\n",
      "  [Step 216] Loss: 1.6129 | Acc: 0.3438\n",
      "  [Step 217] Loss: 1.3905 | Acc: 0.4062\n",
      "  [Step 218] Loss: 0.9440 | Acc: 0.6875\n",
      "  [Step 219] Loss: 1.1626 | Acc: 0.5312\n",
      "  [Step 220] Loss: 1.1555 | Acc: 0.5625\n",
      "  [Step 221] Loss: 1.3042 | Acc: 0.4375\n",
      "  [Step 222] Loss: 1.0982 | Acc: 0.6875\n",
      "  [Step 223] Loss: 1.3437 | Acc: 0.4062\n",
      "  [Step 224] Loss: 1.1960 | Acc: 0.4062\n",
      "  [Step 225] Loss: 1.2634 | Acc: 0.3750\n",
      "  [Step 226] Loss: 1.2906 | Acc: 0.3438\n",
      "  [Step 227] Loss: 1.0891 | Acc: 0.5938\n",
      "  [Step 228] Loss: 1.2838 | Acc: 0.2500\n",
      "  [Step 229] Loss: 1.0632 | Acc: 0.4688\n",
      "  [Step 230] Loss: 1.0780 | Acc: 0.5625\n",
      "  [Step 231] Loss: 1.0096 | Acc: 0.6562\n",
      "  [Step 232] Loss: 0.9632 | Acc: 0.6250\n",
      "  [Step 233] Loss: 1.4481 | Acc: 0.3438\n",
      "  [Step 234] Loss: 1.0568 | Acc: 0.5000\n",
      "  [Step 235] Loss: 1.1973 | Acc: 0.5000\n",
      "  [Step 236] Loss: 0.8095 | Acc: 0.6875\n",
      "  [Step 237] Loss: 1.3395 | Acc: 0.3750\n",
      "  [Step 238] Loss: 1.1166 | Acc: 0.5625\n",
      "  [Step 239] Loss: 0.8995 | Acc: 0.7500\n",
      "  [Step 240] Loss: 1.1748 | Acc: 0.4375\n",
      "  [Step 241] Loss: 1.1036 | Acc: 0.6250\n",
      "  [Step 242] Loss: 0.9028 | Acc: 0.6875\n",
      "  [Step 243] Loss: 0.9689 | Acc: 0.6875\n",
      "  [Step 244] Loss: 0.9105 | Acc: 0.5938\n",
      "  [Step 245] Loss: 1.2895 | Acc: 0.4375\n",
      "  [Step 246] Loss: 0.9779 | Acc: 0.5000\n",
      "  [Step 247] Loss: 0.9830 | Acc: 0.6250\n",
      "  [Step 248] Loss: 0.9550 | Acc: 0.6875\n",
      "  [Step 249] Loss: 1.1752 | Acc: 0.6250\n",
      "  [Step 250] Loss: 0.9127 | Acc: 0.6562\n",
      "  [Step 251] Loss: 1.2022 | Acc: 0.5000\n",
      "  [Step 252] Loss: 1.2020 | Acc: 0.4062\n",
      "  [Step 253] Loss: 0.8575 | Acc: 0.6250\n",
      "  [Step 254] Loss: 0.9285 | Acc: 0.6562\n",
      "  [Step 255] Loss: 1.0341 | Acc: 0.6250\n",
      "  [Step 256] Loss: 1.5026 | Acc: 0.5312\n",
      "  [Step 257] Loss: 0.7364 | Acc: 0.6875\n",
      "  [Step 258] Loss: 1.1077 | Acc: 0.5625\n",
      "  [Step 259] Loss: 1.0721 | Acc: 0.5000\n",
      "  [Step 260] Loss: 1.1472 | Acc: 0.5625\n",
      "  [Step 261] Loss: 1.3433 | Acc: 0.4688\n",
      "  [Step 262] Loss: 1.0269 | Acc: 0.5625\n",
      "  [Step 263] Loss: 1.1747 | Acc: 0.5000\n",
      "  [Step 264] Loss: 0.8387 | Acc: 0.6562\n",
      "  [Step 265] Loss: 1.2496 | Acc: 0.4688\n",
      "  [Step 266] Loss: 1.0805 | Acc: 0.5312\n",
      "  [Step 267] Loss: 1.5814 | Acc: 0.4062\n",
      "  [Step 268] Loss: 1.0558 | Acc: 0.5312\n",
      "  [Step 269] Loss: 1.4118 | Acc: 0.5625\n",
      "  [Step 270] Loss: 1.1704 | Acc: 0.5625\n",
      "  [Step 271] Loss: 1.0729 | Acc: 0.5938\n",
      "  [Step 272] Loss: 1.1416 | Acc: 0.5625\n",
      "  [Step 273] Loss: 0.9860 | Acc: 0.6875\n",
      "  [Step 274] Loss: 0.8127 | Acc: 0.7812\n",
      "  [Step 275] Loss: 1.1046 | Acc: 0.6875\n",
      "  [Step 276] Loss: 1.4605 | Acc: 0.5938\n",
      "  [Step 277] Loss: 1.1141 | Acc: 0.5312\n",
      "  [Step 278] Loss: 0.9957 | Acc: 0.6250\n",
      "  [Step 279] Loss: 1.1893 | Acc: 0.5938\n",
      "  [Step 280] Loss: 1.1085 | Acc: 0.5938\n",
      "  [Step 281] Loss: 0.8704 | Acc: 0.6875\n",
      "  [Step 282] Loss: 1.0170 | Acc: 0.5625\n",
      "  [Step 283] Loss: 0.8820 | Acc: 0.6875\n",
      "  [Step 284] Loss: 1.0179 | Acc: 0.5938\n",
      "  [Step 285] Loss: 1.0935 | Acc: 0.5000\n",
      "  [Step 286] Loss: 1.1548 | Acc: 0.4062\n",
      "  [Step 287] Loss: 0.8992 | Acc: 0.5625\n",
      "  [Step 288] Loss: 0.7238 | Acc: 0.7500\n",
      "  [Step 289] Loss: 0.8222 | Acc: 0.6875\n",
      "  [Step 290] Loss: 0.9598 | Acc: 0.6562\n",
      "  [Step 291] Loss: 0.8568 | Acc: 0.6875\n",
      "  [Step 292] Loss: 1.1345 | Acc: 0.5625\n",
      "  [Step 293] Loss: 0.9162 | Acc: 0.5938\n",
      "  [Step 294] Loss: 0.8379 | Acc: 0.6562\n",
      "  [Step 295] Loss: 0.9115 | Acc: 0.6250\n",
      "  [Step 296] Loss: 0.7903 | Acc: 0.7188\n",
      "  [Step 297] Loss: 1.1957 | Acc: 0.6562\n",
      "  [Step 298] Loss: 0.8591 | Acc: 0.6875\n",
      "  [Step 299] Loss: 1.0959 | Acc: 0.5625\n",
      "  [Step 300] Loss: 1.4806 | Acc: 0.5000\n",
      "  [Step 301] Loss: 0.8885 | Acc: 0.6250\n",
      "  [Step 302] Loss: 0.9624 | Acc: 0.6562\n",
      "  [Step 303] Loss: 0.7015 | Acc: 0.7188\n",
      "  [Step 304] Loss: 1.2877 | Acc: 0.5312\n",
      "  [Step 305] Loss: 0.7517 | Acc: 0.6875\n",
      "  [Step 306] Loss: 1.0302 | Acc: 0.5625\n",
      "  [Step 307] Loss: 0.8457 | Acc: 0.6250\n",
      "  [Step 308] Loss: 0.8950 | Acc: 0.5938\n",
      "  [Step 309] Loss: 0.6694 | Acc: 0.7500\n",
      "  [Step 310] Loss: 1.1681 | Acc: 0.6562\n",
      "  [Step 311] Loss: 0.9913 | Acc: 0.5312\n",
      "  [Step 312] Loss: 0.9702 | Acc: 0.5938\n",
      "  [Step 313] Loss: 1.0404 | Acc: 0.7500\n",
      "  [Step 314] Loss: 0.8639 | Acc: 0.6250\n",
      "  [Step 315] Loss: 0.9696 | Acc: 0.5625\n",
      "  [Step 316] Loss: 1.0880 | Acc: 0.4375\n",
      "  [Step 317] Loss: 0.8401 | Acc: 0.7188\n",
      "  [Step 318] Loss: 1.0919 | Acc: 0.6562\n",
      "  [Step 319] Loss: 1.0676 | Acc: 0.5625\n",
      "  [Step 320] Loss: 0.8589 | Acc: 0.7188\n",
      "  [Step 321] Loss: 0.7082 | Acc: 0.8125\n",
      "  [Step 322] Loss: 1.3948 | Acc: 0.5625\n",
      "  [Step 323] Loss: 0.6463 | Acc: 0.7812\n",
      "  [Step 324] Loss: 1.1423 | Acc: 0.5000\n",
      "  [Step 325] Loss: 1.4964 | Acc: 0.5000\n",
      "  [Step 326] Loss: 1.1523 | Acc: 0.6250\n",
      "  [Step 327] Loss: 1.1739 | Acc: 0.6562\n",
      "  [Step 328] Loss: 0.8947 | Acc: 0.7188\n",
      "  [Step 329] Loss: 0.9318 | Acc: 0.7812\n",
      "  [Step 330] Loss: 0.9880 | Acc: 0.5938\n",
      "[Epoch 1] Train Loss: 0.3887 | Acc: 0.5331 || Val Loss: 1.0298 | Acc: 0.6208\n",
      "Precision: 0.6558 | Recall: 0.5811 | F1: 0.5214 | Time: 1080.94s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.10840584 0.07708858 0.09066308 0.07959373 0.1083402  0.10717862\n",
      "  0.10681802 0.10953403 0.10592744 0.10645043]\n",
      " [0.10511727 0.08676422 0.09195231 0.08583002 0.10500074 0.10533147\n",
      "  0.10535095 0.10512837 0.10386904 0.10565562]\n",
      " [0.10610271 0.08161426 0.09373361 0.08284393 0.1055551  0.10555747\n",
      "  0.10623787 0.10670979 0.10668309 0.10496221]\n",
      " [0.10424723 0.08933033 0.09353394 0.08962851 0.10460027 0.10398373\n",
      "  0.10397322 0.10292883 0.10404147 0.10373249]\n",
      " [0.10324519 0.09252304 0.09425414 0.09277188 0.10283492 0.10309787\n",
      "  0.10252357 0.1033148  0.10335423 0.10208038]\n",
      " [0.10546235 0.08553515 0.09495832 0.08647549 0.10529484 0.10456627\n",
      "  0.10497007 0.10502397 0.10249609 0.10521742]\n",
      " [0.10335225 0.09141777 0.09477027 0.09144156 0.10302105 0.10335824\n",
      "  0.1026208  0.1032466  0.103122   0.10364943]\n",
      " [0.10257132 0.09426411 0.09540344 0.0942116  0.10224452 0.10233966\n",
      "  0.10270859 0.10226312 0.1021208  0.10187288]\n",
      " [0.10317001 0.09237295 0.09415222 0.09286806 0.10284084 0.10316278\n",
      "  0.10277349 0.10259547 0.10295547 0.10310871]\n",
      " [0.10517366 0.08419746 0.09623916 0.08411966 0.10493999 0.10482892\n",
      "  0.10449619 0.1053393  0.1055929  0.1050728 ]\n",
      " [0.10359184 0.09020723 0.09486368 0.09041961 0.10361513 0.10322581\n",
      "  0.10237227 0.10435777 0.1030402  0.10430638]\n",
      " [0.10282987 0.09352499 0.0952411  0.09369498 0.10268012 0.10231923\n",
      "  0.10277679 0.10238593 0.10219244 0.10235456]\n",
      " [0.10318091 0.09209879 0.09385357 0.09217152 0.10298145 0.10298034\n",
      "  0.10253    0.10361803 0.1027808  0.10380462]\n",
      " [0.1040694  0.09024551 0.09219074 0.09040967 0.10433614 0.10360563\n",
      "  0.10386544 0.10449865 0.10317608 0.10360275]\n",
      " [0.1054121  0.08517271 0.09443299 0.08570697 0.10506229 0.10491429\n",
      "  0.10526604 0.10509945 0.10563461 0.10329848]\n",
      " [0.1030632  0.09193946 0.09611671 0.09238549 0.10280526 0.10215387\n",
      "  0.10260251 0.10306675 0.10293969 0.10292702]\n",
      " [0.10253198 0.09425396 0.09540152 0.09451052 0.1025485  0.10213042\n",
      "  0.10260607 0.1025579  0.10158299 0.10187616]\n",
      " [0.10303178 0.09262089 0.09431189 0.09297697 0.10303364 0.10299595\n",
      "  0.1030542  0.10278932 0.10234716 0.10283825]\n",
      " [0.10387006 0.09074884 0.09226937 0.09104687 0.10388712 0.10404148\n",
      "  0.1030535  0.10375754 0.10341799 0.10390726]\n",
      " [0.10438665 0.08867129 0.09050115 0.08898816 0.10359381 0.10430446\n",
      "  0.10431042 0.10403346 0.10611339 0.10509719]\n",
      " [0.1042644  0.08846556 0.0925972  0.08857736 0.10417239 0.10428379\n",
      "  0.1037476  0.10427627 0.10479125 0.10482419]\n",
      " [0.10284983 0.09346016 0.09326313 0.09348807 0.10294843 0.10268076\n",
      "  0.10318443 0.10277794 0.10325392 0.10209333]\n",
      " [0.10198078 0.09463402 0.09551834 0.094713   0.1022972  0.10229397\n",
      "  0.10170623 0.10186048 0.10222581 0.1027702 ]\n",
      " [0.10281051 0.09365127 0.09484949 0.0938605  0.10270873 0.10297214\n",
      "  0.10267144 0.10234152 0.10242799 0.10170634]\n",
      " [0.10341585 0.09154142 0.09273253 0.09157669 0.10323908 0.10338201\n",
      "  0.10357136 0.10337481 0.10382052 0.10334566]\n",
      " [0.1038417  0.08968794 0.09085529 0.08991936 0.10408822 0.10414136\n",
      "  0.10419696 0.1044432  0.10379542 0.10503059]\n",
      " [0.10516771 0.08705197 0.08851516 0.08724508 0.10493898 0.10527576\n",
      "  0.10661153 0.1057471  0.10520358 0.1042432 ]\n",
      " [0.1041319  0.08682466 0.09738395 0.08679512 0.10434097 0.10446949\n",
      "  0.1040801  0.10430222 0.10327692 0.10439466]\n",
      " [0.10248478 0.09269515 0.09694442 0.09181336 0.10229336 0.10243092\n",
      "  0.10314365 0.10218261 0.10359836 0.10241339]\n",
      " [0.10209707 0.0950243  0.09645478 0.09497538 0.1016093  0.10211271\n",
      "  0.10216254 0.10167029 0.10139221 0.10250137]\n",
      " [0.10264228 0.09367562 0.09558847 0.09378999 0.10269199 0.10270037\n",
      "  0.10232145 0.10277704 0.10205088 0.10176188]\n",
      " [0.10282996 0.09203473 0.09426378 0.09234017 0.10336658 0.10278017\n",
      "  0.10324607 0.10311659 0.1024248  0.10359714]\n",
      " [0.10373207 0.09062009 0.09293753 0.09076685 0.10331198 0.10379675\n",
      "  0.10358413 0.10426236 0.10338557 0.10360269]\n",
      " [0.10459686 0.08819135 0.09099505 0.08819956 0.10451744 0.10452698\n",
      "  0.10535742 0.10510017 0.10484564 0.10366956]\n",
      " [0.10556192 0.08591841 0.08884184 0.08589149 0.10532939 0.10568518\n",
      "  0.10534562 0.10717031 0.10584597 0.10440992]]\n",
      "\n",
      "[Epoch 2/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 2.1371 | Acc: 0.6250\n",
      "  [Step 001] Loss: 0.7069 | Acc: 0.6562\n",
      "  [Step 002] Loss: 1.0855 | Acc: 0.7188\n",
      "  [Step 003] Loss: 0.7747 | Acc: 0.6250\n",
      "  [Step 004] Loss: 1.1593 | Acc: 0.6562\n",
      "  [Step 005] Loss: 0.7341 | Acc: 0.6875\n",
      "  [Step 006] Loss: 0.7525 | Acc: 0.6250\n",
      "  [Step 007] Loss: 1.1172 | Acc: 0.5625\n",
      "  [Step 008] Loss: 0.7614 | Acc: 0.7188\n",
      "  [Step 009] Loss: 0.6957 | Acc: 0.6562\n",
      "  [Step 010] Loss: 0.8032 | Acc: 0.6562\n",
      "  [Step 011] Loss: 0.7873 | Acc: 0.7188\n",
      "  [Step 012] Loss: 0.8711 | Acc: 0.7500\n",
      "  [Step 013] Loss: 0.8290 | Acc: 0.6250\n",
      "  [Step 014] Loss: 1.0345 | Acc: 0.4688\n",
      "  [Step 015] Loss: 0.8826 | Acc: 0.5625\n",
      "  [Step 016] Loss: 0.8321 | Acc: 0.6875\n",
      "  [Step 017] Loss: 0.7020 | Acc: 0.7188\n",
      "  [Step 018] Loss: 0.9260 | Acc: 0.6250\n",
      "  [Step 019] Loss: 0.8277 | Acc: 0.7188\n",
      "  [Step 020] Loss: 0.6818 | Acc: 0.6562\n",
      "  [Step 021] Loss: 0.7442 | Acc: 0.7812\n",
      "  [Step 022] Loss: 0.8156 | Acc: 0.6875\n",
      "  [Step 023] Loss: 1.6198 | Acc: 0.5312\n",
      "  [Step 024] Loss: 0.9145 | Acc: 0.5938\n",
      "  [Step 025] Loss: 1.0371 | Acc: 0.6562\n",
      "  [Step 026] Loss: 1.0649 | Acc: 0.5312\n",
      "  [Step 027] Loss: 0.9721 | Acc: 0.6250\n",
      "  [Step 028] Loss: 1.0115 | Acc: 0.6250\n",
      "  [Step 029] Loss: 1.0882 | Acc: 0.6562\n",
      "  [Step 030] Loss: 1.2382 | Acc: 0.6250\n",
      "  [Step 031] Loss: 1.0996 | Acc: 0.5625\n",
      "  [Step 032] Loss: 0.9211 | Acc: 0.6562\n",
      "  [Step 033] Loss: 0.9680 | Acc: 0.6562\n",
      "  [Step 034] Loss: 1.3031 | Acc: 0.5312\n",
      "  [Step 035] Loss: 0.9366 | Acc: 0.6562\n",
      "  [Step 036] Loss: 0.9287 | Acc: 0.7188\n",
      "  [Step 037] Loss: 1.1406 | Acc: 0.5938\n",
      "  [Step 038] Loss: 1.2307 | Acc: 0.5000\n",
      "  [Step 039] Loss: 1.2596 | Acc: 0.7188\n",
      "  [Step 040] Loss: 0.9009 | Acc: 0.6875\n",
      "  [Step 041] Loss: 1.4131 | Acc: 0.4062\n",
      "  [Step 042] Loss: 1.0463 | Acc: 0.6250\n",
      "  [Step 043] Loss: 1.0531 | Acc: 0.5000\n",
      "  [Step 044] Loss: 1.1718 | Acc: 0.5000\n",
      "  [Step 045] Loss: 1.0234 | Acc: 0.6562\n",
      "  [Step 046] Loss: 0.9857 | Acc: 0.5625\n",
      "  [Step 047] Loss: 1.0069 | Acc: 0.6562\n",
      "  [Step 048] Loss: 0.9542 | Acc: 0.7500\n",
      "  [Step 049] Loss: 1.1190 | Acc: 0.5625\n",
      "  [Step 050] Loss: 1.1125 | Acc: 0.5312\n",
      "  [Step 051] Loss: 0.9653 | Acc: 0.6875\n",
      "  [Step 052] Loss: 0.7495 | Acc: 0.7812\n",
      "  [Step 053] Loss: 1.0006 | Acc: 0.6562\n",
      "  [Step 054] Loss: 0.8902 | Acc: 0.5312\n",
      "  [Step 055] Loss: 0.6659 | Acc: 0.7812\n",
      "  [Step 056] Loss: 1.0614 | Acc: 0.5000\n",
      "  [Step 057] Loss: 1.1044 | Acc: 0.4688\n",
      "  [Step 058] Loss: 1.0026 | Acc: 0.5938\n",
      "  [Step 059] Loss: 0.9492 | Acc: 0.7812\n",
      "  [Step 060] Loss: 0.9678 | Acc: 0.5938\n",
      "  [Step 061] Loss: 0.9776 | Acc: 0.5938\n",
      "  [Step 062] Loss: 0.8000 | Acc: 0.6562\n",
      "  [Step 063] Loss: 0.9139 | Acc: 0.6250\n",
      "  [Step 064] Loss: 0.6599 | Acc: 0.8125\n",
      "  [Step 065] Loss: 0.6589 | Acc: 0.7812\n",
      "  [Step 066] Loss: 0.7377 | Acc: 0.6875\n",
      "  [Step 067] Loss: 0.7515 | Acc: 0.6875\n",
      "  [Step 068] Loss: 0.8085 | Acc: 0.6562\n",
      "  [Step 069] Loss: 0.7670 | Acc: 0.6562\n",
      "  [Step 070] Loss: 0.8318 | Acc: 0.6875\n",
      "  [Step 071] Loss: 0.8434 | Acc: 0.6562\n",
      "  [Step 072] Loss: 0.6909 | Acc: 0.7500\n",
      "  [Step 073] Loss: 0.8274 | Acc: 0.7812\n",
      "  [Step 074] Loss: 0.9704 | Acc: 0.5938\n",
      "  [Step 075] Loss: 0.9725 | Acc: 0.5625\n",
      "  [Step 076] Loss: 0.6122 | Acc: 0.6875\n",
      "  [Step 077] Loss: 0.6952 | Acc: 0.7188\n",
      "  [Step 078] Loss: 0.4058 | Acc: 0.8438\n",
      "  [Step 079] Loss: 0.8398 | Acc: 0.7188\n",
      "  [Step 080] Loss: 1.0927 | Acc: 0.7812\n",
      "  [Step 081] Loss: 0.6255 | Acc: 0.7500\n",
      "  [Step 082] Loss: 0.7679 | Acc: 0.6250\n",
      "  [Step 083] Loss: 0.8450 | Acc: 0.7500\n",
      "  [Step 084] Loss: 0.7793 | Acc: 0.7500\n",
      "  [Step 085] Loss: 0.9974 | Acc: 0.5000\n",
      "  [Step 086] Loss: 0.9486 | Acc: 0.5938\n",
      "  [Step 087] Loss: 1.2317 | Acc: 0.6250\n",
      "  [Step 088] Loss: 1.1772 | Acc: 0.5000\n",
      "  [Step 089] Loss: 1.1403 | Acc: 0.5625\n",
      "  [Step 090] Loss: 1.0687 | Acc: 0.6562\n",
      "  [Step 091] Loss: 0.7723 | Acc: 0.6250\n",
      "  [Step 092] Loss: 0.5875 | Acc: 0.9062\n",
      "  [Step 093] Loss: 0.8658 | Acc: 0.7500\n",
      "  [Step 094] Loss: 0.7702 | Acc: 0.7500\n",
      "  [Step 095] Loss: 0.6576 | Acc: 0.7188\n",
      "  [Step 096] Loss: 0.5986 | Acc: 0.8125\n",
      "  [Step 097] Loss: 1.0983 | Acc: 0.5938\n",
      "  [Step 098] Loss: 1.2763 | Acc: 0.4688\n",
      "  [Step 099] Loss: 0.7307 | Acc: 0.7188\n",
      "  [Step 100] Loss: 0.8109 | Acc: 0.5938\n",
      "  [Step 101] Loss: 1.1601 | Acc: 0.5312\n",
      "  [Step 102] Loss: 1.0140 | Acc: 0.5625\n",
      "  [Step 103] Loss: 0.9044 | Acc: 0.6250\n",
      "  [Step 104] Loss: 1.6132 | Acc: 0.3750\n",
      "  [Step 105] Loss: 1.0898 | Acc: 0.5625\n",
      "  [Step 106] Loss: 0.7749 | Acc: 0.7188\n",
      "  [Step 107] Loss: 0.9799 | Acc: 0.5312\n",
      "  [Step 108] Loss: 1.4156 | Acc: 0.4688\n",
      "  [Step 109] Loss: 1.5640 | Acc: 0.4688\n",
      "  [Step 110] Loss: 1.2484 | Acc: 0.4688\n",
      "  [Step 111] Loss: 0.9391 | Acc: 0.6875\n",
      "  [Step 112] Loss: 1.0768 | Acc: 0.5938\n",
      "  [Step 113] Loss: 1.1700 | Acc: 0.4688\n",
      "  [Step 114] Loss: 1.3485 | Acc: 0.4688\n",
      "  [Step 115] Loss: 1.0373 | Acc: 0.6562\n",
      "  [Step 116] Loss: 1.1335 | Acc: 0.5938\n",
      "  [Step 117] Loss: 1.2071 | Acc: 0.4688\n",
      "  [Step 118] Loss: 0.9673 | Acc: 0.5938\n",
      "  [Step 119] Loss: 0.9847 | Acc: 0.5625\n",
      "  [Step 120] Loss: 1.2731 | Acc: 0.4688\n",
      "  [Step 121] Loss: 1.0828 | Acc: 0.5938\n",
      "  [Step 122] Loss: 1.1358 | Acc: 0.6250\n",
      "  [Step 123] Loss: 0.8122 | Acc: 0.6250\n",
      "  [Step 124] Loss: 1.0600 | Acc: 0.6562\n",
      "  [Step 125] Loss: 0.9593 | Acc: 0.5312\n",
      "  [Step 126] Loss: 0.9190 | Acc: 0.6562\n",
      "  [Step 127] Loss: 1.2030 | Acc: 0.5312\n",
      "  [Step 128] Loss: 0.6871 | Acc: 0.7188\n",
      "  [Step 129] Loss: 1.0152 | Acc: 0.5000\n",
      "  [Step 130] Loss: 0.8430 | Acc: 0.6562\n",
      "  [Step 131] Loss: 0.8611 | Acc: 0.6562\n",
      "  [Step 132] Loss: 1.0662 | Acc: 0.5938\n",
      "  [Step 133] Loss: 0.7542 | Acc: 0.7500\n",
      "  [Step 134] Loss: 0.7720 | Acc: 0.7500\n",
      "  [Step 135] Loss: 1.0017 | Acc: 0.7500\n",
      "  [Step 136] Loss: 0.7369 | Acc: 0.6875\n",
      "  [Step 137] Loss: 0.8383 | Acc: 0.6875\n",
      "  [Step 138] Loss: 0.9322 | Acc: 0.6875\n",
      "  [Step 139] Loss: 0.9808 | Acc: 0.5625\n",
      "  [Step 140] Loss: 0.7718 | Acc: 0.6875\n",
      "  [Step 141] Loss: 1.3187 | Acc: 0.5625\n",
      "  [Step 142] Loss: 0.8633 | Acc: 0.6875\n",
      "  [Step 143] Loss: 0.7371 | Acc: 0.7188\n",
      "  [Step 144] Loss: 0.9452 | Acc: 0.7188\n",
      "  [Step 145] Loss: 1.0592 | Acc: 0.6250\n",
      "  [Step 146] Loss: 1.1985 | Acc: 0.4375\n",
      "  [Step 147] Loss: 0.7652 | Acc: 0.6875\n",
      "  [Step 148] Loss: 0.9669 | Acc: 0.6875\n",
      "  [Step 149] Loss: 0.8170 | Acc: 0.5938\n",
      "  [Step 150] Loss: 0.7693 | Acc: 0.6875\n",
      "  [Step 151] Loss: 1.1531 | Acc: 0.5000\n",
      "  [Step 152] Loss: 0.6923 | Acc: 0.7812\n",
      "  [Step 153] Loss: 0.7161 | Acc: 0.6875\n",
      "  [Step 154] Loss: 1.1004 | Acc: 0.5625\n",
      "  [Step 155] Loss: 0.6532 | Acc: 0.7812\n",
      "  [Step 156] Loss: 0.8872 | Acc: 0.6250\n",
      "  [Step 157] Loss: 0.9572 | Acc: 0.5938\n",
      "  [Step 158] Loss: 0.9373 | Acc: 0.6562\n",
      "  [Step 159] Loss: 0.4635 | Acc: 0.8125\n",
      "  [Step 160] Loss: 0.6098 | Acc: 0.7812\n",
      "  [Step 161] Loss: 0.7737 | Acc: 0.7500\n",
      "  [Step 162] Loss: 0.9665 | Acc: 0.6562\n",
      "  [Step 163] Loss: 0.6827 | Acc: 0.7188\n",
      "  [Step 164] Loss: 0.9935 | Acc: 0.5625\n",
      "  [Step 165] Loss: 1.8171 | Acc: 0.6875\n",
      "  [Step 166] Loss: 0.6821 | Acc: 0.7188\n",
      "  [Step 167] Loss: 0.9444 | Acc: 0.6250\n",
      "  [Step 168] Loss: 0.8612 | Acc: 0.7188\n",
      "  [Step 169] Loss: 0.7579 | Acc: 0.6562\n",
      "  [Step 170] Loss: 0.6884 | Acc: 0.8125\n",
      "  [Step 171] Loss: 0.9648 | Acc: 0.6562\n",
      "  [Step 172] Loss: 0.6738 | Acc: 0.7812\n",
      "  [Step 173] Loss: 1.0812 | Acc: 0.6562\n",
      "  [Step 174] Loss: 1.0408 | Acc: 0.5625\n",
      "  [Step 175] Loss: 0.8243 | Acc: 0.7188\n",
      "  [Step 176] Loss: 0.7128 | Acc: 0.7500\n",
      "  [Step 177] Loss: 1.0047 | Acc: 0.5938\n",
      "  [Step 178] Loss: 0.6288 | Acc: 0.7188\n",
      "  [Step 179] Loss: 1.0387 | Acc: 0.6250\n",
      "  [Step 180] Loss: 0.8530 | Acc: 0.6562\n",
      "  [Step 181] Loss: 0.7353 | Acc: 0.7812\n",
      "  [Step 182] Loss: 1.0725 | Acc: 0.5625\n",
      "  [Step 183] Loss: 0.9248 | Acc: 0.6562\n",
      "  [Step 184] Loss: 0.9059 | Acc: 0.6250\n",
      "  [Step 185] Loss: 0.8806 | Acc: 0.7500\n",
      "  [Step 186] Loss: 0.7249 | Acc: 0.7188\n",
      "  [Step 187] Loss: 0.7869 | Acc: 0.7188\n",
      "  [Step 188] Loss: 0.8539 | Acc: 0.6562\n",
      "  [Step 189] Loss: 0.7510 | Acc: 0.7188\n",
      "  [Step 190] Loss: 0.8126 | Acc: 0.5938\n",
      "  [Step 191] Loss: 0.4633 | Acc: 0.8125\n",
      "  [Step 192] Loss: 0.6991 | Acc: 0.7500\n",
      "  [Step 193] Loss: 0.7387 | Acc: 0.8438\n",
      "  [Step 194] Loss: 0.9461 | Acc: 0.5938\n",
      "  [Step 195] Loss: 0.7862 | Acc: 0.6562\n",
      "  [Step 196] Loss: 1.2357 | Acc: 0.5625\n",
      "  [Step 197] Loss: 0.6604 | Acc: 0.8438\n",
      "  [Step 198] Loss: 1.1593 | Acc: 0.4688\n",
      "  [Step 199] Loss: 1.1163 | Acc: 0.5625\n",
      "  [Step 200] Loss: 1.0175 | Acc: 0.6250\n",
      "  [Step 201] Loss: 0.8609 | Acc: 0.6875\n",
      "  [Step 202] Loss: 0.8778 | Acc: 0.6562\n",
      "  [Step 203] Loss: 0.6921 | Acc: 0.7812\n",
      "  [Step 204] Loss: 0.9569 | Acc: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 1.2275 | Acc: 0.5625\n",
      "  [Step 206] Loss: 0.7924 | Acc: 0.5938\n",
      "  [Step 207] Loss: 1.0251 | Acc: 0.6250\n",
      "  [Step 208] Loss: 0.8469 | Acc: 0.6250\n",
      "  [Step 209] Loss: 0.8677 | Acc: 0.6250\n",
      "  [Step 210] Loss: 1.4360 | Acc: 0.4688\n",
      "  [Step 211] Loss: 1.3553 | Acc: 0.5625\n",
      "  [Step 212] Loss: 1.0047 | Acc: 0.5938\n",
      "  [Step 213] Loss: 1.0088 | Acc: 0.6250\n",
      "  [Step 214] Loss: 0.9857 | Acc: 0.5938\n",
      "  [Step 215] Loss: 0.9435 | Acc: 0.5000\n",
      "  [Step 216] Loss: 0.9178 | Acc: 0.6562\n",
      "  [Step 217] Loss: 1.2286 | Acc: 0.4688\n",
      "  [Step 218] Loss: 1.0072 | Acc: 0.5312\n",
      "  [Step 219] Loss: 0.8894 | Acc: 0.7188\n",
      "  [Step 220] Loss: 0.9891 | Acc: 0.5938\n",
      "  [Step 221] Loss: 0.8701 | Acc: 0.7188\n",
      "  [Step 222] Loss: 0.6740 | Acc: 0.7812\n",
      "  [Step 223] Loss: 0.9275 | Acc: 0.6562\n",
      "  [Step 224] Loss: 1.0161 | Acc: 0.6562\n",
      "  [Step 225] Loss: 1.0837 | Acc: 0.6562\n",
      "  [Step 226] Loss: 1.0858 | Acc: 0.5000\n",
      "  [Step 227] Loss: 1.2749 | Acc: 0.4062\n",
      "  [Step 228] Loss: 1.4103 | Acc: 0.4062\n",
      "  [Step 229] Loss: 0.9567 | Acc: 0.6562\n",
      "  [Step 230] Loss: 0.9500 | Acc: 0.6875\n",
      "  [Step 231] Loss: 1.1552 | Acc: 0.5312\n",
      "  [Step 232] Loss: 0.9525 | Acc: 0.5625\n",
      "  [Step 233] Loss: 0.8154 | Acc: 0.6875\n",
      "  [Step 234] Loss: 1.0392 | Acc: 0.5938\n",
      "  [Step 235] Loss: 1.1249 | Acc: 0.5312\n",
      "  [Step 236] Loss: 1.2620 | Acc: 0.5000\n",
      "  [Step 237] Loss: 1.1976 | Acc: 0.4375\n",
      "  [Step 238] Loss: 0.7064 | Acc: 0.6562\n",
      "  [Step 239] Loss: 0.9739 | Acc: 0.6250\n",
      "  [Step 240] Loss: 1.2947 | Acc: 0.5000\n",
      "  [Step 241] Loss: 1.3676 | Acc: 0.4688\n",
      "  [Step 242] Loss: 0.9353 | Acc: 0.5625\n",
      "  [Step 243] Loss: 1.3599 | Acc: 0.4688\n",
      "  [Step 244] Loss: 0.9523 | Acc: 0.6250\n",
      "  [Step 245] Loss: 1.0045 | Acc: 0.5625\n",
      "  [Step 246] Loss: 1.0278 | Acc: 0.5625\n",
      "  [Step 247] Loss: 0.9760 | Acc: 0.5938\n",
      "  [Step 248] Loss: 0.9229 | Acc: 0.5312\n",
      "  [Step 249] Loss: 1.0813 | Acc: 0.5312\n",
      "  [Step 250] Loss: 1.1345 | Acc: 0.5938\n",
      "  [Step 251] Loss: 0.8391 | Acc: 0.6250\n",
      "  [Step 252] Loss: 0.9599 | Acc: 0.5625\n",
      "  [Step 253] Loss: 0.6916 | Acc: 0.6875\n",
      "  [Step 254] Loss: 0.8990 | Acc: 0.6875\n",
      "  [Step 255] Loss: 1.3898 | Acc: 0.4688\n",
      "  [Step 256] Loss: 0.9713 | Acc: 0.5938\n",
      "  [Step 257] Loss: 0.5512 | Acc: 0.7188\n",
      "  [Step 258] Loss: 1.1663 | Acc: 0.4688\n",
      "  [Step 259] Loss: 0.9005 | Acc: 0.6562\n",
      "  [Step 260] Loss: 1.0641 | Acc: 0.5938\n",
      "  [Step 261] Loss: 0.8760 | Acc: 0.6562\n",
      "  [Step 262] Loss: 0.7055 | Acc: 0.6875\n",
      "  [Step 263] Loss: 1.1930 | Acc: 0.5625\n",
      "  [Step 264] Loss: 0.7466 | Acc: 0.7812\n",
      "  [Step 265] Loss: 0.7162 | Acc: 0.8125\n",
      "  [Step 266] Loss: 0.5466 | Acc: 0.8438\n",
      "  [Step 267] Loss: 0.8956 | Acc: 0.6562\n",
      "  [Step 268] Loss: 0.6596 | Acc: 0.8438\n",
      "  [Step 269] Loss: 0.9501 | Acc: 0.6875\n",
      "  [Step 270] Loss: 0.9222 | Acc: 0.6250\n",
      "  [Step 271] Loss: 0.8072 | Acc: 0.6562\n",
      "  [Step 272] Loss: 0.8277 | Acc: 0.6562\n",
      "  [Step 273] Loss: 0.8935 | Acc: 0.6562\n",
      "  [Step 274] Loss: 0.9089 | Acc: 0.5938\n",
      "  [Step 275] Loss: 1.0143 | Acc: 0.5938\n",
      "  [Step 276] Loss: 0.8577 | Acc: 0.6250\n",
      "  [Step 277] Loss: 0.7468 | Acc: 0.7812\n",
      "  [Step 278] Loss: 1.1517 | Acc: 0.4688\n",
      "  [Step 279] Loss: 0.6887 | Acc: 0.7812\n",
      "  [Step 280] Loss: 0.7596 | Acc: 0.6875\n",
      "  [Step 281] Loss: 0.7294 | Acc: 0.6875\n",
      "  [Step 282] Loss: 0.7149 | Acc: 0.6875\n",
      "  [Step 283] Loss: 0.7118 | Acc: 0.7188\n",
      "  [Step 284] Loss: 0.7698 | Acc: 0.7500\n",
      "  [Step 285] Loss: 0.5436 | Acc: 0.7500\n",
      "  [Step 286] Loss: 0.8971 | Acc: 0.5938\n",
      "  [Step 287] Loss: 0.7067 | Acc: 0.7812\n",
      "  [Step 288] Loss: 0.7364 | Acc: 0.7188\n",
      "  [Step 289] Loss: 0.8077 | Acc: 0.5938\n",
      "  [Step 290] Loss: 0.7956 | Acc: 0.6875\n",
      "  [Step 291] Loss: 0.6414 | Acc: 0.7500\n",
      "  [Step 292] Loss: 0.9970 | Acc: 0.6250\n",
      "  [Step 293] Loss: 0.9184 | Acc: 0.5938\n",
      "  [Step 294] Loss: 0.7735 | Acc: 0.6875\n",
      "  [Step 295] Loss: 0.8561 | Acc: 0.7188\n",
      "  [Step 296] Loss: 0.9581 | Acc: 0.6250\n",
      "  [Step 297] Loss: 0.7372 | Acc: 0.7500\n",
      "  [Step 298] Loss: 0.6924 | Acc: 0.7812\n",
      "  [Step 299] Loss: 0.8877 | Acc: 0.6875\n",
      "  [Step 300] Loss: 0.4605 | Acc: 0.8750\n",
      "  [Step 301] Loss: 0.5683 | Acc: 0.8125\n",
      "  [Step 302] Loss: 0.6892 | Acc: 0.7812\n",
      "  [Step 303] Loss: 0.6616 | Acc: 0.6562\n",
      "  [Step 304] Loss: 1.2062 | Acc: 0.6875\n",
      "  [Step 305] Loss: 0.6998 | Acc: 0.7188\n",
      "  [Step 306] Loss: 1.0820 | Acc: 0.5938\n",
      "  [Step 307] Loss: 0.7150 | Acc: 0.7500\n",
      "  [Step 308] Loss: 0.4538 | Acc: 0.8438\n",
      "  [Step 309] Loss: 0.8417 | Acc: 0.7188\n",
      "  [Step 310] Loss: 0.6262 | Acc: 0.7812\n",
      "  [Step 311] Loss: 0.7176 | Acc: 0.6250\n",
      "  [Step 312] Loss: 0.5932 | Acc: 0.8125\n",
      "  [Step 313] Loss: 0.9942 | Acc: 0.5000\n",
      "  [Step 314] Loss: 0.6759 | Acc: 0.7188\n",
      "  [Step 315] Loss: 0.7088 | Acc: 0.7812\n",
      "  [Step 316] Loss: 0.8174 | Acc: 0.7188\n",
      "  [Step 317] Loss: 0.7244 | Acc: 0.7500\n",
      "  [Step 318] Loss: 0.8017 | Acc: 0.7500\n",
      "  [Step 319] Loss: 1.1974 | Acc: 0.5000\n",
      "  [Step 320] Loss: 1.0561 | Acc: 0.4688\n",
      "  [Step 321] Loss: 0.7357 | Acc: 0.6562\n",
      "  [Step 322] Loss: 0.6651 | Acc: 0.6562\n",
      "  [Step 323] Loss: 0.6718 | Acc: 0.7188\n",
      "  [Step 324] Loss: 0.9670 | Acc: 0.5625\n",
      "  [Step 325] Loss: 1.1163 | Acc: 0.5000\n",
      "  [Step 326] Loss: 0.8671 | Acc: 0.5938\n",
      "  [Step 327] Loss: 0.6865 | Acc: 0.7812\n",
      "  [Step 328] Loss: 1.0633 | Acc: 0.7500\n",
      "  [Step 329] Loss: 0.8384 | Acc: 0.6875\n",
      "  [Step 330] Loss: 1.0002 | Acc: 0.6250\n",
      "[Epoch 2] Train Loss: 0.3083 | Acc: 0.6446 || Val Loss: 0.8682 | Acc: 0.6298\n",
      "Precision: 0.5783 | Recall: 0.4996 | F1: 0.4771 | Time: 1077.08s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.11253954 0.06598399 0.09156881 0.06932908 0.1119293  0.10973619\n",
      "  0.11317289 0.11310359 0.105088   0.10754858]\n",
      " [0.11162895 0.07449232 0.08216232 0.07198899 0.10931385 0.11245632\n",
      "  0.10909925 0.10918733 0.1080635  0.11160717]\n",
      " [0.10868978 0.07165395 0.09605079 0.07269687 0.10792359 0.10802537\n",
      "  0.10980205 0.1092355  0.10999182 0.10593028]\n",
      " [0.10942518 0.07864088 0.08468114 0.07865374 0.10981972 0.10855451\n",
      "  0.10839511 0.10411303 0.10849448 0.10922223]\n",
      " [0.10608437 0.08710752 0.0902354  0.0875885  0.10427297 0.10449131\n",
      "  0.10554477 0.105422   0.10562826 0.1036249 ]\n",
      " [0.10850476 0.07731365 0.09733209 0.07810549 0.10720614 0.10685818\n",
      "  0.10696703 0.10731935 0.10270273 0.10769056]\n",
      " [0.1075017  0.08204632 0.08742744 0.08152431 0.10708947 0.10699973\n",
      "  0.10538259 0.10650889 0.10750119 0.10801832]\n",
      " [0.10462169 0.08945298 0.09182926 0.0896086  0.10461252 0.10363959\n",
      "  0.10500585 0.10337732 0.10384619 0.10400594]\n",
      " [0.10596448 0.08724288 0.0901389  0.08785988 0.10459599 0.10512933\n",
      "  0.10450236 0.10440031 0.10550033 0.10466548]\n",
      " [0.10904118 0.07425892 0.09696487 0.07326099 0.10833502 0.1080781\n",
      "  0.10572185 0.10772653 0.10824219 0.10837037]\n",
      " [0.10766653 0.08073912 0.08778137 0.0810371  0.10706951 0.1077184\n",
      "  0.10412597 0.10877658 0.10658969 0.10849572]\n",
      " [0.10539794 0.08842841 0.09132577 0.08845133 0.10492975 0.10356609\n",
      "  0.10527891 0.10565314 0.10381944 0.10314932]\n",
      " [0.10562705 0.0859054  0.08885921 0.08593255 0.10508294 0.1052577\n",
      "  0.10520066 0.1055774  0.10558655 0.1069705 ]\n",
      " [0.10760951 0.0825869  0.08605503 0.08280016 0.1067819  0.10708818\n",
      "  0.10641751 0.10816623 0.10579153 0.10670302]\n",
      " [0.10864729 0.07579654 0.09616747 0.07486803 0.10743987 0.10831179\n",
      "  0.10768811 0.10688747 0.10939199 0.10480143]\n",
      " [0.10668646 0.08264663 0.08988306 0.08266439 0.10700222 0.10504253\n",
      "  0.1062187  0.10870513 0.10510267 0.10604819]\n",
      " [0.10476924 0.0897254  0.09214194 0.08981583 0.10366097 0.10412899\n",
      "  0.1044193  0.10369002 0.10388505 0.10376329]\n",
      " [0.1054085  0.08688237 0.08993347 0.08706639 0.10431889 0.10566798\n",
      "  0.10532391 0.1046399  0.10514295 0.10561562]\n",
      " [0.10722265 0.08374432 0.08692327 0.08400665 0.10703859 0.1067645\n",
      "  0.10503801 0.10736518 0.10509343 0.10680332]\n",
      " [0.10763123 0.08029925 0.08353935 0.08046561 0.10584855 0.10721884\n",
      "  0.10627323 0.10657234 0.11093301 0.11121864]\n",
      " [0.10741906 0.07949009 0.08943553 0.07800963 0.10712583 0.10726625\n",
      "  0.10786688 0.10828624 0.10861066 0.10648982]\n",
      " [0.1067267  0.08529448 0.08530851 0.08516674 0.10672397 0.10656878\n",
      "  0.10710715 0.1049611  0.10699783 0.10514477]\n",
      " [0.10414413 0.09024075 0.09229803 0.09011235 0.10465942 0.10314664\n",
      "  0.10374845 0.10327617 0.10315898 0.10521506]\n",
      " [0.10534215 0.08795395 0.09025835 0.08798632 0.10466871 0.10550034\n",
      "  0.10513298 0.10461123 0.10453202 0.104014  ]\n",
      " [0.10614085 0.08418976 0.08707711 0.08409133 0.10672243 0.10559615\n",
      "  0.10718381 0.10607693 0.10791429 0.10500741]\n",
      " [0.10748439 0.0815686  0.0842851  0.08161616 0.10739601 0.10707466\n",
      "  0.10719039 0.10678519 0.10633513 0.11026441]\n",
      " [0.10890467 0.0775108  0.08061065 0.07763579 0.10859241 0.10864212\n",
      "  0.11156887 0.10982879 0.10796026 0.10874566]\n",
      " [0.10723884 0.07669772 0.09936105 0.07540551 0.10816304 0.10713064\n",
      "  0.10582411 0.1067301  0.10603413 0.1074148 ]\n",
      " [0.10615098 0.08389125 0.0905631  0.08290219 0.10593762 0.10565437\n",
      "  0.10660681 0.10434006 0.1077721  0.10618146]\n",
      " [0.10430603 0.09035816 0.09350345 0.09006937 0.10330479 0.10371678\n",
      "  0.10449225 0.10315638 0.10207402 0.10501885]\n",
      " [0.10521453 0.08814766 0.0918875  0.08798846 0.10546627 0.10499069\n",
      "  0.10487756 0.10421771 0.10457204 0.10263765]\n",
      " [0.10555577 0.08506656 0.08944681 0.08500913 0.10595939 0.10494339\n",
      "  0.10528888 0.10596768 0.10611945 0.10664295]\n",
      " [0.10707089 0.08277501 0.08739223 0.08257942 0.10622885 0.10678807\n",
      "  0.10644427 0.10787015 0.10679063 0.1060604 ]\n",
      " [0.1084122  0.07910553 0.08435421 0.07885873 0.10813399 0.10795391\n",
      "  0.10769536 0.10965622 0.10875966 0.10707023]\n",
      " [0.1096573  0.07537512 0.08086986 0.0750657  0.109667   0.10960181\n",
      "  0.10924568 0.11350898 0.10867711 0.1083314 ]]\n",
      "\n",
      "[Epoch 3/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 2.9531 | Acc: 0.5000\n",
      "  [Step 001] Loss: 1.1821 | Acc: 0.5312\n",
      "  [Step 002] Loss: 0.9799 | Acc: 0.6875\n",
      "  [Step 003] Loss: 0.7580 | Acc: 0.7188\n",
      "  [Step 004] Loss: 0.9939 | Acc: 0.5938\n",
      "  [Step 005] Loss: 0.9512 | Acc: 0.5625\n",
      "  [Step 006] Loss: 0.6557 | Acc: 0.7812\n",
      "  [Step 007] Loss: 0.8379 | Acc: 0.6562\n",
      "  [Step 008] Loss: 0.8892 | Acc: 0.6562\n",
      "  [Step 009] Loss: 0.7287 | Acc: 0.6875\n",
      "  [Step 010] Loss: 0.9066 | Acc: 0.6875\n",
      "  [Step 011] Loss: 0.8788 | Acc: 0.7188\n",
      "  [Step 012] Loss: 0.9314 | Acc: 0.7188\n",
      "  [Step 013] Loss: 0.6285 | Acc: 0.6875\n",
      "  [Step 014] Loss: 0.8765 | Acc: 0.5625\n",
      "  [Step 015] Loss: 1.1068 | Acc: 0.4375\n",
      "  [Step 016] Loss: 0.9805 | Acc: 0.5938\n",
      "  [Step 017] Loss: 0.5520 | Acc: 0.7500\n",
      "  [Step 018] Loss: 0.8393 | Acc: 0.6562\n",
      "  [Step 019] Loss: 1.3302 | Acc: 0.4375\n",
      "  [Step 020] Loss: 1.0900 | Acc: 0.4688\n",
      "  [Step 021] Loss: 0.9740 | Acc: 0.5625\n",
      "  [Step 022] Loss: 0.9492 | Acc: 0.5938\n",
      "  [Step 023] Loss: 0.6272 | Acc: 0.7188\n",
      "  [Step 024] Loss: 0.7616 | Acc: 0.6562\n",
      "  [Step 025] Loss: 1.2071 | Acc: 0.5312\n",
      "  [Step 026] Loss: 0.7408 | Acc: 0.6875\n",
      "  [Step 027] Loss: 0.5089 | Acc: 0.7812\n",
      "  [Step 028] Loss: 0.7902 | Acc: 0.6250\n",
      "  [Step 029] Loss: 0.7258 | Acc: 0.6250\n",
      "  [Step 030] Loss: 0.6356 | Acc: 0.7500\n",
      "  [Step 031] Loss: 0.7540 | Acc: 0.6875\n",
      "  [Step 032] Loss: 0.9141 | Acc: 0.6875\n",
      "  [Step 033] Loss: 0.9579 | Acc: 0.6250\n",
      "  [Step 034] Loss: 0.4865 | Acc: 0.7812\n",
      "  [Step 035] Loss: 0.6478 | Acc: 0.6875\n",
      "  [Step 036] Loss: 0.8563 | Acc: 0.5938\n",
      "  [Step 037] Loss: 0.8788 | Acc: 0.6875\n",
      "  [Step 038] Loss: 0.9382 | Acc: 0.6250\n",
      "  [Step 039] Loss: 0.7472 | Acc: 0.6562\n",
      "  [Step 040] Loss: 0.6344 | Acc: 0.8438\n",
      "  [Step 041] Loss: 1.3146 | Acc: 0.5938\n",
      "  [Step 042] Loss: 1.2344 | Acc: 0.6250\n",
      "  [Step 043] Loss: 0.6615 | Acc: 0.7188\n",
      "  [Step 044] Loss: 0.8674 | Acc: 0.6250\n",
      "  [Step 045] Loss: 0.9868 | Acc: 0.6250\n",
      "  [Step 046] Loss: 0.8908 | Acc: 0.5312\n",
      "  [Step 047] Loss: 0.8659 | Acc: 0.6250\n",
      "  [Step 048] Loss: 0.9331 | Acc: 0.6875\n",
      "  [Step 049] Loss: 0.5747 | Acc: 0.8125\n",
      "  [Step 050] Loss: 0.9087 | Acc: 0.6562\n",
      "  [Step 051] Loss: 0.7244 | Acc: 0.7812\n",
      "  [Step 052] Loss: 0.9046 | Acc: 0.6875\n",
      "  [Step 053] Loss: 0.9144 | Acc: 0.6250\n",
      "  [Step 054] Loss: 0.7776 | Acc: 0.7188\n",
      "  [Step 055] Loss: 0.8043 | Acc: 0.6562\n",
      "  [Step 056] Loss: 0.6468 | Acc: 0.7812\n",
      "  [Step 057] Loss: 1.0492 | Acc: 0.5938\n",
      "  [Step 058] Loss: 0.6349 | Acc: 0.7812\n",
      "  [Step 059] Loss: 0.6997 | Acc: 0.7812\n",
      "  [Step 060] Loss: 0.7034 | Acc: 0.7500\n",
      "  [Step 061] Loss: 0.9603 | Acc: 0.6875\n",
      "  [Step 062] Loss: 0.7764 | Acc: 0.6875\n",
      "  [Step 063] Loss: 0.9345 | Acc: 0.6875\n",
      "  [Step 064] Loss: 0.6325 | Acc: 0.7812\n",
      "  [Step 065] Loss: 0.8702 | Acc: 0.7500\n",
      "  [Step 066] Loss: 0.8881 | Acc: 0.6875\n",
      "  [Step 067] Loss: 0.9701 | Acc: 0.5625\n",
      "  [Step 068] Loss: 0.9047 | Acc: 0.6562\n",
      "  [Step 069] Loss: 0.9116 | Acc: 0.6875\n",
      "  [Step 070] Loss: 1.0224 | Acc: 0.6562\n",
      "  [Step 071] Loss: 0.9697 | Acc: 0.7500\n",
      "  [Step 072] Loss: 0.8535 | Acc: 0.6875\n",
      "  [Step 073] Loss: 0.7667 | Acc: 0.7500\n",
      "  [Step 074] Loss: 1.1223 | Acc: 0.5625\n",
      "  [Step 075] Loss: 1.1425 | Acc: 0.7188\n",
      "  [Step 076] Loss: 1.0483 | Acc: 0.6875\n",
      "  [Step 077] Loss: 0.5775 | Acc: 0.7188\n",
      "  [Step 078] Loss: 0.8494 | Acc: 0.6562\n",
      "  [Step 079] Loss: 0.5308 | Acc: 0.9062\n",
      "  [Step 080] Loss: 0.5804 | Acc: 0.7188\n",
      "  [Step 081] Loss: 0.9434 | Acc: 0.5938\n",
      "  [Step 082] Loss: 0.7475 | Acc: 0.6250\n",
      "  [Step 083] Loss: 0.7029 | Acc: 0.6875\n",
      "  [Step 084] Loss: 0.6730 | Acc: 0.8125\n",
      "  [Step 085] Loss: 0.7465 | Acc: 0.6875\n",
      "  [Step 086] Loss: 0.9644 | Acc: 0.7188\n",
      "  [Step 087] Loss: 0.6786 | Acc: 0.7500\n",
      "  [Step 088] Loss: 0.7617 | Acc: 0.6875\n",
      "  [Step 089] Loss: 0.8450 | Acc: 0.6875\n",
      "  [Step 090] Loss: 0.6667 | Acc: 0.6875\n",
      "  [Step 091] Loss: 0.7351 | Acc: 0.6875\n",
      "  [Step 092] Loss: 0.6074 | Acc: 0.7812\n",
      "  [Step 093] Loss: 1.0273 | Acc: 0.5938\n",
      "  [Step 094] Loss: 0.7900 | Acc: 0.7188\n",
      "  [Step 095] Loss: 0.9469 | Acc: 0.6250\n",
      "  [Step 096] Loss: 1.0933 | Acc: 0.5938\n",
      "  [Step 097] Loss: 0.6534 | Acc: 0.8750\n",
      "  [Step 098] Loss: 0.8383 | Acc: 0.6875\n",
      "  [Step 099] Loss: 0.8297 | Acc: 0.7812\n",
      "  [Step 100] Loss: 1.4306 | Acc: 0.6250\n",
      "  [Step 101] Loss: 0.8723 | Acc: 0.5312\n",
      "  [Step 102] Loss: 0.9756 | Acc: 0.6875\n",
      "  [Step 103] Loss: 0.7214 | Acc: 0.8438\n",
      "  [Step 104] Loss: 1.0654 | Acc: 0.4688\n",
      "  [Step 105] Loss: 0.8949 | Acc: 0.6562\n",
      "  [Step 106] Loss: 0.6741 | Acc: 0.7188\n",
      "  [Step 107] Loss: 0.9916 | Acc: 0.6562\n",
      "  [Step 108] Loss: 1.0100 | Acc: 0.5625\n",
      "  [Step 109] Loss: 0.9409 | Acc: 0.6562\n",
      "  [Step 110] Loss: 0.8827 | Acc: 0.5625\n",
      "  [Step 111] Loss: 0.7487 | Acc: 0.6562\n",
      "  [Step 112] Loss: 0.7097 | Acc: 0.6562\n",
      "  [Step 113] Loss: 0.6717 | Acc: 0.7812\n",
      "  [Step 114] Loss: 0.9593 | Acc: 0.7188\n",
      "  [Step 115] Loss: 0.9870 | Acc: 0.6562\n",
      "  [Step 116] Loss: 1.1151 | Acc: 0.5000\n",
      "  [Step 117] Loss: 0.7173 | Acc: 0.6250\n",
      "  [Step 118] Loss: 0.8383 | Acc: 0.6875\n",
      "  [Step 119] Loss: 0.7148 | Acc: 0.7812\n",
      "  [Step 120] Loss: 0.6456 | Acc: 0.7188\n",
      "  [Step 121] Loss: 0.8979 | Acc: 0.7188\n",
      "  [Step 122] Loss: 0.8772 | Acc: 0.7188\n",
      "  [Step 123] Loss: 1.2126 | Acc: 0.5938\n",
      "  [Step 124] Loss: 0.8016 | Acc: 0.7500\n",
      "  [Step 125] Loss: 0.7378 | Acc: 0.7188\n",
      "  [Step 126] Loss: 0.9509 | Acc: 0.5625\n",
      "  [Step 127] Loss: 1.2464 | Acc: 0.5312\n",
      "  [Step 128] Loss: 0.7059 | Acc: 0.7188\n",
      "  [Step 129] Loss: 1.0607 | Acc: 0.5938\n",
      "  [Step 130] Loss: 0.7048 | Acc: 0.7500\n",
      "  [Step 131] Loss: 0.8525 | Acc: 0.6562\n",
      "  [Step 132] Loss: 0.9903 | Acc: 0.5625\n",
      "  [Step 133] Loss: 0.7141 | Acc: 0.7500\n",
      "  [Step 134] Loss: 0.7755 | Acc: 0.7812\n",
      "  [Step 135] Loss: 0.8319 | Acc: 0.6250\n",
      "  [Step 136] Loss: 0.9662 | Acc: 0.5938\n",
      "  [Step 137] Loss: 1.0267 | Acc: 0.6562\n",
      "  [Step 138] Loss: 0.6219 | Acc: 0.7188\n",
      "  [Step 139] Loss: 1.0060 | Acc: 0.5312\n",
      "  [Step 140] Loss: 0.5775 | Acc: 0.8125\n",
      "  [Step 141] Loss: 1.0541 | Acc: 0.5938\n",
      "  [Step 142] Loss: 0.6642 | Acc: 0.7812\n",
      "  [Step 143] Loss: 0.8743 | Acc: 0.6562\n",
      "  [Step 144] Loss: 0.7604 | Acc: 0.7500\n",
      "  [Step 145] Loss: 0.6934 | Acc: 0.7812\n",
      "  [Step 146] Loss: 0.6282 | Acc: 0.8125\n",
      "  [Step 147] Loss: 0.9580 | Acc: 0.6250\n",
      "  [Step 148] Loss: 0.7329 | Acc: 0.6562\n",
      "  [Step 149] Loss: 0.8849 | Acc: 0.5938\n",
      "  [Step 150] Loss: 0.9039 | Acc: 0.5938\n",
      "  [Step 151] Loss: 0.6467 | Acc: 0.7500\n",
      "  [Step 152] Loss: 0.8319 | Acc: 0.7500\n",
      "  [Step 153] Loss: 0.8982 | Acc: 0.5312\n",
      "  [Step 154] Loss: 0.8135 | Acc: 0.7188\n",
      "  [Step 155] Loss: 1.0019 | Acc: 0.6562\n",
      "  [Step 156] Loss: 0.5672 | Acc: 0.7188\n",
      "  [Step 157] Loss: 0.7024 | Acc: 0.6562\n",
      "  [Step 158] Loss: 0.6531 | Acc: 0.7812\n",
      "  [Step 159] Loss: 0.7086 | Acc: 0.7500\n",
      "  [Step 160] Loss: 0.6834 | Acc: 0.7812\n",
      "  [Step 161] Loss: 0.7537 | Acc: 0.7812\n",
      "  [Step 162] Loss: 0.8015 | Acc: 0.6875\n",
      "  [Step 163] Loss: 0.7142 | Acc: 0.7500\n",
      "  [Step 164] Loss: 0.6363 | Acc: 0.7500\n",
      "  [Step 165] Loss: 0.5210 | Acc: 0.8438\n",
      "  [Step 166] Loss: 0.5766 | Acc: 0.8125\n",
      "  [Step 167] Loss: 0.8881 | Acc: 0.6562\n",
      "  [Step 168] Loss: 0.8243 | Acc: 0.8125\n",
      "  [Step 169] Loss: 0.5200 | Acc: 0.8125\n",
      "  [Step 170] Loss: 0.9932 | Acc: 0.5938\n",
      "  [Step 171] Loss: 0.7582 | Acc: 0.6250\n",
      "  [Step 172] Loss: 0.5772 | Acc: 0.7812\n",
      "  [Step 173] Loss: 0.6682 | Acc: 0.7500\n",
      "  [Step 174] Loss: 1.2229 | Acc: 0.4062\n",
      "  [Step 175] Loss: 0.7372 | Acc: 0.8125\n",
      "  [Step 176] Loss: 0.6470 | Acc: 0.7812\n",
      "  [Step 177] Loss: 0.8199 | Acc: 0.6875\n",
      "  [Step 178] Loss: 0.5587 | Acc: 0.8125\n",
      "  [Step 179] Loss: 0.8778 | Acc: 0.7812\n",
      "  [Step 180] Loss: 0.7044 | Acc: 0.7500\n",
      "  [Step 181] Loss: 1.1196 | Acc: 0.6562\n",
      "  [Step 182] Loss: 0.6046 | Acc: 0.7812\n",
      "  [Step 183] Loss: 1.0450 | Acc: 0.6562\n",
      "  [Step 184] Loss: 0.7260 | Acc: 0.7500\n",
      "  [Step 185] Loss: 1.1620 | Acc: 0.5000\n",
      "  [Step 186] Loss: 1.0456 | Acc: 0.5938\n",
      "  [Step 187] Loss: 0.9251 | Acc: 0.5938\n",
      "  [Step 188] Loss: 0.9019 | Acc: 0.7500\n",
      "  [Step 189] Loss: 1.1203 | Acc: 0.5000\n",
      "  [Step 190] Loss: 0.6315 | Acc: 0.7812\n",
      "  [Step 191] Loss: 0.7536 | Acc: 0.7500\n",
      "  [Step 192] Loss: 0.8315 | Acc: 0.6250\n",
      "  [Step 193] Loss: 0.8785 | Acc: 0.6875\n",
      "  [Step 194] Loss: 0.8285 | Acc: 0.7188\n",
      "  [Step 195] Loss: 0.8561 | Acc: 0.7188\n",
      "  [Step 196] Loss: 0.6410 | Acc: 0.8125\n",
      "  [Step 197] Loss: 0.6438 | Acc: 0.8438\n",
      "  [Step 198] Loss: 0.8529 | Acc: 0.6562\n",
      "  [Step 199] Loss: 0.6883 | Acc: 0.7188\n",
      "  [Step 200] Loss: 1.0382 | Acc: 0.4688\n",
      "  [Step 201] Loss: 1.2025 | Acc: 0.4688\n",
      "  [Step 202] Loss: 0.9422 | Acc: 0.6875\n",
      "  [Step 203] Loss: 0.8033 | Acc: 0.5938\n",
      "  [Step 204] Loss: 0.7116 | Acc: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 0.9425 | Acc: 0.6250\n",
      "  [Step 206] Loss: 0.7783 | Acc: 0.7188\n",
      "  [Step 207] Loss: 0.7761 | Acc: 0.7812\n",
      "  [Step 208] Loss: 0.8005 | Acc: 0.6875\n",
      "  [Step 209] Loss: 1.1095 | Acc: 0.5625\n",
      "  [Step 210] Loss: 0.9112 | Acc: 0.6250\n",
      "  [Step 211] Loss: 0.6963 | Acc: 0.6562\n",
      "  [Step 212] Loss: 0.7610 | Acc: 0.7188\n",
      "  [Step 213] Loss: 0.4580 | Acc: 0.9062\n",
      "  [Step 214] Loss: 0.8250 | Acc: 0.6562\n",
      "  [Step 215] Loss: 1.1210 | Acc: 0.6250\n",
      "  [Step 216] Loss: 0.6523 | Acc: 0.7812\n",
      "  [Step 217] Loss: 0.7810 | Acc: 0.7500\n",
      "  [Step 218] Loss: 0.7861 | Acc: 0.6562\n",
      "  [Step 219] Loss: 0.6678 | Acc: 0.6875\n",
      "  [Step 220] Loss: 0.7593 | Acc: 0.6562\n",
      "  [Step 221] Loss: 0.6044 | Acc: 0.8125\n",
      "  [Step 222] Loss: 0.7125 | Acc: 0.7500\n",
      "  [Step 223] Loss: 0.7188 | Acc: 0.6875\n",
      "  [Step 224] Loss: 0.9521 | Acc: 0.5938\n",
      "  [Step 225] Loss: 0.9192 | Acc: 0.6562\n",
      "  [Step 226] Loss: 0.5971 | Acc: 0.7500\n",
      "  [Step 227] Loss: 0.6158 | Acc: 0.8750\n",
      "  [Step 228] Loss: 0.8814 | Acc: 0.6250\n",
      "  [Step 229] Loss: 0.9164 | Acc: 0.6562\n",
      "  [Step 230] Loss: 0.6937 | Acc: 0.7188\n",
      "  [Step 231] Loss: 0.9627 | Acc: 0.6562\n",
      "  [Step 232] Loss: 0.7090 | Acc: 0.6875\n",
      "  [Step 233] Loss: 0.8644 | Acc: 0.6875\n",
      "  [Step 234] Loss: 0.5931 | Acc: 0.8125\n",
      "  [Step 235] Loss: 0.6024 | Acc: 0.7812\n",
      "  [Step 236] Loss: 0.9656 | Acc: 0.5938\n",
      "  [Step 237] Loss: 0.7058 | Acc: 0.8125\n",
      "  [Step 238] Loss: 0.9119 | Acc: 0.5938\n",
      "  [Step 239] Loss: 1.0559 | Acc: 0.4688\n",
      "  [Step 240] Loss: 0.5552 | Acc: 0.7500\n",
      "  [Step 241] Loss: 0.6516 | Acc: 0.7812\n",
      "  [Step 242] Loss: 0.6542 | Acc: 0.7812\n",
      "  [Step 243] Loss: 0.7879 | Acc: 0.6875\n",
      "  [Step 244] Loss: 0.8426 | Acc: 0.7812\n",
      "  [Step 245] Loss: 0.6255 | Acc: 0.7500\n",
      "  [Step 246] Loss: 0.6238 | Acc: 0.7500\n",
      "  [Step 247] Loss: 0.8568 | Acc: 0.6562\n",
      "  [Step 248] Loss: 0.5152 | Acc: 0.8438\n",
      "  [Step 249] Loss: 0.7170 | Acc: 0.7188\n",
      "  [Step 250] Loss: 0.9939 | Acc: 0.5938\n",
      "  [Step 251] Loss: 1.0422 | Acc: 0.6250\n",
      "  [Step 252] Loss: 0.8850 | Acc: 0.5938\n",
      "  [Step 253] Loss: 0.6224 | Acc: 0.7812\n",
      "  [Step 254] Loss: 0.7648 | Acc: 0.6562\n",
      "  [Step 255] Loss: 0.7360 | Acc: 0.6562\n",
      "  [Step 256] Loss: 0.7406 | Acc: 0.7500\n",
      "  [Step 257] Loss: 1.0117 | Acc: 0.6250\n",
      "  [Step 258] Loss: 0.7988 | Acc: 0.7500\n",
      "  [Step 259] Loss: 0.6185 | Acc: 0.7500\n",
      "  [Step 260] Loss: 1.2399 | Acc: 0.6562\n",
      "  [Step 261] Loss: 1.1382 | Acc: 0.5625\n",
      "  [Step 262] Loss: 0.7053 | Acc: 0.7812\n",
      "  [Step 263] Loss: 0.8049 | Acc: 0.5938\n",
      "  [Step 264] Loss: 0.7762 | Acc: 0.7500\n",
      "  [Step 265] Loss: 0.8471 | Acc: 0.6562\n",
      "  [Step 266] Loss: 0.8825 | Acc: 0.7500\n",
      "  [Step 267] Loss: 0.8506 | Acc: 0.6562\n",
      "  [Step 268] Loss: 0.6660 | Acc: 0.7188\n",
      "  [Step 269] Loss: 0.8699 | Acc: 0.6875\n",
      "  [Step 270] Loss: 0.8053 | Acc: 0.7812\n",
      "  [Step 271] Loss: 0.8574 | Acc: 0.7188\n",
      "  [Step 272] Loss: 0.6703 | Acc: 0.7188\n",
      "  [Step 273] Loss: 0.6333 | Acc: 0.8125\n",
      "  [Step 274] Loss: 0.5228 | Acc: 0.7812\n",
      "  [Step 275] Loss: 0.6068 | Acc: 0.8438\n",
      "  [Step 276] Loss: 0.7353 | Acc: 0.6875\n",
      "  [Step 277] Loss: 0.7521 | Acc: 0.6562\n",
      "  [Step 278] Loss: 0.8539 | Acc: 0.7188\n",
      "  [Step 279] Loss: 0.7508 | Acc: 0.6875\n",
      "  [Step 280] Loss: 1.1236 | Acc: 0.6250\n",
      "  [Step 281] Loss: 0.8267 | Acc: 0.7500\n",
      "  [Step 282] Loss: 1.0207 | Acc: 0.6562\n",
      "  [Step 283] Loss: 0.8277 | Acc: 0.6250\n",
      "  [Step 284] Loss: 0.9006 | Acc: 0.6250\n",
      "  [Step 285] Loss: 0.9064 | Acc: 0.4688\n",
      "  [Step 286] Loss: 0.6333 | Acc: 0.7812\n",
      "  [Step 287] Loss: 0.9084 | Acc: 0.5938\n",
      "  [Step 288] Loss: 0.5675 | Acc: 0.7812\n",
      "  [Step 289] Loss: 0.9601 | Acc: 0.7188\n",
      "  [Step 290] Loss: 1.1176 | Acc: 0.5625\n",
      "  [Step 291] Loss: 0.9436 | Acc: 0.6250\n",
      "  [Step 292] Loss: 0.7939 | Acc: 0.5938\n",
      "  [Step 293] Loss: 0.5490 | Acc: 0.7812\n",
      "  [Step 294] Loss: 0.8292 | Acc: 0.6875\n",
      "  [Step 295] Loss: 0.8236 | Acc: 0.6250\n",
      "  [Step 296] Loss: 0.8171 | Acc: 0.6250\n",
      "  [Step 297] Loss: 0.7450 | Acc: 0.6562\n",
      "  [Step 298] Loss: 0.9388 | Acc: 0.6250\n",
      "  [Step 299] Loss: 0.9161 | Acc: 0.6562\n",
      "  [Step 300] Loss: 0.6037 | Acc: 0.7188\n",
      "  [Step 301] Loss: 0.6494 | Acc: 0.6875\n",
      "  [Step 302] Loss: 0.7788 | Acc: 0.5938\n",
      "  [Step 303] Loss: 0.7886 | Acc: 0.7188\n",
      "  [Step 304] Loss: 0.6541 | Acc: 0.7812\n",
      "  [Step 305] Loss: 0.8769 | Acc: 0.7188\n",
      "  [Step 306] Loss: 0.9264 | Acc: 0.6562\n",
      "  [Step 307] Loss: 0.6775 | Acc: 0.7812\n",
      "  [Step 308] Loss: 0.8772 | Acc: 0.6250\n",
      "  [Step 309] Loss: 0.6724 | Acc: 0.6562\n",
      "  [Step 310] Loss: 0.8463 | Acc: 0.7188\n",
      "  [Step 311] Loss: 0.6813 | Acc: 0.8438\n",
      "  [Step 312] Loss: 0.7828 | Acc: 0.6562\n",
      "  [Step 313] Loss: 0.9811 | Acc: 0.5938\n",
      "  [Step 314] Loss: 1.2035 | Acc: 0.5625\n",
      "  [Step 315] Loss: 0.7359 | Acc: 0.7500\n",
      "  [Step 316] Loss: 0.8452 | Acc: 0.7188\n",
      "  [Step 317] Loss: 0.7960 | Acc: 0.7188\n",
      "  [Step 318] Loss: 0.6853 | Acc: 0.7500\n",
      "  [Step 319] Loss: 1.0451 | Acc: 0.6562\n",
      "  [Step 320] Loss: 0.9958 | Acc: 0.5625\n",
      "  [Step 321] Loss: 0.8553 | Acc: 0.5938\n",
      "  [Step 322] Loss: 0.8673 | Acc: 0.5938\n",
      "  [Step 323] Loss: 1.2970 | Acc: 0.6250\n",
      "  [Step 324] Loss: 0.6145 | Acc: 0.7188\n",
      "  [Step 325] Loss: 0.8145 | Acc: 0.7812\n",
      "  [Step 326] Loss: 0.9221 | Acc: 0.6250\n",
      "  [Step 327] Loss: 1.0950 | Acc: 0.6562\n",
      "  [Step 328] Loss: 0.7985 | Acc: 0.7188\n",
      "  [Step 329] Loss: 0.9853 | Acc: 0.6250\n",
      "  [Step 330] Loss: 0.9958 | Acc: 0.6875\n",
      "[Epoch 3] Train Loss: 0.2785 | Acc: 0.6831 || Val Loss: 0.8687 | Acc: 0.6443\n",
      "Precision: 0.5175 | Recall: 0.5203 | F1: 0.4922 | Time: 1079.14s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.11244512 0.0599745  0.09461714 0.06427084 0.11211283 0.10670078\n",
      "  0.11698889 0.12247499 0.10429979 0.10611507]\n",
      " [0.11626726 0.06576432 0.07548656 0.06336655 0.11251905 0.1156714\n",
      "  0.11056133 0.11349195 0.11102393 0.11584765]\n",
      " [0.10897012 0.06641079 0.10042419 0.06827797 0.10804216 0.10827853\n",
      "  0.11332836 0.11096933 0.10981125 0.10548732]\n",
      " [0.11295956 0.0711069  0.07931728 0.07104657 0.11405021 0.11329579\n",
      "  0.10970882 0.10631411 0.11121542 0.11098535]\n",
      " [0.10758466 0.08463127 0.08941704 0.08555601 0.10522086 0.10490991\n",
      "  0.10580961 0.10629811 0.10790559 0.10266695]\n",
      " [0.10964391 0.07294627 0.10126096 0.07459785 0.10809262 0.10687813\n",
      "  0.10729773 0.10786553 0.10262688 0.10879008]\n",
      " [0.10965945 0.07558513 0.08283226 0.07499374 0.11074384 0.11030898\n",
      "  0.10771422 0.10907236 0.10920724 0.10988275]\n",
      " [0.1054507  0.08733974 0.09106903 0.08784124 0.10487898 0.10360488\n",
      "  0.10687471 0.10503771 0.10451998 0.10338309]\n",
      " [0.10722454 0.08451825 0.08865585 0.0853983  0.10581138 0.10545785\n",
      "  0.10564017 0.10535661 0.10636919 0.10556787]\n",
      " [0.11059779 0.06859975 0.09877001 0.06829954 0.11055755 0.10957061\n",
      "  0.10478959 0.10923368 0.10834584 0.11123554]\n",
      " [0.11135786 0.07407981 0.08313759 0.07432002 0.10922516 0.11107487\n",
      "  0.10456204 0.11058848 0.10887095 0.11278329]\n",
      " [0.1066965  0.08582915 0.08980745 0.08602684 0.1049309  0.10436837\n",
      "  0.10727981 0.10630139 0.1039721  0.10478748]\n",
      " [0.10737484 0.08281676 0.08671042 0.0830958  0.10516106 0.10686898\n",
      "  0.10653472 0.10539834 0.10664062 0.10939835]\n",
      " [0.10938332 0.07863839 0.0832322  0.07910504 0.10834914 0.10734089\n",
      "  0.10814914 0.11280839 0.10626842 0.10672504]\n",
      " [0.11001384 0.07130326 0.09895358 0.0702038  0.10815235 0.10906476\n",
      "  0.10842574 0.1072513  0.11049924 0.1061321 ]\n",
      " [0.10976519 0.07564501 0.08507948 0.07560062 0.10901918 0.10744051\n",
      "  0.10857297 0.11340432 0.10738787 0.10808481]\n",
      " [0.106102   0.08739385 0.09096514 0.08769001 0.10438251 0.10596413\n",
      "  0.10467684 0.10567676 0.10453776 0.10261098]\n",
      " [0.1072396  0.08421055 0.08843898 0.08443176 0.10582788 0.10777281\n",
      "  0.10485239 0.10563039 0.10518138 0.10641433]\n",
      " [0.10900903 0.08027222 0.08460438 0.08077323 0.1091976  0.10800854\n",
      "  0.10584888 0.1103949  0.10436368 0.10752756]\n",
      " [0.1096307  0.07605699 0.08010041 0.07641544 0.10661819 0.10939134\n",
      "  0.10753228 0.10926308 0.11180235 0.11318924]\n",
      " [0.10866478 0.07409889 0.09147688 0.07249142 0.10773579 0.10878313\n",
      "  0.11005213 0.11028738 0.10925782 0.10715178]\n",
      " [0.10962751 0.07895868 0.08094895 0.07875433 0.109328   0.10984958\n",
      "  0.1089253  0.1062643  0.1096371  0.1077063 ]\n",
      " [0.10501938 0.08815965 0.0916922  0.08811991 0.10504847 0.10390937\n",
      "  0.10359547 0.10413254 0.10350452 0.10681853]\n",
      " [0.10636248 0.08519367 0.08895855 0.08529167 0.10605285 0.10623308\n",
      "  0.10576737 0.10631334 0.10516453 0.10466237]\n",
      " [0.10761687 0.08095329 0.085521   0.0809763  0.10787778 0.10623004\n",
      "  0.10987182 0.10559001 0.10982421 0.10553866]\n",
      " [0.10897882 0.0774786  0.08152197 0.07760271 0.10962947 0.10816146\n",
      "  0.108798   0.10841586 0.10834917 0.11106391]\n",
      " [0.11079628 0.07247435 0.07689049 0.0727217  0.11133111 0.10975942\n",
      "  0.11455603 0.11245987 0.11017883 0.1088319 ]\n",
      " [0.10868704 0.07129382 0.10492125 0.06980207 0.10848716 0.10829105\n",
      "  0.10631343 0.10789753 0.10650367 0.10780296]\n",
      " [0.10948258 0.07792957 0.0867739  0.07693587 0.10961013 0.10759149\n",
      "  0.10868854 0.10641372 0.10866331 0.10791089]\n",
      " [0.10523432 0.08814206 0.09330204 0.08792896 0.10270785 0.10511354\n",
      "  0.10575687 0.1030132  0.1025463  0.10625483]\n",
      " [0.10597645 0.08532517 0.09108752 0.08511578 0.1060523  0.10575302\n",
      "  0.10618978 0.10701856 0.10433427 0.1031471 ]\n",
      " [0.1066855  0.08185301 0.08831779 0.08180689 0.10781    0.10604037\n",
      "  0.10705821 0.1062408  0.10722324 0.10696421]\n",
      " [0.10901056 0.07902515 0.08565756 0.07886191 0.10716003 0.10908386\n",
      "  0.10788825 0.10948005 0.10718573 0.10664685]\n",
      " [0.1103725  0.07432523 0.08147827 0.07412425 0.10959341 0.11004204\n",
      "  0.10916004 0.11198682 0.11019774 0.10871966]\n",
      " [0.11157367 0.07070431 0.07829284 0.07042955 0.11051065 0.1109584\n",
      "  0.11167312 0.11648618 0.11052808 0.10884319]]\n",
      "\n",
      "[Epoch 4/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 1.8237 | Acc: 0.5625\n",
      "  [Step 001] Loss: 0.7165 | Acc: 0.7812\n",
      "  [Step 002] Loss: 0.9831 | Acc: 0.6875\n",
      "  [Step 003] Loss: 0.8710 | Acc: 0.6875\n",
      "  [Step 004] Loss: 0.7885 | Acc: 0.6250\n",
      "  [Step 005] Loss: 0.6350 | Acc: 0.8125\n",
      "  [Step 006] Loss: 0.6992 | Acc: 0.8125\n",
      "  [Step 007] Loss: 0.7671 | Acc: 0.6875\n",
      "  [Step 008] Loss: 1.0306 | Acc: 0.4688\n",
      "  [Step 009] Loss: 0.8068 | Acc: 0.6875\n",
      "  [Step 010] Loss: 0.6454 | Acc: 0.7188\n",
      "  [Step 011] Loss: 1.0335 | Acc: 0.5938\n",
      "  [Step 012] Loss: 0.7480 | Acc: 0.6875\n",
      "  [Step 013] Loss: 0.8449 | Acc: 0.5938\n",
      "  [Step 014] Loss: 0.8027 | Acc: 0.6250\n",
      "  [Step 015] Loss: 1.0112 | Acc: 0.6250\n",
      "  [Step 016] Loss: 0.7496 | Acc: 0.7188\n",
      "  [Step 017] Loss: 0.8197 | Acc: 0.6875\n",
      "  [Step 018] Loss: 0.7852 | Acc: 0.7500\n",
      "  [Step 019] Loss: 0.6264 | Acc: 0.7812\n",
      "  [Step 020] Loss: 0.8527 | Acc: 0.5938\n",
      "  [Step 021] Loss: 0.6035 | Acc: 0.7188\n",
      "  [Step 022] Loss: 0.7227 | Acc: 0.6250\n",
      "  [Step 023] Loss: 0.9818 | Acc: 0.5000\n",
      "  [Step 024] Loss: 0.7478 | Acc: 0.6562\n",
      "  [Step 025] Loss: 0.9052 | Acc: 0.6250\n",
      "  [Step 026] Loss: 0.7795 | Acc: 0.7812\n",
      "  [Step 027] Loss: 0.6196 | Acc: 0.8125\n",
      "  [Step 028] Loss: 0.7643 | Acc: 0.7812\n",
      "  [Step 029] Loss: 0.7803 | Acc: 0.7812\n",
      "  [Step 030] Loss: 0.8810 | Acc: 0.6250\n",
      "  [Step 031] Loss: 0.7874 | Acc: 0.7188\n",
      "  [Step 032] Loss: 0.7357 | Acc: 0.7500\n",
      "  [Step 033] Loss: 0.6860 | Acc: 0.6875\n",
      "  [Step 034] Loss: 0.8617 | Acc: 0.6562\n",
      "  [Step 035] Loss: 0.6318 | Acc: 0.7500\n",
      "  [Step 036] Loss: 0.4715 | Acc: 0.7812\n",
      "  [Step 037] Loss: 0.6146 | Acc: 0.6875\n",
      "  [Step 038] Loss: 0.6894 | Acc: 0.6875\n",
      "  [Step 039] Loss: 1.4401 | Acc: 0.4688\n",
      "  [Step 040] Loss: 0.6286 | Acc: 0.7188\n",
      "  [Step 041] Loss: 0.8302 | Acc: 0.6562\n",
      "  [Step 042] Loss: 0.8654 | Acc: 0.6250\n",
      "  [Step 043] Loss: 0.6486 | Acc: 0.7188\n",
      "  [Step 044] Loss: 0.6916 | Acc: 0.7188\n",
      "  [Step 045] Loss: 0.6504 | Acc: 0.7500\n",
      "  [Step 046] Loss: 0.5703 | Acc: 0.8438\n",
      "  [Step 047] Loss: 0.7273 | Acc: 0.7500\n",
      "  [Step 048] Loss: 0.8138 | Acc: 0.7500\n",
      "  [Step 049] Loss: 0.4240 | Acc: 0.8438\n",
      "  [Step 050] Loss: 0.8327 | Acc: 0.6562\n",
      "  [Step 051] Loss: 1.2859 | Acc: 0.6250\n",
      "  [Step 052] Loss: 0.5786 | Acc: 0.8438\n",
      "  [Step 053] Loss: 0.7235 | Acc: 0.6875\n",
      "  [Step 054] Loss: 1.1565 | Acc: 0.5312\n",
      "  [Step 055] Loss: 0.6045 | Acc: 0.7500\n",
      "  [Step 056] Loss: 0.8389 | Acc: 0.7812\n",
      "  [Step 057] Loss: 0.6580 | Acc: 0.6875\n",
      "  [Step 058] Loss: 0.5623 | Acc: 0.7500\n",
      "  [Step 059] Loss: 0.7647 | Acc: 0.7188\n",
      "  [Step 060] Loss: 0.8300 | Acc: 0.6875\n",
      "  [Step 061] Loss: 0.5223 | Acc: 0.8125\n",
      "  [Step 062] Loss: 0.7533 | Acc: 0.7812\n",
      "  [Step 063] Loss: 0.5613 | Acc: 0.7500\n",
      "  [Step 064] Loss: 0.6854 | Acc: 0.6875\n",
      "  [Step 065] Loss: 0.7808 | Acc: 0.7188\n",
      "  [Step 066] Loss: 0.4980 | Acc: 0.8438\n",
      "  [Step 067] Loss: 0.4584 | Acc: 0.7500\n",
      "  [Step 068] Loss: 1.4148 | Acc: 0.5625\n",
      "  [Step 069] Loss: 0.6940 | Acc: 0.7812\n",
      "  [Step 070] Loss: 0.5278 | Acc: 0.7812\n",
      "  [Step 071] Loss: 0.5472 | Acc: 0.7812\n",
      "  [Step 072] Loss: 0.7297 | Acc: 0.8438\n",
      "  [Step 073] Loss: 0.8901 | Acc: 0.5938\n",
      "  [Step 074] Loss: 0.6426 | Acc: 0.7812\n",
      "  [Step 075] Loss: 1.0306 | Acc: 0.5938\n",
      "  [Step 076] Loss: 0.6312 | Acc: 0.7812\n",
      "  [Step 077] Loss: 0.6236 | Acc: 0.7500\n",
      "  [Step 078] Loss: 1.0043 | Acc: 0.5312\n",
      "  [Step 079] Loss: 0.8329 | Acc: 0.6250\n",
      "  [Step 080] Loss: 0.7130 | Acc: 0.7812\n",
      "  [Step 081] Loss: 0.8273 | Acc: 0.6875\n",
      "  [Step 082] Loss: 0.7607 | Acc: 0.7500\n",
      "  [Step 083] Loss: 0.4958 | Acc: 0.8750\n",
      "  [Step 084] Loss: 1.5105 | Acc: 0.6562\n",
      "  [Step 085] Loss: 1.1236 | Acc: 0.7500\n",
      "  [Step 086] Loss: 0.6627 | Acc: 0.8438\n",
      "  [Step 087] Loss: 0.9430 | Acc: 0.5938\n",
      "  [Step 088] Loss: 0.6917 | Acc: 0.7500\n",
      "  [Step 089] Loss: 0.7167 | Acc: 0.7812\n",
      "  [Step 090] Loss: 0.9576 | Acc: 0.5938\n",
      "  [Step 091] Loss: 0.8454 | Acc: 0.6250\n",
      "  [Step 092] Loss: 0.5883 | Acc: 0.8125\n",
      "  [Step 093] Loss: 0.7282 | Acc: 0.7500\n",
      "  [Step 094] Loss: 0.7703 | Acc: 0.6875\n",
      "  [Step 095] Loss: 0.5686 | Acc: 0.8125\n",
      "  [Step 096] Loss: 1.1441 | Acc: 0.6562\n",
      "  [Step 097] Loss: 0.6889 | Acc: 0.7500\n",
      "  [Step 098] Loss: 1.1763 | Acc: 0.5938\n",
      "  [Step 099] Loss: 0.8308 | Acc: 0.5938\n",
      "  [Step 100] Loss: 1.0198 | Acc: 0.7188\n",
      "  [Step 101] Loss: 0.8920 | Acc: 0.6875\n",
      "  [Step 102] Loss: 0.9801 | Acc: 0.5312\n",
      "  [Step 103] Loss: 1.0594 | Acc: 0.5000\n",
      "  [Step 104] Loss: 0.9725 | Acc: 0.5625\n",
      "  [Step 105] Loss: 0.8051 | Acc: 0.6250\n",
      "  [Step 106] Loss: 0.5221 | Acc: 0.8125\n",
      "  [Step 107] Loss: 0.7918 | Acc: 0.6875\n",
      "  [Step 108] Loss: 0.8661 | Acc: 0.7500\n",
      "  [Step 109] Loss: 0.7148 | Acc: 0.7188\n",
      "  [Step 110] Loss: 0.7083 | Acc: 0.7188\n",
      "  [Step 111] Loss: 0.6885 | Acc: 0.7188\n",
      "  [Step 112] Loss: 1.1617 | Acc: 0.7188\n",
      "  [Step 113] Loss: 0.6455 | Acc: 0.7188\n",
      "  [Step 114] Loss: 0.5683 | Acc: 0.8438\n",
      "  [Step 115] Loss: 0.7723 | Acc: 0.5938\n",
      "  [Step 116] Loss: 0.7661 | Acc: 0.7500\n",
      "  [Step 117] Loss: 0.5507 | Acc: 0.7500\n",
      "  [Step 118] Loss: 0.9016 | Acc: 0.6562\n",
      "  [Step 119] Loss: 0.6423 | Acc: 0.8125\n",
      "  [Step 120] Loss: 0.8297 | Acc: 0.5938\n",
      "  [Step 121] Loss: 0.9508 | Acc: 0.6562\n",
      "  [Step 122] Loss: 1.0991 | Acc: 0.4688\n",
      "  [Step 123] Loss: 0.6003 | Acc: 0.7500\n",
      "  [Step 124] Loss: 0.7073 | Acc: 0.6562\n",
      "  [Step 125] Loss: 0.7376 | Acc: 0.6875\n",
      "  [Step 126] Loss: 1.2792 | Acc: 0.5312\n",
      "  [Step 127] Loss: 0.5094 | Acc: 0.8125\n",
      "  [Step 128] Loss: 0.8152 | Acc: 0.7188\n",
      "  [Step 129] Loss: 0.6804 | Acc: 0.6875\n",
      "  [Step 130] Loss: 0.7608 | Acc: 0.7188\n",
      "  [Step 131] Loss: 0.5144 | Acc: 0.8125\n",
      "  [Step 132] Loss: 0.4886 | Acc: 0.8125\n",
      "  [Step 133] Loss: 0.6489 | Acc: 0.7500\n",
      "  [Step 134] Loss: 0.8081 | Acc: 0.6875\n",
      "  [Step 135] Loss: 0.8374 | Acc: 0.7188\n",
      "  [Step 136] Loss: 0.5907 | Acc: 0.8125\n",
      "  [Step 137] Loss: 0.9202 | Acc: 0.7500\n",
      "  [Step 138] Loss: 0.9306 | Acc: 0.5938\n",
      "  [Step 139] Loss: 1.0604 | Acc: 0.6562\n",
      "  [Step 140] Loss: 0.7913 | Acc: 0.6250\n",
      "  [Step 141] Loss: 0.8873 | Acc: 0.6562\n",
      "  [Step 142] Loss: 0.9976 | Acc: 0.6250\n",
      "  [Step 143] Loss: 0.7812 | Acc: 0.7812\n",
      "  [Step 144] Loss: 0.5132 | Acc: 0.8438\n",
      "  [Step 145] Loss: 0.4497 | Acc: 0.7812\n",
      "  [Step 146] Loss: 0.6897 | Acc: 0.7188\n",
      "  [Step 147] Loss: 0.7526 | Acc: 0.6562\n",
      "  [Step 148] Loss: 0.5471 | Acc: 0.8125\n",
      "  [Step 149] Loss: 0.9283 | Acc: 0.7188\n",
      "  [Step 150] Loss: 0.6639 | Acc: 0.6250\n",
      "  [Step 151] Loss: 0.8078 | Acc: 0.7188\n",
      "  [Step 152] Loss: 1.1278 | Acc: 0.6250\n",
      "  [Step 153] Loss: 0.8709 | Acc: 0.6875\n",
      "  [Step 154] Loss: 0.9483 | Acc: 0.6250\n",
      "  [Step 155] Loss: 0.8360 | Acc: 0.7500\n",
      "  [Step 156] Loss: 0.6339 | Acc: 0.7188\n",
      "  [Step 157] Loss: 1.0576 | Acc: 0.5938\n",
      "  [Step 158] Loss: 0.9122 | Acc: 0.7188\n",
      "  [Step 159] Loss: 0.6049 | Acc: 0.8125\n",
      "  [Step 160] Loss: 0.9602 | Acc: 0.7812\n",
      "  [Step 161] Loss: 0.9059 | Acc: 0.6875\n",
      "  [Step 162] Loss: 0.7045 | Acc: 0.7500\n",
      "  [Step 163] Loss: 0.8252 | Acc: 0.6562\n",
      "  [Step 164] Loss: 0.6165 | Acc: 0.8125\n",
      "  [Step 165] Loss: 0.8044 | Acc: 0.7500\n",
      "  [Step 166] Loss: 0.9901 | Acc: 0.6250\n",
      "  [Step 167] Loss: 0.6746 | Acc: 0.7188\n",
      "  [Step 168] Loss: 0.4614 | Acc: 0.8125\n",
      "  [Step 169] Loss: 0.6076 | Acc: 0.7500\n",
      "  [Step 170] Loss: 1.0160 | Acc: 0.5938\n",
      "  [Step 171] Loss: 0.5715 | Acc: 0.8125\n",
      "  [Step 172] Loss: 0.6253 | Acc: 0.7812\n",
      "  [Step 173] Loss: 0.8226 | Acc: 0.7500\n",
      "  [Step 174] Loss: 0.6566 | Acc: 0.8438\n",
      "  [Step 175] Loss: 0.6413 | Acc: 0.7812\n",
      "  [Step 176] Loss: 0.4402 | Acc: 0.8750\n",
      "  [Step 177] Loss: 0.7959 | Acc: 0.6875\n",
      "  [Step 178] Loss: 0.4236 | Acc: 0.8438\n",
      "  [Step 179] Loss: 0.9485 | Acc: 0.6875\n",
      "  [Step 180] Loss: 0.6799 | Acc: 0.7188\n",
      "  [Step 181] Loss: 0.7720 | Acc: 0.6875\n",
      "  [Step 182] Loss: 0.9770 | Acc: 0.7500\n",
      "  [Step 183] Loss: 0.6108 | Acc: 0.8125\n",
      "  [Step 184] Loss: 0.6323 | Acc: 0.8125\n",
      "  [Step 185] Loss: 0.8085 | Acc: 0.7500\n",
      "  [Step 186] Loss: 1.1173 | Acc: 0.5938\n",
      "  [Step 187] Loss: 0.6679 | Acc: 0.7812\n",
      "  [Step 188] Loss: 1.2949 | Acc: 0.5000\n",
      "  [Step 189] Loss: 1.1035 | Acc: 0.5000\n",
      "  [Step 190] Loss: 0.6851 | Acc: 0.7188\n",
      "  [Step 191] Loss: 0.5844 | Acc: 0.8125\n",
      "  [Step 192] Loss: 0.5095 | Acc: 0.8438\n",
      "  [Step 193] Loss: 0.6269 | Acc: 0.7188\n",
      "  [Step 194] Loss: 0.6882 | Acc: 0.6562\n",
      "  [Step 195] Loss: 0.6996 | Acc: 0.7500\n",
      "  [Step 196] Loss: 0.8925 | Acc: 0.5938\n",
      "  [Step 197] Loss: 0.6780 | Acc: 0.7188\n",
      "  [Step 198] Loss: 0.8214 | Acc: 0.6875\n",
      "  [Step 199] Loss: 0.7793 | Acc: 0.7188\n",
      "  [Step 200] Loss: 0.9223 | Acc: 0.5938\n",
      "  [Step 201] Loss: 0.6004 | Acc: 0.8750\n",
      "  [Step 202] Loss: 0.8095 | Acc: 0.6875\n",
      "  [Step 203] Loss: 0.8207 | Acc: 0.7500\n",
      "  [Step 204] Loss: 0.5034 | Acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 0.5262 | Acc: 0.8125\n",
      "  [Step 206] Loss: 1.2007 | Acc: 0.6250\n",
      "  [Step 207] Loss: 0.8466 | Acc: 0.7188\n",
      "  [Step 208] Loss: 0.6894 | Acc: 0.6562\n",
      "  [Step 209] Loss: 0.6747 | Acc: 0.6875\n",
      "  [Step 210] Loss: 0.5263 | Acc: 0.8438\n",
      "  [Step 211] Loss: 0.9774 | Acc: 0.6875\n",
      "  [Step 212] Loss: 0.7696 | Acc: 0.6562\n",
      "  [Step 213] Loss: 0.8347 | Acc: 0.7500\n",
      "  [Step 214] Loss: 0.5795 | Acc: 0.7500\n",
      "  [Step 215] Loss: 1.0167 | Acc: 0.6250\n",
      "  [Step 216] Loss: 1.0573 | Acc: 0.5625\n",
      "  [Step 217] Loss: 0.5088 | Acc: 0.8750\n",
      "  [Step 218] Loss: 1.0218 | Acc: 0.4688\n",
      "  [Step 219] Loss: 0.6709 | Acc: 0.8438\n",
      "  [Step 220] Loss: 1.0388 | Acc: 0.5625\n",
      "  [Step 221] Loss: 0.8067 | Acc: 0.6562\n",
      "  [Step 222] Loss: 0.9528 | Acc: 0.7188\n",
      "  [Step 223] Loss: 0.7449 | Acc: 0.7188\n",
      "  [Step 224] Loss: 0.8049 | Acc: 0.6250\n",
      "  [Step 225] Loss: 0.5919 | Acc: 0.7812\n",
      "  [Step 226] Loss: 0.9400 | Acc: 0.7188\n",
      "  [Step 227] Loss: 0.6899 | Acc: 0.7500\n",
      "  [Step 228] Loss: 0.8331 | Acc: 0.6562\n",
      "  [Step 229] Loss: 1.1425 | Acc: 0.5938\n",
      "  [Step 230] Loss: 0.8212 | Acc: 0.7500\n",
      "  [Step 231] Loss: 1.0617 | Acc: 0.7188\n",
      "  [Step 232] Loss: 0.7550 | Acc: 0.6562\n",
      "  [Step 233] Loss: 0.5242 | Acc: 0.8750\n",
      "  [Step 234] Loss: 0.6770 | Acc: 0.7188\n",
      "  [Step 235] Loss: 0.8422 | Acc: 0.5625\n",
      "  [Step 236] Loss: 0.7449 | Acc: 0.7500\n",
      "  [Step 237] Loss: 0.5114 | Acc: 0.7812\n",
      "  [Step 238] Loss: 0.8565 | Acc: 0.6562\n",
      "  [Step 239] Loss: 0.4623 | Acc: 0.8750\n",
      "  [Step 240] Loss: 0.5547 | Acc: 0.8438\n",
      "  [Step 241] Loss: 0.9925 | Acc: 0.6875\n",
      "  [Step 242] Loss: 0.5141 | Acc: 0.8750\n",
      "  [Step 243] Loss: 0.8506 | Acc: 0.6875\n",
      "  [Step 244] Loss: 0.7166 | Acc: 0.7188\n",
      "  [Step 245] Loss: 0.6146 | Acc: 0.8125\n",
      "  [Step 246] Loss: 0.6873 | Acc: 0.7188\n",
      "  [Step 247] Loss: 0.5413 | Acc: 0.7812\n",
      "  [Step 248] Loss: 0.7080 | Acc: 0.7188\n",
      "  [Step 249] Loss: 0.6202 | Acc: 0.8125\n",
      "  [Step 250] Loss: 0.7515 | Acc: 0.7812\n",
      "  [Step 251] Loss: 0.9700 | Acc: 0.5938\n",
      "  [Step 252] Loss: 0.5002 | Acc: 0.8750\n",
      "  [Step 253] Loss: 0.5711 | Acc: 0.8125\n",
      "  [Step 254] Loss: 0.6538 | Acc: 0.7188\n",
      "  [Step 255] Loss: 0.4438 | Acc: 0.8125\n",
      "  [Step 256] Loss: 0.7711 | Acc: 0.6875\n",
      "  [Step 257] Loss: 0.7216 | Acc: 0.6875\n",
      "  [Step 258] Loss: 0.7593 | Acc: 0.6875\n",
      "  [Step 259] Loss: 0.5180 | Acc: 0.8125\n",
      "  [Step 260] Loss: 0.4941 | Acc: 0.8125\n",
      "  [Step 261] Loss: 0.9091 | Acc: 0.7500\n",
      "  [Step 262] Loss: 0.6651 | Acc: 0.7812\n",
      "  [Step 263] Loss: 0.6324 | Acc: 0.6250\n",
      "  [Step 264] Loss: 0.8869 | Acc: 0.6562\n",
      "  [Step 265] Loss: 0.3573 | Acc: 0.8750\n",
      "  [Step 266] Loss: 0.5618 | Acc: 0.7812\n",
      "  [Step 267] Loss: 0.8115 | Acc: 0.6875\n",
      "  [Step 268] Loss: 0.9225 | Acc: 0.6875\n",
      "  [Step 269] Loss: 1.0197 | Acc: 0.6250\n",
      "  [Step 270] Loss: 0.8477 | Acc: 0.6875\n",
      "  [Step 271] Loss: 0.8967 | Acc: 0.6562\n",
      "  [Step 272] Loss: 1.1665 | Acc: 0.5312\n",
      "  [Step 273] Loss: 0.9111 | Acc: 0.6562\n",
      "  [Step 274] Loss: 0.6384 | Acc: 0.8125\n",
      "  [Step 275] Loss: 0.4163 | Acc: 0.8125\n",
      "  [Step 276] Loss: 0.5470 | Acc: 0.7188\n",
      "  [Step 277] Loss: 0.9378 | Acc: 0.7812\n",
      "  [Step 278] Loss: 1.4233 | Acc: 0.6250\n",
      "  [Step 279] Loss: 0.7780 | Acc: 0.6875\n",
      "  [Step 280] Loss: 0.8796 | Acc: 0.8125\n",
      "  [Step 281] Loss: 0.7768 | Acc: 0.6250\n",
      "  [Step 282] Loss: 0.8507 | Acc: 0.6250\n",
      "  [Step 283] Loss: 0.6907 | Acc: 0.7500\n",
      "  [Step 284] Loss: 0.8408 | Acc: 0.6875\n",
      "  [Step 285] Loss: 0.8269 | Acc: 0.6875\n",
      "  [Step 286] Loss: 0.5100 | Acc: 0.7812\n",
      "  [Step 287] Loss: 1.1573 | Acc: 0.7188\n",
      "  [Step 288] Loss: 0.5749 | Acc: 0.7188\n",
      "  [Step 289] Loss: 0.8347 | Acc: 0.5938\n",
      "  [Step 290] Loss: 0.6236 | Acc: 0.7500\n",
      "  [Step 291] Loss: 0.9418 | Acc: 0.6250\n",
      "  [Step 292] Loss: 0.4820 | Acc: 0.8125\n",
      "  [Step 293] Loss: 0.6013 | Acc: 0.7188\n",
      "  [Step 294] Loss: 0.7439 | Acc: 0.7188\n",
      "  [Step 295] Loss: 0.7146 | Acc: 0.6875\n",
      "  [Step 296] Loss: 0.8088 | Acc: 0.6875\n",
      "  [Step 297] Loss: 0.7544 | Acc: 0.6875\n",
      "  [Step 298] Loss: 0.8614 | Acc: 0.6875\n",
      "  [Step 299] Loss: 0.6196 | Acc: 0.8750\n",
      "  [Step 300] Loss: 0.6715 | Acc: 0.8125\n",
      "  [Step 301] Loss: 0.5638 | Acc: 0.7500\n",
      "  [Step 302] Loss: 1.2296 | Acc: 0.7188\n",
      "  [Step 303] Loss: 0.3643 | Acc: 0.9062\n",
      "  [Step 304] Loss: 0.5421 | Acc: 0.8438\n",
      "  [Step 305] Loss: 0.8126 | Acc: 0.7188\n",
      "  [Step 306] Loss: 0.8166 | Acc: 0.5938\n",
      "  [Step 307] Loss: 0.6353 | Acc: 0.7500\n",
      "  [Step 308] Loss: 1.2156 | Acc: 0.7500\n",
      "  [Step 309] Loss: 0.6159 | Acc: 0.7500\n",
      "  [Step 310] Loss: 0.7987 | Acc: 0.6250\n",
      "  [Step 311] Loss: 0.7090 | Acc: 0.6875\n",
      "  [Step 312] Loss: 0.5217 | Acc: 0.7188\n",
      "  [Step 313] Loss: 0.7758 | Acc: 0.6875\n",
      "  [Step 314] Loss: 0.7554 | Acc: 0.6250\n",
      "  [Step 315] Loss: 0.7364 | Acc: 0.6875\n",
      "  [Step 316] Loss: 0.6700 | Acc: 0.7812\n",
      "  [Step 317] Loss: 0.6276 | Acc: 0.8438\n",
      "  [Step 318] Loss: 0.9642 | Acc: 0.7188\n",
      "  [Step 319] Loss: 0.8589 | Acc: 0.6250\n",
      "  [Step 320] Loss: 0.6828 | Acc: 0.7500\n",
      "  [Step 321] Loss: 0.4855 | Acc: 0.8438\n",
      "  [Step 322] Loss: 0.7186 | Acc: 0.7500\n",
      "  [Step 323] Loss: 0.6960 | Acc: 0.7188\n",
      "  [Step 324] Loss: 1.0712 | Acc: 0.5625\n",
      "  [Step 325] Loss: 0.5969 | Acc: 0.8125\n",
      "  [Step 326] Loss: 1.1279 | Acc: 0.5312\n",
      "  [Step 327] Loss: 0.9276 | Acc: 0.7188\n",
      "  [Step 328] Loss: 0.8784 | Acc: 0.6875\n",
      "  [Step 329] Loss: 0.8518 | Acc: 0.6250\n",
      "  [Step 330] Loss: 0.5711 | Acc: 0.7812\n",
      "[Epoch 4] Train Loss: 0.2580 | Acc: 0.7122 || Val Loss: 0.8192 | Acc: 0.6800\n",
      "Precision: 0.5622 | Recall: 0.6161 | F1: 0.5762 | Time: 1078.34s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.10974382 0.0568589  0.10484489 0.06159373 0.10918941 0.10341681\n",
      "  0.11684407 0.13051294 0.10158413 0.1054113 ]\n",
      " [0.11797269 0.06212996 0.07498465 0.05957051 0.11462485 0.11635745\n",
      "  0.1102043  0.11338715 0.11461008 0.11615828]\n",
      " [0.10788994 0.06386521 0.11232248 0.06607358 0.10641658 0.10679889\n",
      "  0.11563632 0.11082934 0.10650976 0.10365791]\n",
      " [0.1148577  0.06769225 0.0792183  0.06748375 0.11541697 0.11491203\n",
      "  0.10884742 0.10698666 0.11188255 0.11270238]\n",
      " [0.10757148 0.08480762 0.09205797 0.08618127 0.10603563 0.10475478\n",
      "  0.10528611 0.10518344 0.10703943 0.10108221]\n",
      " [0.10886385 0.0706646  0.11049231 0.07257093 0.1078426  0.10586739\n",
      "  0.1063605  0.10787492 0.1012319  0.108231  ]\n",
      " [0.11142549 0.07237705 0.08259685 0.07166945 0.11115262 0.11055478\n",
      "  0.10855766 0.10949546 0.11132246 0.11084817]\n",
      " [0.10514034 0.08720119 0.09285936 0.08807021 0.10476372 0.10395656\n",
      "  0.10693784 0.10446615 0.10421885 0.10238585]\n",
      " [0.10755484 0.08408079 0.0899849  0.0851941  0.10643184 0.10503397\n",
      "  0.10529451 0.10531227 0.10628753 0.10482521]\n",
      " [0.11091545 0.06555779 0.10578499 0.06499586 0.11085553 0.10966244\n",
      "  0.10236464 0.11131696 0.10742336 0.11112297]\n",
      " [0.11282074 0.07062667 0.08277625 0.07065479 0.11008743 0.11211617\n",
      "  0.10518218 0.11311866 0.10958456 0.11303263]\n",
      " [0.10694646 0.08548507 0.09127552 0.08577811 0.1048713  0.10372026\n",
      "  0.10743058 0.10623    0.10327642 0.10498626]\n",
      " [0.10809723 0.08188875 0.08735307 0.08221853 0.10427348 0.10610794\n",
      "  0.10797014 0.10550185 0.10713591 0.10945307]\n",
      " [0.10985314 0.07740784 0.08382735 0.0780329  0.1080029  0.10760202\n",
      "  0.10866883 0.11326892 0.10724927 0.10608683]\n",
      " [0.11005611 0.06871127 0.10623461 0.06728625 0.10773519 0.10879961\n",
      "  0.10764549 0.10764689 0.10984252 0.10604206]\n",
      " [0.11091366 0.07163838 0.08425862 0.07123564 0.10995825 0.10854982\n",
      "  0.11064844 0.11640008 0.10732183 0.10907528]\n",
      " [0.10602494 0.08696593 0.09242783 0.08729936 0.10501863 0.10540058\n",
      "  0.10468703 0.10541074 0.10421433 0.10255062]\n",
      " [0.1071319  0.08286036 0.08883127 0.08296548 0.10604433 0.10782211\n",
      "  0.10510878 0.10680885 0.10586514 0.10656183]\n",
      " [0.10943563 0.07928725 0.08553912 0.07980687 0.10858721 0.1088273\n",
      "  0.10516165 0.11175852 0.10424622 0.10735022]\n",
      " [0.10985807 0.07461839 0.08012064 0.07489011 0.10690077 0.10999552\n",
      "  0.10664434 0.10985843 0.11246337 0.11465035]\n",
      " [0.10860416 0.0700646  0.09596943 0.06827905 0.10840336 0.10855623\n",
      "  0.11105842 0.11222588 0.10885626 0.10798261]\n",
      " [0.11190938 0.07546476 0.08018688 0.07482864 0.11053392 0.11207651\n",
      "  0.11018234 0.10749625 0.10999793 0.10732338]\n",
      " [0.10536311 0.08764328 0.09330879 0.08755194 0.10534233 0.10323495\n",
      "  0.10351247 0.1040663  0.10315772 0.10681909]\n",
      " [0.10706616 0.08411912 0.08979655 0.08411417 0.10619049 0.1059946\n",
      "  0.10615091 0.10678283 0.10453311 0.10525206]\n",
      " [0.10843884 0.07965556 0.08665005 0.07962842 0.10901697 0.10731228\n",
      "  0.10970081 0.10515546 0.1099996  0.10444207]\n",
      " [0.11022405 0.07618416 0.08219415 0.07614953 0.10968464 0.10907934\n",
      "  0.10891187 0.10807168 0.10839128 0.11110935]\n",
      " [0.11162469 0.07142839 0.0779783  0.07165083 0.11186079 0.11032714\n",
      "  0.11510761 0.11176401 0.10940851 0.10884977]\n",
      " [0.10836299 0.06703279 0.11352479 0.06516826 0.10839538 0.10855339\n",
      "  0.10571457 0.10880195 0.10603327 0.10841263]\n",
      " [0.11106387 0.07449018 0.08717147 0.07322413 0.1102341  0.10937048\n",
      "  0.10920851 0.10743091 0.10970582 0.10810053]\n",
      " [0.1054879  0.08768389 0.09560294 0.08737934 0.10258092 0.10384141\n",
      "  0.10624959 0.10325357 0.10216384 0.10575663]\n",
      " [0.10633769 0.08436909 0.09280598 0.08402821 0.10609283 0.10584236\n",
      "  0.10580751 0.1066956  0.10582538 0.10219537]\n",
      " [0.10694875 0.08064061 0.09010512 0.08048641 0.10713862 0.10537887\n",
      "  0.10759889 0.10624716 0.10783391 0.10762164]\n",
      " [0.10950298 0.07793995 0.08730324 0.077599   0.10854418 0.10855731\n",
      "  0.10861409 0.10950204 0.10716151 0.10527562]\n",
      " [0.11075771 0.07334726 0.08348358 0.07307947 0.11028818 0.11027656\n",
      "  0.10771457 0.11230119 0.11050957 0.10824188]\n",
      " [0.11208252 0.06925496 0.07987903 0.06887946 0.111286   0.11067958\n",
      "  0.11233867 0.11807171 0.11020467 0.10732336]]\n",
      "\n",
      "[Epoch 5/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 1.7545 | Acc: 0.3750\n",
      "  [Step 001] Loss: 1.0028 | Acc: 0.5938\n",
      "  [Step 002] Loss: 0.7988 | Acc: 0.6562\n",
      "  [Step 003] Loss: 0.9595 | Acc: 0.5938\n",
      "  [Step 004] Loss: 0.7440 | Acc: 0.6875\n",
      "  [Step 005] Loss: 0.6413 | Acc: 0.6562\n",
      "  [Step 006] Loss: 0.9647 | Acc: 0.5312\n",
      "  [Step 007] Loss: 0.9811 | Acc: 0.5938\n",
      "  [Step 008] Loss: 0.7432 | Acc: 0.6562\n",
      "  [Step 009] Loss: 0.8550 | Acc: 0.6562\n",
      "  [Step 010] Loss: 0.9420 | Acc: 0.5312\n",
      "  [Step 011] Loss: 0.6981 | Acc: 0.6875\n",
      "  [Step 012] Loss: 0.8445 | Acc: 0.6250\n",
      "  [Step 013] Loss: 0.7145 | Acc: 0.7188\n",
      "  [Step 014] Loss: 0.7594 | Acc: 0.6875\n",
      "  [Step 015] Loss: 0.4842 | Acc: 0.8438\n",
      "  [Step 016] Loss: 0.8283 | Acc: 0.7188\n",
      "  [Step 017] Loss: 0.7042 | Acc: 0.7812\n",
      "  [Step 018] Loss: 1.0358 | Acc: 0.6250\n",
      "  [Step 019] Loss: 0.8721 | Acc: 0.5312\n",
      "  [Step 020] Loss: 0.5858 | Acc: 0.8750\n",
      "  [Step 021] Loss: 0.7050 | Acc: 0.7812\n",
      "  [Step 022] Loss: 0.8689 | Acc: 0.6250\n",
      "  [Step 023] Loss: 0.4954 | Acc: 0.8438\n",
      "  [Step 024] Loss: 0.7403 | Acc: 0.7188\n",
      "  [Step 025] Loss: 1.4058 | Acc: 0.5938\n",
      "  [Step 026] Loss: 0.5708 | Acc: 0.8125\n",
      "  [Step 027] Loss: 0.6582 | Acc: 0.7500\n",
      "  [Step 028] Loss: 0.6941 | Acc: 0.7500\n",
      "  [Step 029] Loss: 0.5750 | Acc: 0.7812\n",
      "  [Step 030] Loss: 1.2010 | Acc: 0.5938\n",
      "  [Step 031] Loss: 0.9038 | Acc: 0.6250\n",
      "  [Step 032] Loss: 0.9749 | Acc: 0.5938\n",
      "  [Step 033] Loss: 1.0867 | Acc: 0.6562\n",
      "  [Step 034] Loss: 0.6747 | Acc: 0.7812\n",
      "  [Step 035] Loss: 0.7921 | Acc: 0.7500\n",
      "  [Step 036] Loss: 0.5821 | Acc: 0.8125\n",
      "  [Step 037] Loss: 0.6988 | Acc: 0.6562\n",
      "  [Step 038] Loss: 0.7058 | Acc: 0.7500\n",
      "  [Step 039] Loss: 0.7126 | Acc: 0.7812\n",
      "  [Step 040] Loss: 0.6889 | Acc: 0.7500\n",
      "  [Step 041] Loss: 0.8780 | Acc: 0.6875\n",
      "  [Step 042] Loss: 0.7993 | Acc: 0.6562\n",
      "  [Step 043] Loss: 0.5919 | Acc: 0.8438\n",
      "  [Step 044] Loss: 0.5122 | Acc: 0.8438\n",
      "  [Step 045] Loss: 0.6779 | Acc: 0.7500\n",
      "  [Step 046] Loss: 0.7629 | Acc: 0.7500\n",
      "  [Step 047] Loss: 0.6955 | Acc: 0.8125\n",
      "  [Step 048] Loss: 0.7920 | Acc: 0.7188\n",
      "  [Step 049] Loss: 0.8240 | Acc: 0.6250\n",
      "  [Step 050] Loss: 0.9766 | Acc: 0.6250\n",
      "  [Step 051] Loss: 0.6943 | Acc: 0.7500\n",
      "  [Step 052] Loss: 0.7710 | Acc: 0.7188\n",
      "  [Step 053] Loss: 0.8305 | Acc: 0.6875\n",
      "  [Step 054] Loss: 0.7566 | Acc: 0.6875\n",
      "  [Step 055] Loss: 0.5174 | Acc: 0.8750\n",
      "  [Step 056] Loss: 1.0239 | Acc: 0.5312\n",
      "  [Step 057] Loss: 0.7078 | Acc: 0.7500\n",
      "  [Step 058] Loss: 0.7917 | Acc: 0.6562\n",
      "  [Step 059] Loss: 0.6511 | Acc: 0.8125\n",
      "  [Step 060] Loss: 0.8454 | Acc: 0.6562\n",
      "  [Step 061] Loss: 1.2242 | Acc: 0.5938\n",
      "  [Step 062] Loss: 1.0902 | Acc: 0.6875\n",
      "  [Step 063] Loss: 0.5209 | Acc: 0.8125\n",
      "  [Step 064] Loss: 0.6214 | Acc: 0.8438\n",
      "  [Step 065] Loss: 0.7873 | Acc: 0.6875\n",
      "  [Step 066] Loss: 0.8028 | Acc: 0.7500\n",
      "  [Step 067] Loss: 0.6930 | Acc: 0.7500\n",
      "  [Step 068] Loss: 0.8238 | Acc: 0.7500\n",
      "  [Step 069] Loss: 1.1220 | Acc: 0.5938\n",
      "  [Step 070] Loss: 0.7529 | Acc: 0.6875\n",
      "  [Step 071] Loss: 0.7968 | Acc: 0.7500\n",
      "  [Step 072] Loss: 0.9309 | Acc: 0.6250\n",
      "  [Step 073] Loss: 0.8002 | Acc: 0.6875\n",
      "  [Step 074] Loss: 0.7760 | Acc: 0.6875\n",
      "  [Step 075] Loss: 0.4714 | Acc: 0.8125\n",
      "  [Step 076] Loss: 0.7737 | Acc: 0.6562\n",
      "  [Step 077] Loss: 0.8439 | Acc: 0.6250\n",
      "  [Step 078] Loss: 0.7226 | Acc: 0.7188\n",
      "  [Step 079] Loss: 0.8454 | Acc: 0.6875\n",
      "  [Step 080] Loss: 0.9514 | Acc: 0.7188\n",
      "  [Step 081] Loss: 0.4182 | Acc: 0.9062\n",
      "  [Step 082] Loss: 0.5103 | Acc: 0.8125\n",
      "  [Step 083] Loss: 0.6182 | Acc: 0.7188\n",
      "  [Step 084] Loss: 0.8576 | Acc: 0.6875\n",
      "  [Step 085] Loss: 0.6350 | Acc: 0.5938\n",
      "  [Step 086] Loss: 0.7614 | Acc: 0.7500\n",
      "  [Step 087] Loss: 0.7706 | Acc: 0.6562\n",
      "  [Step 088] Loss: 1.0275 | Acc: 0.6875\n",
      "  [Step 089] Loss: 0.9064 | Acc: 0.6562\n",
      "  [Step 090] Loss: 0.8405 | Acc: 0.6250\n",
      "  [Step 091] Loss: 0.5667 | Acc: 0.7812\n",
      "  [Step 092] Loss: 0.8344 | Acc: 0.7812\n",
      "  [Step 093] Loss: 0.7683 | Acc: 0.6250\n",
      "  [Step 094] Loss: 0.7812 | Acc: 0.7812\n",
      "  [Step 095] Loss: 0.8166 | Acc: 0.7812\n",
      "  [Step 096] Loss: 0.8865 | Acc: 0.6562\n",
      "  [Step 097] Loss: 0.5929 | Acc: 0.7812\n",
      "  [Step 098] Loss: 0.6669 | Acc: 0.7188\n",
      "  [Step 099] Loss: 0.6934 | Acc: 0.6250\n",
      "  [Step 100] Loss: 0.5425 | Acc: 0.7500\n",
      "  [Step 101] Loss: 0.5765 | Acc: 0.7188\n",
      "  [Step 102] Loss: 0.5456 | Acc: 0.8750\n",
      "  [Step 103] Loss: 0.6269 | Acc: 0.8438\n",
      "  [Step 104] Loss: 0.9399 | Acc: 0.7188\n",
      "  [Step 105] Loss: 0.6160 | Acc: 0.6875\n",
      "  [Step 106] Loss: 0.8636 | Acc: 0.7812\n",
      "  [Step 107] Loss: 0.8396 | Acc: 0.5938\n",
      "  [Step 108] Loss: 0.8048 | Acc: 0.5938\n",
      "  [Step 109] Loss: 0.9004 | Acc: 0.7500\n",
      "  [Step 110] Loss: 0.6902 | Acc: 0.7188\n",
      "  [Step 111] Loss: 1.0264 | Acc: 0.6562\n",
      "  [Step 112] Loss: 0.7743 | Acc: 0.7188\n",
      "  [Step 113] Loss: 0.9327 | Acc: 0.6250\n",
      "  [Step 114] Loss: 1.0053 | Acc: 0.5312\n",
      "  [Step 115] Loss: 1.1031 | Acc: 0.5625\n",
      "  [Step 116] Loss: 0.9745 | Acc: 0.5938\n",
      "  [Step 117] Loss: 0.8317 | Acc: 0.7500\n",
      "  [Step 118] Loss: 0.6442 | Acc: 0.6875\n",
      "  [Step 119] Loss: 0.8519 | Acc: 0.6562\n",
      "  [Step 120] Loss: 0.8302 | Acc: 0.6562\n",
      "  [Step 121] Loss: 1.1901 | Acc: 0.6250\n",
      "  [Step 122] Loss: 0.9832 | Acc: 0.6875\n",
      "  [Step 123] Loss: 0.6317 | Acc: 0.7188\n",
      "  [Step 124] Loss: 0.8745 | Acc: 0.6250\n",
      "  [Step 125] Loss: 0.9247 | Acc: 0.6875\n",
      "  [Step 126] Loss: 0.7289 | Acc: 0.8125\n",
      "  [Step 127] Loss: 1.0254 | Acc: 0.5625\n",
      "  [Step 128] Loss: 0.6872 | Acc: 0.7500\n",
      "  [Step 129] Loss: 1.1049 | Acc: 0.5312\n",
      "  [Step 130] Loss: 0.6659 | Acc: 0.8438\n",
      "  [Step 131] Loss: 0.7584 | Acc: 0.7500\n",
      "  [Step 132] Loss: 0.8728 | Acc: 0.7188\n",
      "  [Step 133] Loss: 0.7935 | Acc: 0.6875\n",
      "  [Step 134] Loss: 0.5074 | Acc: 0.8125\n",
      "  [Step 135] Loss: 0.7742 | Acc: 0.6562\n",
      "  [Step 136] Loss: 0.8408 | Acc: 0.6562\n",
      "  [Step 137] Loss: 0.7505 | Acc: 0.6875\n",
      "  [Step 138] Loss: 0.6752 | Acc: 0.7188\n",
      "  [Step 139] Loss: 0.7671 | Acc: 0.6875\n",
      "  [Step 140] Loss: 0.7882 | Acc: 0.6562\n",
      "  [Step 141] Loss: 0.6031 | Acc: 0.8125\n",
      "  [Step 142] Loss: 0.8642 | Acc: 0.5312\n",
      "  [Step 143] Loss: 0.7313 | Acc: 0.6875\n",
      "  [Step 144] Loss: 0.7248 | Acc: 0.7812\n",
      "  [Step 145] Loss: 0.7560 | Acc: 0.6875\n",
      "  [Step 146] Loss: 0.6199 | Acc: 0.7812\n",
      "  [Step 147] Loss: 0.5781 | Acc: 0.8125\n",
      "  [Step 148] Loss: 1.0382 | Acc: 0.5625\n",
      "  [Step 149] Loss: 0.5876 | Acc: 0.7812\n",
      "  [Step 150] Loss: 0.7404 | Acc: 0.5938\n",
      "  [Step 151] Loss: 0.9661 | Acc: 0.6875\n",
      "  [Step 152] Loss: 0.4684 | Acc: 0.8750\n",
      "  [Step 153] Loss: 0.5105 | Acc: 0.8750\n",
      "  [Step 154] Loss: 0.6610 | Acc: 0.7812\n",
      "  [Step 155] Loss: 0.9900 | Acc: 0.6875\n",
      "  [Step 156] Loss: 0.6712 | Acc: 0.7812\n",
      "  [Step 157] Loss: 0.5139 | Acc: 0.8438\n",
      "  [Step 158] Loss: 0.8450 | Acc: 0.5938\n",
      "  [Step 159] Loss: 0.7337 | Acc: 0.6875\n",
      "  [Step 160] Loss: 0.7839 | Acc: 0.7812\n",
      "  [Step 161] Loss: 0.8507 | Acc: 0.7188\n",
      "  [Step 162] Loss: 0.6896 | Acc: 0.7500\n",
      "  [Step 163] Loss: 0.8178 | Acc: 0.6250\n",
      "  [Step 164] Loss: 0.5655 | Acc: 0.8125\n",
      "  [Step 165] Loss: 0.9716 | Acc: 0.7500\n",
      "  [Step 166] Loss: 0.5079 | Acc: 0.8125\n",
      "  [Step 167] Loss: 0.9956 | Acc: 0.7188\n",
      "  [Step 168] Loss: 0.6213 | Acc: 0.7812\n",
      "  [Step 169] Loss: 0.7546 | Acc: 0.6562\n",
      "  [Step 170] Loss: 0.6526 | Acc: 0.7812\n",
      "  [Step 171] Loss: 0.6315 | Acc: 0.7500\n",
      "  [Step 172] Loss: 0.6214 | Acc: 0.7812\n",
      "  [Step 173] Loss: 0.6593 | Acc: 0.7500\n",
      "  [Step 174] Loss: 0.6856 | Acc: 0.7500\n",
      "  [Step 175] Loss: 0.5446 | Acc: 0.7812\n",
      "  [Step 176] Loss: 0.6290 | Acc: 0.7188\n",
      "  [Step 177] Loss: 0.6654 | Acc: 0.6562\n",
      "  [Step 178] Loss: 1.2915 | Acc: 0.6250\n",
      "  [Step 179] Loss: 0.7532 | Acc: 0.7500\n",
      "  [Step 180] Loss: 0.6637 | Acc: 0.8438\n",
      "  [Step 181] Loss: 0.7011 | Acc: 0.7188\n",
      "  [Step 182] Loss: 0.6755 | Acc: 0.7188\n",
      "  [Step 183] Loss: 0.5029 | Acc: 0.8125\n",
      "  [Step 184] Loss: 0.5964 | Acc: 0.7812\n",
      "  [Step 185] Loss: 0.7651 | Acc: 0.7812\n",
      "  [Step 186] Loss: 0.6277 | Acc: 0.7188\n",
      "  [Step 187] Loss: 0.9289 | Acc: 0.6875\n",
      "  [Step 188] Loss: 0.5653 | Acc: 0.7500\n",
      "  [Step 189] Loss: 0.8345 | Acc: 0.6562\n",
      "  [Step 190] Loss: 0.8323 | Acc: 0.6562\n",
      "  [Step 191] Loss: 0.8826 | Acc: 0.6875\n",
      "  [Step 192] Loss: 1.1136 | Acc: 0.5938\n",
      "  [Step 193] Loss: 0.6562 | Acc: 0.7500\n",
      "  [Step 194] Loss: 0.5279 | Acc: 0.7812\n",
      "  [Step 195] Loss: 0.7172 | Acc: 0.7500\n",
      "  [Step 196] Loss: 0.7795 | Acc: 0.6562\n",
      "  [Step 197] Loss: 0.4776 | Acc: 0.7500\n",
      "  [Step 198] Loss: 0.4644 | Acc: 0.8438\n",
      "  [Step 199] Loss: 1.0293 | Acc: 0.6562\n",
      "  [Step 200] Loss: 0.4360 | Acc: 0.8438\n",
      "  [Step 201] Loss: 0.8611 | Acc: 0.6250\n",
      "  [Step 202] Loss: 0.6209 | Acc: 0.7812\n",
      "  [Step 203] Loss: 0.9634 | Acc: 0.8125\n",
      "  [Step 204] Loss: 0.7101 | Acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 0.9722 | Acc: 0.7188\n",
      "  [Step 206] Loss: 0.5365 | Acc: 0.8125\n",
      "  [Step 207] Loss: 0.9048 | Acc: 0.6562\n",
      "  [Step 208] Loss: 0.6458 | Acc: 0.7500\n",
      "  [Step 209] Loss: 0.8780 | Acc: 0.6562\n",
      "  [Step 210] Loss: 0.6533 | Acc: 0.7812\n",
      "  [Step 211] Loss: 0.7340 | Acc: 0.6875\n",
      "  [Step 212] Loss: 0.7282 | Acc: 0.7500\n",
      "  [Step 213] Loss: 0.6247 | Acc: 0.6875\n",
      "  [Step 214] Loss: 0.9919 | Acc: 0.6562\n",
      "  [Step 215] Loss: 1.0398 | Acc: 0.7812\n",
      "  [Step 216] Loss: 0.7760 | Acc: 0.6875\n",
      "  [Step 217] Loss: 0.8458 | Acc: 0.6562\n",
      "  [Step 218] Loss: 0.5450 | Acc: 0.8125\n",
      "  [Step 219] Loss: 0.5244 | Acc: 0.8125\n",
      "  [Step 220] Loss: 0.7513 | Acc: 0.6562\n",
      "  [Step 221] Loss: 0.5625 | Acc: 0.7812\n",
      "  [Step 222] Loss: 0.6162 | Acc: 0.8125\n",
      "  [Step 223] Loss: 0.6785 | Acc: 0.6875\n",
      "  [Step 224] Loss: 0.6904 | Acc: 0.7500\n",
      "  [Step 225] Loss: 0.7872 | Acc: 0.8125\n",
      "  [Step 226] Loss: 0.7226 | Acc: 0.7812\n",
      "  [Step 227] Loss: 0.5483 | Acc: 0.8438\n",
      "  [Step 228] Loss: 0.8019 | Acc: 0.7812\n",
      "  [Step 229] Loss: 0.7996 | Acc: 0.6875\n",
      "  [Step 230] Loss: 0.5646 | Acc: 0.8125\n",
      "  [Step 231] Loss: 1.0205 | Acc: 0.5938\n",
      "  [Step 232] Loss: 1.0709 | Acc: 0.6562\n",
      "  [Step 233] Loss: 0.7483 | Acc: 0.7812\n",
      "  [Step 234] Loss: 0.9469 | Acc: 0.5938\n",
      "  [Step 235] Loss: 0.6361 | Acc: 0.7812\n",
      "  [Step 236] Loss: 0.5579 | Acc: 0.8125\n",
      "  [Step 237] Loss: 1.0044 | Acc: 0.6875\n",
      "  [Step 238] Loss: 0.9852 | Acc: 0.6875\n",
      "  [Step 239] Loss: 0.7598 | Acc: 0.7188\n",
      "  [Step 240] Loss: 0.5113 | Acc: 0.8125\n",
      "  [Step 241] Loss: 0.5659 | Acc: 0.8125\n",
      "  [Step 242] Loss: 0.6850 | Acc: 0.8438\n",
      "  [Step 243] Loss: 0.6009 | Acc: 0.6875\n",
      "  [Step 244] Loss: 0.5057 | Acc: 0.8125\n",
      "  [Step 245] Loss: 0.6019 | Acc: 0.6250\n",
      "  [Step 246] Loss: 0.6550 | Acc: 0.7812\n",
      "  [Step 247] Loss: 0.4963 | Acc: 0.8125\n",
      "  [Step 248] Loss: 0.8292 | Acc: 0.6875\n",
      "  [Step 249] Loss: 0.6972 | Acc: 0.6875\n",
      "  [Step 250] Loss: 0.6552 | Acc: 0.7188\n",
      "  [Step 251] Loss: 0.6592 | Acc: 0.7500\n",
      "  [Step 252] Loss: 0.7208 | Acc: 0.7500\n",
      "  [Step 253] Loss: 0.8609 | Acc: 0.7188\n",
      "  [Step 254] Loss: 0.7185 | Acc: 0.8125\n",
      "  [Step 255] Loss: 0.9566 | Acc: 0.7812\n",
      "  [Step 256] Loss: 0.8132 | Acc: 0.6562\n",
      "  [Step 257] Loss: 0.5947 | Acc: 0.8125\n",
      "  [Step 258] Loss: 0.7431 | Acc: 0.7188\n",
      "  [Step 259] Loss: 0.4462 | Acc: 0.8750\n",
      "  [Step 260] Loss: 0.8409 | Acc: 0.6562\n",
      "  [Step 261] Loss: 0.3788 | Acc: 0.8750\n",
      "  [Step 262] Loss: 0.7616 | Acc: 0.7188\n",
      "  [Step 263] Loss: 0.6408 | Acc: 0.8125\n",
      "  [Step 264] Loss: 0.6418 | Acc: 0.7500\n",
      "  [Step 265] Loss: 0.5422 | Acc: 0.7812\n",
      "  [Step 266] Loss: 0.5531 | Acc: 0.7500\n",
      "  [Step 267] Loss: 0.4559 | Acc: 0.8750\n",
      "  [Step 268] Loss: 0.8733 | Acc: 0.6875\n",
      "  [Step 269] Loss: 0.9923 | Acc: 0.6250\n",
      "  [Step 270] Loss: 0.4965 | Acc: 0.8125\n",
      "  [Step 271] Loss: 0.6567 | Acc: 0.7500\n",
      "  [Step 272] Loss: 0.8569 | Acc: 0.7188\n",
      "  [Step 273] Loss: 0.7786 | Acc: 0.7812\n",
      "  [Step 274] Loss: 0.6864 | Acc: 0.6875\n",
      "  [Step 275] Loss: 0.7177 | Acc: 0.7812\n",
      "  [Step 276] Loss: 2.0481 | Acc: 0.7812\n",
      "  [Step 277] Loss: 0.7118 | Acc: 0.7500\n",
      "  [Step 278] Loss: 0.8378 | Acc: 0.7188\n",
      "  [Step 279] Loss: 0.7206 | Acc: 0.7188\n",
      "  [Step 280] Loss: 0.6366 | Acc: 0.7188\n",
      "  [Step 281] Loss: 0.6534 | Acc: 0.7500\n",
      "  [Step 282] Loss: 0.6694 | Acc: 0.7188\n",
      "  [Step 283] Loss: 0.8090 | Acc: 0.6250\n",
      "  [Step 284] Loss: 0.8401 | Acc: 0.6250\n",
      "  [Step 285] Loss: 0.5905 | Acc: 0.7812\n",
      "  [Step 286] Loss: 1.0137 | Acc: 0.5312\n",
      "  [Step 287] Loss: 0.8128 | Acc: 0.6875\n",
      "  [Step 288] Loss: 0.6838 | Acc: 0.6875\n",
      "  [Step 289] Loss: 0.4746 | Acc: 0.8438\n",
      "  [Step 290] Loss: 0.6141 | Acc: 0.7500\n",
      "  [Step 291] Loss: 0.6378 | Acc: 0.6562\n",
      "  [Step 292] Loss: 0.6928 | Acc: 0.6875\n",
      "  [Step 293] Loss: 0.5920 | Acc: 0.7500\n",
      "  [Step 294] Loss: 0.7020 | Acc: 0.7500\n",
      "  [Step 295] Loss: 0.9746 | Acc: 0.6875\n",
      "  [Step 296] Loss: 0.5874 | Acc: 0.8125\n",
      "  [Step 297] Loss: 0.5628 | Acc: 0.8438\n",
      "  [Step 298] Loss: 1.3025 | Acc: 0.5938\n",
      "  [Step 299] Loss: 0.7937 | Acc: 0.6875\n",
      "  [Step 300] Loss: 0.5004 | Acc: 0.7812\n",
      "  [Step 301] Loss: 0.9578 | Acc: 0.6250\n",
      "  [Step 302] Loss: 0.7689 | Acc: 0.7500\n",
      "  [Step 303] Loss: 1.0443 | Acc: 0.5938\n",
      "  [Step 304] Loss: 0.6804 | Acc: 0.7188\n",
      "  [Step 305] Loss: 0.9956 | Acc: 0.6562\n",
      "  [Step 306] Loss: 0.9441 | Acc: 0.5625\n",
      "  [Step 307] Loss: 0.8457 | Acc: 0.5938\n",
      "  [Step 308] Loss: 0.4931 | Acc: 0.8750\n",
      "  [Step 309] Loss: 0.7650 | Acc: 0.7812\n",
      "  [Step 310] Loss: 0.4721 | Acc: 0.8438\n",
      "  [Step 311] Loss: 1.2577 | Acc: 0.5625\n",
      "  [Step 312] Loss: 0.7653 | Acc: 0.7500\n",
      "  [Step 313] Loss: 0.8046 | Acc: 0.6562\n",
      "  [Step 314] Loss: 0.6344 | Acc: 0.7500\n",
      "  [Step 315] Loss: 0.8279 | Acc: 0.6875\n",
      "  [Step 316] Loss: 0.4833 | Acc: 0.7812\n",
      "  [Step 317] Loss: 0.7920 | Acc: 0.6875\n",
      "  [Step 318] Loss: 0.7873 | Acc: 0.7188\n",
      "  [Step 319] Loss: 0.8289 | Acc: 0.7188\n",
      "  [Step 320] Loss: 0.7502 | Acc: 0.7500\n",
      "  [Step 321] Loss: 0.6790 | Acc: 0.6875\n",
      "  [Step 322] Loss: 0.6275 | Acc: 0.7500\n",
      "  [Step 323] Loss: 0.5523 | Acc: 0.8438\n",
      "  [Step 324] Loss: 0.5367 | Acc: 0.7812\n",
      "  [Step 325] Loss: 0.7360 | Acc: 0.7500\n",
      "  [Step 326] Loss: 0.7086 | Acc: 0.7500\n",
      "  [Step 327] Loss: 0.8141 | Acc: 0.6562\n",
      "  [Step 328] Loss: 0.6285 | Acc: 0.7500\n",
      "  [Step 329] Loss: 0.6130 | Acc: 0.7500\n",
      "  [Step 330] Loss: 0.6416 | Acc: 0.8125\n",
      "[Epoch 5] Train Loss: 0.2533 | Acc: 0.7187 || Val Loss: 0.7029 | Acc: 0.7369\n",
      "Precision: 0.6088 | Recall: 0.5962 | F1: 0.5980 | Time: 1070.73s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.10766993 0.05338665 0.10954469 0.05821248 0.10738543 0.10159996\n",
      "  0.11600982 0.14337945 0.09924192 0.10356969]\n",
      " [0.11927094 0.05976304 0.07570186 0.05726016 0.11489424 0.11598121\n",
      "  0.11088507 0.11472803 0.11558389 0.11593161]\n",
      " [0.10761708 0.0608605  0.11982862 0.06313229 0.10604764 0.10619181\n",
      "  0.11785355 0.11212922 0.10318307 0.10315617]\n",
      " [0.11583886 0.06542781 0.07997718 0.0651973  0.11566725 0.11541054\n",
      "  0.10924237 0.10734648 0.11264597 0.11324622]\n",
      " [0.10818972 0.08393127 0.09347953 0.08568602 0.10682904 0.10520087\n",
      "  0.10564285 0.10565264 0.10576816 0.09961984]\n",
      " [0.10891199 0.06762268 0.11507924 0.06947958 0.10784186 0.10589793\n",
      "  0.10680515 0.10835955 0.1005243  0.10947768]\n",
      " [0.11212537 0.07015745 0.08297542 0.06954193 0.11114753 0.11118768\n",
      "  0.10948261 0.10973619 0.11270959 0.11093624]\n",
      " [0.10567962 0.08618931 0.09362166 0.08739372 0.10495085 0.10372289\n",
      "  0.10686813 0.10497332 0.10476922 0.10183129]\n",
      " [0.10832511 0.08267651 0.09000308 0.08406867 0.10674711 0.10570665\n",
      "  0.10629901 0.10555834 0.1060907  0.1045249 ]\n",
      " [0.11203817 0.06178513 0.1073551  0.06121816 0.1110462  0.11045218\n",
      "  0.10260884 0.11540653 0.1062383  0.11185139]\n",
      " [0.11366705 0.06868219 0.08343941 0.06868999 0.11148725 0.11318436\n",
      "  0.10501755 0.11359354 0.10996345 0.11227518]\n",
      " [0.10745195 0.08392417 0.09122936 0.08441932 0.10557412 0.10406861\n",
      "  0.10783546 0.10715924 0.10417497 0.10416273]\n",
      " [0.10906649 0.07975004 0.08634374 0.0801786  0.10475393 0.10682259\n",
      "  0.1088943  0.1073015  0.10781352 0.10907532]\n",
      " [0.11057326 0.07535001 0.08313528 0.07621527 0.10812306 0.10816453\n",
      "  0.10906685 0.11573041 0.10781179 0.10582955]\n",
      " [0.1100345  0.06549587 0.11040445 0.06421203 0.10804154 0.10923351\n",
      "  0.1073371  0.10800233 0.10971647 0.10752209]\n",
      " [0.11197204 0.06893828 0.08406935 0.06841078 0.11060483 0.11000521\n",
      "  0.11155596 0.119191   0.10628814 0.10896439]\n",
      " [0.10653151 0.08580681 0.09299944 0.08622403 0.1053909  0.10541502\n",
      "  0.10503556 0.10590224 0.1039211  0.10277338]\n",
      " [0.10743913 0.08118478 0.08870926 0.08136143 0.10619745 0.10805167\n",
      "  0.10643321 0.10657487 0.1075338  0.10651432]\n",
      " [0.11028816 0.07772285 0.08561352 0.07830039 0.10911728 0.11016338\n",
      "  0.10587066 0.11263838 0.10414425 0.10614114]\n",
      " [0.11014099 0.07290506 0.07982749 0.07326666 0.10706817 0.11046864\n",
      "  0.10615145 0.11138524 0.11437888 0.11440741]\n",
      " [0.10907365 0.06567527 0.09731019 0.06415682 0.10876783 0.10890215\n",
      "  0.1130145  0.11600226 0.10820614 0.1088912 ]\n",
      " [0.11346192 0.07287294 0.08006834 0.07205937 0.11112857 0.11283385\n",
      "  0.11083184 0.10860763 0.11023043 0.10790506]\n",
      " [0.10610361 0.0859872  0.09351644 0.08603682 0.1063095  0.10358525\n",
      "  0.10387278 0.10480745 0.10249059 0.10729033]\n",
      " [0.10803291 0.08194501 0.08915084 0.08195984 0.10686401 0.10634345\n",
      "  0.10704653 0.10706362 0.10591064 0.10568319]\n",
      " [0.10947225 0.07755189 0.08641699 0.07761742 0.10949461 0.10790648\n",
      "  0.11004019 0.1055032  0.11001107 0.10598592]\n",
      " [0.11153167 0.07422625 0.08195775 0.07424753 0.11046994 0.11042356\n",
      "  0.10918418 0.10759077 0.10907624 0.11129211]\n",
      " [0.11243279 0.06955747 0.0779341  0.06987433 0.11245456 0.11047256\n",
      "  0.11536731 0.11228678 0.10938606 0.11023407]\n",
      " [0.10826823 0.06274784 0.11820173 0.06110707 0.10846633 0.10850143\n",
      "  0.1054598  0.11102182 0.10647122 0.10975458]\n",
      " [0.11176138 0.07219279 0.08819304 0.07079599 0.11082465 0.11026067\n",
      "  0.10896115 0.10857105 0.10993892 0.10850046]\n",
      " [0.10588535 0.0864598  0.09685596 0.08618095 0.10258868 0.10371182\n",
      "  0.10599197 0.10292937 0.10281679 0.10657933]\n",
      " [0.10690911 0.08275695 0.09358692 0.082417   0.10687206 0.10661031\n",
      "  0.1060514  0.10715682 0.10633501 0.10130447]\n",
      " [0.10761761 0.07893226 0.09091262 0.07878333 0.10744736 0.10544561\n",
      "  0.10742351 0.10647422 0.1088253  0.1081382 ]\n",
      " [0.11014712 0.07644797 0.08840562 0.07609381 0.10869373 0.10916193\n",
      "  0.11017592 0.10975751 0.10664259 0.10447378]\n",
      " [0.11095606 0.07189344 0.0848466  0.07166179 0.11009821 0.11069658\n",
      "  0.10747527 0.112797   0.11177008 0.10780497]\n",
      " [0.11283549 0.06774236 0.08101662 0.06740875 0.11197297 0.11134788\n",
      "  0.10940337 0.11894636 0.11218794 0.10713829]]\n",
      "\n",
      "[Epoch 6/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 1.6495 | Acc: 0.5938\n",
      "  [Step 001] Loss: 1.1327 | Acc: 0.5625\n",
      "  [Step 002] Loss: 1.0268 | Acc: 0.5938\n",
      "  [Step 003] Loss: 0.9358 | Acc: 0.6250\n",
      "  [Step 004] Loss: 0.7954 | Acc: 0.6875\n",
      "  [Step 005] Loss: 0.6523 | Acc: 0.7500\n",
      "  [Step 006] Loss: 0.5697 | Acc: 0.6875\n",
      "  [Step 007] Loss: 0.8231 | Acc: 0.6875\n",
      "  [Step 008] Loss: 0.7369 | Acc: 0.7500\n",
      "  [Step 009] Loss: 0.5635 | Acc: 0.8125\n",
      "  [Step 010] Loss: 0.4695 | Acc: 0.8125\n",
      "  [Step 011] Loss: 0.5153 | Acc: 0.7812\n",
      "  [Step 012] Loss: 0.8198 | Acc: 0.6875\n",
      "  [Step 013] Loss: 1.0590 | Acc: 0.5625\n",
      "  [Step 014] Loss: 0.5632 | Acc: 0.8125\n",
      "  [Step 015] Loss: 0.5675 | Acc: 0.7812\n",
      "  [Step 016] Loss: 0.4042 | Acc: 0.9062\n",
      "  [Step 017] Loss: 0.8071 | Acc: 0.6875\n",
      "  [Step 018] Loss: 0.5989 | Acc: 0.8438\n",
      "  [Step 019] Loss: 0.5787 | Acc: 0.7812\n",
      "  [Step 020] Loss: 0.7178 | Acc: 0.6562\n",
      "  [Step 021] Loss: 0.6671 | Acc: 0.7812\n",
      "  [Step 022] Loss: 0.6481 | Acc: 0.7188\n",
      "  [Step 023] Loss: 0.9265 | Acc: 0.7188\n",
      "  [Step 024] Loss: 1.1517 | Acc: 0.5938\n",
      "  [Step 025] Loss: 0.7361 | Acc: 0.6562\n",
      "  [Step 026] Loss: 0.5129 | Acc: 0.8438\n",
      "  [Step 027] Loss: 0.6534 | Acc: 0.7500\n",
      "  [Step 028] Loss: 0.9678 | Acc: 0.6875\n",
      "  [Step 029] Loss: 0.5960 | Acc: 0.7188\n",
      "  [Step 030] Loss: 0.7322 | Acc: 0.6875\n",
      "  [Step 031] Loss: 0.6009 | Acc: 0.7500\n",
      "  [Step 032] Loss: 0.5054 | Acc: 0.8750\n",
      "  [Step 033] Loss: 0.5964 | Acc: 0.7500\n",
      "  [Step 034] Loss: 0.9107 | Acc: 0.5938\n",
      "  [Step 035] Loss: 0.7924 | Acc: 0.7500\n",
      "  [Step 036] Loss: 0.9602 | Acc: 0.6562\n",
      "  [Step 037] Loss: 0.5915 | Acc: 0.8125\n",
      "  [Step 038] Loss: 0.6228 | Acc: 0.7812\n",
      "  [Step 039] Loss: 0.5857 | Acc: 0.7500\n",
      "  [Step 040] Loss: 0.8158 | Acc: 0.6875\n",
      "  [Step 041] Loss: 0.8397 | Acc: 0.7500\n",
      "  [Step 042] Loss: 1.0504 | Acc: 0.6250\n",
      "  [Step 043] Loss: 0.8810 | Acc: 0.6875\n",
      "  [Step 044] Loss: 0.5585 | Acc: 0.7812\n",
      "  [Step 045] Loss: 0.7290 | Acc: 0.8438\n",
      "  [Step 046] Loss: 0.4655 | Acc: 0.8125\n",
      "  [Step 047] Loss: 0.9275 | Acc: 0.6250\n",
      "  [Step 048] Loss: 0.8013 | Acc: 0.6875\n",
      "  [Step 049] Loss: 0.5749 | Acc: 0.8125\n",
      "  [Step 050] Loss: 0.9806 | Acc: 0.5625\n",
      "  [Step 051] Loss: 0.8596 | Acc: 0.6875\n",
      "  [Step 052] Loss: 0.6780 | Acc: 0.8750\n",
      "  [Step 053] Loss: 0.7621 | Acc: 0.6875\n",
      "  [Step 054] Loss: 0.7324 | Acc: 0.7500\n",
      "  [Step 055] Loss: 0.8345 | Acc: 0.6875\n",
      "  [Step 056] Loss: 0.8097 | Acc: 0.7500\n",
      "  [Step 057] Loss: 0.5848 | Acc: 0.8125\n",
      "  [Step 058] Loss: 0.4900 | Acc: 0.8438\n",
      "  [Step 059] Loss: 0.5684 | Acc: 0.8438\n",
      "  [Step 060] Loss: 1.0287 | Acc: 0.6250\n",
      "  [Step 061] Loss: 0.4645 | Acc: 0.8438\n",
      "  [Step 062] Loss: 0.5363 | Acc: 0.8750\n",
      "  [Step 063] Loss: 0.6290 | Acc: 0.7812\n",
      "  [Step 064] Loss: 0.9280 | Acc: 0.6250\n",
      "  [Step 065] Loss: 1.0586 | Acc: 0.6250\n",
      "  [Step 066] Loss: 0.6295 | Acc: 0.7500\n",
      "  [Step 067] Loss: 0.6912 | Acc: 0.6875\n",
      "  [Step 068] Loss: 0.7808 | Acc: 0.6562\n",
      "  [Step 069] Loss: 0.8584 | Acc: 0.7500\n",
      "  [Step 070] Loss: 0.7107 | Acc: 0.7188\n",
      "  [Step 071] Loss: 0.7198 | Acc: 0.7812\n",
      "  [Step 072] Loss: 0.5378 | Acc: 0.8125\n",
      "  [Step 073] Loss: 0.6603 | Acc: 0.7188\n",
      "  [Step 074] Loss: 0.7018 | Acc: 0.7812\n",
      "  [Step 075] Loss: 0.6662 | Acc: 0.7500\n",
      "  [Step 076] Loss: 0.7049 | Acc: 0.7188\n",
      "  [Step 077] Loss: 0.6248 | Acc: 0.6875\n",
      "  [Step 078] Loss: 0.6939 | Acc: 0.7812\n",
      "  [Step 079] Loss: 0.6626 | Acc: 0.7500\n",
      "  [Step 080] Loss: 0.7556 | Acc: 0.7500\n",
      "  [Step 081] Loss: 0.6794 | Acc: 0.7500\n",
      "  [Step 082] Loss: 0.9097 | Acc: 0.6875\n",
      "  [Step 083] Loss: 0.6615 | Acc: 0.7188\n",
      "  [Step 084] Loss: 0.7753 | Acc: 0.7500\n",
      "  [Step 085] Loss: 0.4800 | Acc: 0.9062\n",
      "  [Step 086] Loss: 0.6823 | Acc: 0.7812\n",
      "  [Step 087] Loss: 0.4326 | Acc: 0.8125\n",
      "  [Step 088] Loss: 0.4014 | Acc: 0.8438\n",
      "  [Step 089] Loss: 0.7571 | Acc: 0.6250\n",
      "  [Step 090] Loss: 0.6827 | Acc: 0.7188\n",
      "  [Step 091] Loss: 0.6211 | Acc: 0.7500\n",
      "  [Step 092] Loss: 0.5617 | Acc: 0.8438\n",
      "  [Step 093] Loss: 0.7000 | Acc: 0.7188\n",
      "  [Step 094] Loss: 0.4660 | Acc: 0.8438\n",
      "  [Step 095] Loss: 0.3534 | Acc: 0.8750\n",
      "  [Step 096] Loss: 0.7797 | Acc: 0.7500\n",
      "  [Step 097] Loss: 0.5329 | Acc: 0.8438\n",
      "  [Step 098] Loss: 0.6628 | Acc: 0.8125\n",
      "  [Step 099] Loss: 0.8030 | Acc: 0.8125\n",
      "  [Step 100] Loss: 0.8911 | Acc: 0.7812\n",
      "  [Step 101] Loss: 1.0120 | Acc: 0.6562\n",
      "  [Step 102] Loss: 0.8932 | Acc: 0.5625\n",
      "  [Step 103] Loss: 1.0743 | Acc: 0.5312\n",
      "  [Step 104] Loss: 0.9403 | Acc: 0.5938\n",
      "  [Step 105] Loss: 0.8864 | Acc: 0.6875\n",
      "  [Step 106] Loss: 0.9118 | Acc: 0.6875\n",
      "  [Step 107] Loss: 0.5413 | Acc: 0.7188\n",
      "  [Step 108] Loss: 0.7075 | Acc: 0.5938\n",
      "  [Step 109] Loss: 0.6340 | Acc: 0.7188\n",
      "  [Step 110] Loss: 0.7009 | Acc: 0.7188\n",
      "  [Step 111] Loss: 0.9369 | Acc: 0.6250\n",
      "  [Step 112] Loss: 0.9759 | Acc: 0.6250\n",
      "  [Step 113] Loss: 0.5442 | Acc: 0.7500\n",
      "  [Step 114] Loss: 0.9569 | Acc: 0.5312\n",
      "  [Step 115] Loss: 0.5495 | Acc: 0.7812\n",
      "  [Step 116] Loss: 1.2593 | Acc: 0.6562\n",
      "  [Step 117] Loss: 0.5037 | Acc: 0.8125\n",
      "  [Step 118] Loss: 0.9081 | Acc: 0.6562\n",
      "  [Step 119] Loss: 0.5732 | Acc: 0.7500\n",
      "  [Step 120] Loss: 1.1366 | Acc: 0.5312\n",
      "  [Step 121] Loss: 0.8564 | Acc: 0.6250\n",
      "  [Step 122] Loss: 0.8054 | Acc: 0.6875\n",
      "  [Step 123] Loss: 1.0301 | Acc: 0.5938\n",
      "  [Step 124] Loss: 0.7483 | Acc: 0.6250\n",
      "  [Step 125] Loss: 0.7111 | Acc: 0.6875\n",
      "  [Step 126] Loss: 0.6231 | Acc: 0.8438\n",
      "  [Step 127] Loss: 0.8336 | Acc: 0.7188\n",
      "  [Step 128] Loss: 0.7046 | Acc: 0.7188\n",
      "  [Step 129] Loss: 0.8590 | Acc: 0.6562\n",
      "  [Step 130] Loss: 0.6025 | Acc: 0.8125\n",
      "  [Step 131] Loss: 0.9964 | Acc: 0.5938\n",
      "  [Step 132] Loss: 0.7338 | Acc: 0.6875\n",
      "  [Step 133] Loss: 0.9527 | Acc: 0.6562\n",
      "  [Step 134] Loss: 1.1992 | Acc: 0.6562\n",
      "  [Step 135] Loss: 0.6059 | Acc: 0.7812\n",
      "  [Step 136] Loss: 0.7386 | Acc: 0.6562\n",
      "  [Step 137] Loss: 0.9924 | Acc: 0.6875\n",
      "  [Step 138] Loss: 0.7192 | Acc: 0.7812\n",
      "  [Step 139] Loss: 0.7622 | Acc: 0.7812\n",
      "  [Step 140] Loss: 1.0920 | Acc: 0.6250\n",
      "  [Step 141] Loss: 0.6772 | Acc: 0.7500\n",
      "  [Step 142] Loss: 0.7362 | Acc: 0.6875\n",
      "  [Step 143] Loss: 0.9837 | Acc: 0.5000\n",
      "  [Step 144] Loss: 0.4690 | Acc: 0.8438\n",
      "  [Step 145] Loss: 0.5527 | Acc: 0.7812\n",
      "  [Step 146] Loss: 0.7801 | Acc: 0.7188\n",
      "  [Step 147] Loss: 0.6311 | Acc: 0.8125\n",
      "  [Step 148] Loss: 1.1137 | Acc: 0.5625\n",
      "  [Step 149] Loss: 0.5754 | Acc: 0.7188\n",
      "  [Step 150] Loss: 0.5553 | Acc: 0.7188\n",
      "  [Step 151] Loss: 0.8908 | Acc: 0.7188\n",
      "  [Step 152] Loss: 0.7726 | Acc: 0.7188\n",
      "  [Step 153] Loss: 0.6142 | Acc: 0.8438\n",
      "  [Step 154] Loss: 1.2051 | Acc: 0.6250\n",
      "  [Step 155] Loss: 0.7824 | Acc: 0.7188\n",
      "  [Step 156] Loss: 0.6861 | Acc: 0.7188\n",
      "  [Step 157] Loss: 0.9866 | Acc: 0.6250\n",
      "  [Step 158] Loss: 0.5586 | Acc: 0.8438\n",
      "  [Step 159] Loss: 0.7940 | Acc: 0.6562\n",
      "  [Step 160] Loss: 0.9575 | Acc: 0.5938\n",
      "  [Step 161] Loss: 0.9015 | Acc: 0.6875\n",
      "  [Step 162] Loss: 0.6978 | Acc: 0.7812\n",
      "  [Step 163] Loss: 0.7090 | Acc: 0.7188\n",
      "  [Step 164] Loss: 0.6277 | Acc: 0.7188\n",
      "  [Step 165] Loss: 0.7057 | Acc: 0.6562\n",
      "  [Step 166] Loss: 0.6106 | Acc: 0.7812\n",
      "  [Step 167] Loss: 1.0386 | Acc: 0.6250\n",
      "  [Step 168] Loss: 0.8732 | Acc: 0.6562\n",
      "  [Step 169] Loss: 0.6548 | Acc: 0.8125\n",
      "  [Step 170] Loss: 0.5991 | Acc: 0.8125\n",
      "  [Step 171] Loss: 0.8631 | Acc: 0.6562\n",
      "  [Step 172] Loss: 0.8344 | Acc: 0.6250\n",
      "  [Step 173] Loss: 0.8350 | Acc: 0.6875\n",
      "  [Step 174] Loss: 0.4801 | Acc: 0.9375\n",
      "  [Step 175] Loss: 0.6610 | Acc: 0.7188\n",
      "  [Step 176] Loss: 0.7607 | Acc: 0.7188\n",
      "  [Step 177] Loss: 0.5975 | Acc: 0.7500\n",
      "  [Step 178] Loss: 0.6564 | Acc: 0.7188\n",
      "  [Step 179] Loss: 0.6987 | Acc: 0.6875\n",
      "  [Step 180] Loss: 0.6864 | Acc: 0.7188\n",
      "  [Step 181] Loss: 0.4352 | Acc: 0.8438\n",
      "  [Step 182] Loss: 0.7479 | Acc: 0.6875\n",
      "  [Step 183] Loss: 0.9312 | Acc: 0.5938\n",
      "  [Step 184] Loss: 1.1089 | Acc: 0.5938\n",
      "  [Step 185] Loss: 0.8769 | Acc: 0.5938\n",
      "  [Step 186] Loss: 1.0399 | Acc: 0.5938\n",
      "  [Step 187] Loss: 0.6489 | Acc: 0.7188\n",
      "  [Step 188] Loss: 0.7518 | Acc: 0.6562\n",
      "  [Step 189] Loss: 0.6766 | Acc: 0.6875\n",
      "  [Step 190] Loss: 0.7515 | Acc: 0.6875\n",
      "  [Step 191] Loss: 0.7067 | Acc: 0.6875\n",
      "  [Step 192] Loss: 0.8994 | Acc: 0.7812\n",
      "  [Step 193] Loss: 0.5196 | Acc: 0.8438\n",
      "  [Step 194] Loss: 0.6462 | Acc: 0.7812\n",
      "  [Step 195] Loss: 0.9926 | Acc: 0.7188\n",
      "  [Step 196] Loss: 1.0569 | Acc: 0.7188\n",
      "  [Step 197] Loss: 0.7751 | Acc: 0.6562\n",
      "  [Step 198] Loss: 0.5696 | Acc: 0.7812\n",
      "  [Step 199] Loss: 0.7590 | Acc: 0.7812\n",
      "  [Step 200] Loss: 1.0430 | Acc: 0.6562\n",
      "  [Step 201] Loss: 0.7599 | Acc: 0.6875\n",
      "  [Step 202] Loss: 0.8687 | Acc: 0.6875\n",
      "  [Step 203] Loss: 0.6203 | Acc: 0.7812\n",
      "  [Step 204] Loss: 0.5896 | Acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 0.4778 | Acc: 0.9062\n",
      "  [Step 206] Loss: 0.5676 | Acc: 0.8125\n",
      "  [Step 207] Loss: 0.6565 | Acc: 0.7812\n",
      "  [Step 208] Loss: 0.5339 | Acc: 0.8750\n",
      "  [Step 209] Loss: 0.8188 | Acc: 0.8125\n",
      "  [Step 210] Loss: 0.6961 | Acc: 0.7500\n",
      "  [Step 211] Loss: 0.6671 | Acc: 0.7188\n",
      "  [Step 212] Loss: 0.3646 | Acc: 0.8750\n",
      "  [Step 213] Loss: 0.8949 | Acc: 0.6562\n",
      "  [Step 214] Loss: 0.7112 | Acc: 0.7812\n",
      "  [Step 215] Loss: 0.4975 | Acc: 0.8750\n",
      "  [Step 216] Loss: 0.7553 | Acc: 0.6875\n",
      "  [Step 217] Loss: 0.6970 | Acc: 0.7812\n",
      "  [Step 218] Loss: 0.5649 | Acc: 0.8125\n",
      "  [Step 219] Loss: 0.7789 | Acc: 0.7188\n",
      "  [Step 220] Loss: 0.6876 | Acc: 0.8125\n",
      "  [Step 221] Loss: 0.6533 | Acc: 0.7500\n",
      "  [Step 222] Loss: 0.5183 | Acc: 0.8125\n",
      "  [Step 223] Loss: 0.7364 | Acc: 0.7188\n",
      "  [Step 224] Loss: 0.8164 | Acc: 0.6250\n",
      "  [Step 225] Loss: 0.6850 | Acc: 0.6875\n",
      "  [Step 226] Loss: 0.6764 | Acc: 0.7500\n",
      "  [Step 227] Loss: 0.7646 | Acc: 0.6875\n",
      "  [Step 228] Loss: 0.8401 | Acc: 0.6875\n",
      "  [Step 229] Loss: 0.7643 | Acc: 0.7188\n",
      "  [Step 230] Loss: 0.4132 | Acc: 0.8438\n",
      "  [Step 231] Loss: 0.7715 | Acc: 0.7188\n",
      "  [Step 232] Loss: 0.7194 | Acc: 0.7500\n",
      "  [Step 233] Loss: 0.7838 | Acc: 0.6875\n",
      "  [Step 234] Loss: 0.7706 | Acc: 0.7500\n",
      "  [Step 235] Loss: 0.5665 | Acc: 0.8125\n",
      "  [Step 236] Loss: 0.6317 | Acc: 0.8438\n",
      "  [Step 237] Loss: 0.7724 | Acc: 0.6250\n",
      "  [Step 238] Loss: 0.6833 | Acc: 0.6562\n",
      "  [Step 239] Loss: 0.5710 | Acc: 0.7188\n",
      "  [Step 240] Loss: 0.8207 | Acc: 0.6875\n",
      "  [Step 241] Loss: 0.6471 | Acc: 0.7500\n",
      "  [Step 242] Loss: 0.6870 | Acc: 0.6875\n",
      "  [Step 243] Loss: 0.4125 | Acc: 0.8438\n",
      "  [Step 244] Loss: 0.6656 | Acc: 0.7188\n",
      "  [Step 245] Loss: 0.7022 | Acc: 0.6875\n",
      "  [Step 246] Loss: 0.4570 | Acc: 0.8750\n",
      "  [Step 247] Loss: 0.4276 | Acc: 0.7812\n",
      "  [Step 248] Loss: 0.6994 | Acc: 0.6562\n",
      "  [Step 249] Loss: 0.7804 | Acc: 0.7812\n",
      "  [Step 250] Loss: 0.7127 | Acc: 0.6250\n",
      "  [Step 251] Loss: 0.8285 | Acc: 0.6875\n",
      "  [Step 252] Loss: 0.4182 | Acc: 0.8750\n",
      "  [Step 253] Loss: 0.4094 | Acc: 0.8438\n",
      "  [Step 254] Loss: 0.5202 | Acc: 0.7500\n",
      "  [Step 255] Loss: 0.9271 | Acc: 0.6562\n",
      "  [Step 256] Loss: 0.4887 | Acc: 0.8438\n",
      "  [Step 257] Loss: 0.5251 | Acc: 0.8125\n",
      "  [Step 258] Loss: 0.6204 | Acc: 0.7812\n",
      "  [Step 259] Loss: 1.1522 | Acc: 0.5312\n",
      "  [Step 260] Loss: 0.9274 | Acc: 0.5938\n",
      "  [Step 261] Loss: 0.6064 | Acc: 0.7812\n",
      "  [Step 262] Loss: 0.5808 | Acc: 0.7500\n",
      "  [Step 263] Loss: 1.1891 | Acc: 0.6250\n",
      "  [Step 264] Loss: 0.8344 | Acc: 0.5625\n",
      "  [Step 265] Loss: 0.6695 | Acc: 0.6250\n",
      "  [Step 266] Loss: 0.5658 | Acc: 0.8125\n",
      "  [Step 267] Loss: 1.0859 | Acc: 0.6562\n",
      "  [Step 268] Loss: 0.7761 | Acc: 0.7188\n",
      "  [Step 269] Loss: 0.7490 | Acc: 0.7188\n",
      "  [Step 270] Loss: 0.7388 | Acc: 0.8125\n",
      "  [Step 271] Loss: 1.0611 | Acc: 0.6250\n",
      "  [Step 272] Loss: 0.8152 | Acc: 0.6875\n",
      "  [Step 273] Loss: 0.7505 | Acc: 0.6562\n",
      "  [Step 274] Loss: 0.8044 | Acc: 0.6875\n",
      "  [Step 275] Loss: 0.7811 | Acc: 0.7188\n",
      "  [Step 276] Loss: 0.9703 | Acc: 0.6250\n",
      "  [Step 277] Loss: 0.4273 | Acc: 0.8125\n",
      "  [Step 278] Loss: 0.8171 | Acc: 0.7188\n",
      "  [Step 279] Loss: 0.4586 | Acc: 0.8125\n",
      "  [Step 280] Loss: 0.4950 | Acc: 0.8750\n",
      "  [Step 281] Loss: 0.8948 | Acc: 0.6875\n",
      "  [Step 282] Loss: 0.7557 | Acc: 0.6875\n",
      "  [Step 283] Loss: 0.8781 | Acc: 0.7188\n",
      "  [Step 284] Loss: 0.8427 | Acc: 0.6875\n",
      "  [Step 285] Loss: 0.6988 | Acc: 0.7500\n",
      "  [Step 286] Loss: 0.5782 | Acc: 0.8125\n",
      "  [Step 287] Loss: 0.6221 | Acc: 0.8125\n",
      "  [Step 288] Loss: 0.4561 | Acc: 0.7812\n",
      "  [Step 289] Loss: 0.6387 | Acc: 0.7812\n",
      "  [Step 290] Loss: 0.7597 | Acc: 0.6875\n",
      "  [Step 291] Loss: 0.7961 | Acc: 0.6562\n",
      "  [Step 292] Loss: 0.4955 | Acc: 0.8125\n",
      "  [Step 293] Loss: 0.7762 | Acc: 0.6875\n",
      "  [Step 294] Loss: 0.6365 | Acc: 0.7188\n",
      "  [Step 295] Loss: 0.6490 | Acc: 0.7500\n",
      "  [Step 296] Loss: 0.6879 | Acc: 0.8125\n",
      "  [Step 297] Loss: 0.6020 | Acc: 0.7812\n",
      "  [Step 298] Loss: 0.5523 | Acc: 0.7812\n",
      "  [Step 299] Loss: 0.5591 | Acc: 0.7500\n",
      "  [Step 300] Loss: 0.5721 | Acc: 0.7812\n",
      "  [Step 301] Loss: 0.7715 | Acc: 0.6562\n",
      "  [Step 302] Loss: 0.7461 | Acc: 0.6875\n",
      "  [Step 303] Loss: 0.5633 | Acc: 0.7500\n",
      "  [Step 304] Loss: 0.5758 | Acc: 0.8125\n",
      "  [Step 305] Loss: 1.0447 | Acc: 0.6562\n",
      "  [Step 306] Loss: 0.7611 | Acc: 0.7812\n",
      "  [Step 307] Loss: 0.4117 | Acc: 0.8438\n",
      "  [Step 308] Loss: 0.5366 | Acc: 0.8125\n",
      "  [Step 309] Loss: 0.6621 | Acc: 0.7188\n",
      "  [Step 310] Loss: 0.7093 | Acc: 0.7188\n",
      "  [Step 311] Loss: 0.7192 | Acc: 0.7500\n",
      "  [Step 312] Loss: 0.7747 | Acc: 0.6562\n",
      "  [Step 313] Loss: 0.6845 | Acc: 0.7188\n",
      "  [Step 314] Loss: 0.4860 | Acc: 0.8125\n",
      "  [Step 315] Loss: 0.5800 | Acc: 0.7188\n",
      "  [Step 316] Loss: 0.3632 | Acc: 0.8438\n",
      "  [Step 317] Loss: 0.8504 | Acc: 0.6562\n",
      "  [Step 318] Loss: 0.7974 | Acc: 0.6875\n",
      "  [Step 319] Loss: 0.8886 | Acc: 0.6562\n",
      "  [Step 320] Loss: 0.6495 | Acc: 0.8125\n",
      "  [Step 321] Loss: 0.5997 | Acc: 0.7812\n",
      "  [Step 322] Loss: 0.5654 | Acc: 0.8125\n",
      "  [Step 323] Loss: 0.9835 | Acc: 0.6250\n",
      "  [Step 324] Loss: 0.8522 | Acc: 0.7188\n",
      "  [Step 325] Loss: 0.6698 | Acc: 0.7500\n",
      "  [Step 326] Loss: 0.5652 | Acc: 0.8125\n",
      "  [Step 327] Loss: 0.5876 | Acc: 0.7812\n",
      "  [Step 328] Loss: 0.8645 | Acc: 0.7188\n",
      "  [Step 329] Loss: 0.4742 | Acc: 0.8750\n",
      "  [Step 330] Loss: 0.8447 | Acc: 0.6250\n",
      "[Epoch 6] Train Loss: 0.2429 | Acc: 0.7279 || Val Loss: 0.6814 | Acc: 0.7576\n",
      "Precision: 0.7361 | Recall: 0.6567 | F1: 0.6278 | Time: 1072.07s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.10129376 0.05029255 0.11937878 0.05589397 0.10043132 0.09605249\n",
      "  0.11616706 0.16926767 0.09194927 0.09927314]\n",
      " [0.12141081 0.05691533 0.07557584 0.05457651 0.11340272 0.11899164\n",
      "  0.11193082 0.11736098 0.11536398 0.11447141]\n",
      " [0.10537814 0.05858881 0.13357498 0.06158651 0.10419311 0.10416244\n",
      "  0.1189341  0.11263555 0.09914175 0.1018046 ]\n",
      " [0.1169438  0.06261923 0.07978117 0.06240902 0.1183675  0.11537787\n",
      "  0.10991021 0.1068949  0.11410035 0.11359599]\n",
      " [0.10788566 0.08407272 0.09693949 0.0867601  0.10613248 0.10462115\n",
      "  0.10582819 0.10502062 0.10444521 0.09829438]\n",
      " [0.10757171 0.065805   0.12560901 0.06845161 0.10635246 0.10413524\n",
      "  0.10618341 0.10854721 0.09932609 0.10801826]\n",
      " [0.11336388 0.06770805 0.08279847 0.06709587 0.11151615 0.11217712\n",
      "  0.10980982 0.11079696 0.11318231 0.11155143]\n",
      " [0.10527465 0.08608915 0.09601089 0.08802056 0.10508993 0.10318328\n",
      "  0.10660774 0.10491466 0.10438763 0.10042152]\n",
      " [0.10831746 0.08253522 0.09162062 0.08441136 0.10641482 0.10568275\n",
      "  0.10660771 0.10507239 0.10583159 0.10350611]\n",
      " [0.11142202 0.059521   0.1163851  0.05928447 0.11080946 0.11012868\n",
      "  0.10024144 0.11782574 0.10459378 0.10978834]\n",
      " [0.11443593 0.06574585 0.08275021 0.06571213 0.11193883 0.11470435\n",
      "  0.10560603 0.11574589 0.11062313 0.1127376 ]\n",
      " [0.10775332 0.08392615 0.0936291  0.08493291 0.10473417 0.10291059\n",
      "  0.10753585 0.10670955 0.10460425 0.10326402]\n",
      " [0.10906899 0.07914127 0.08729797 0.07985871 0.10530823 0.10611784\n",
      "  0.10970283 0.10768729 0.10800066 0.10781613]\n",
      " [0.1105151  0.07481211 0.08425477 0.07601492 0.10661343 0.10855807\n",
      "  0.10991497 0.11696248 0.10756881 0.10478534]\n",
      " [0.10980015 0.06363953 0.11902222 0.06228067 0.10718214 0.1084138\n",
      "  0.10648354 0.10970679 0.10778769 0.10568357]\n",
      " [0.11293153 0.06581082 0.08312175 0.06515741 0.11127998 0.11081586\n",
      "  0.1132752  0.12276156 0.10590196 0.1089439 ]\n",
      " [0.10665459 0.0856022  0.09513588 0.08637258 0.10566755 0.10484678\n",
      "  0.10494912 0.10552554 0.10254361 0.10270219]\n",
      " [0.10713114 0.08029902 0.08947307 0.08060929 0.10612763 0.10875007\n",
      "  0.10610121 0.10644443 0.10810184 0.10696242]\n",
      " [0.11032485 0.07704335 0.08658129 0.07781536 0.1081413  0.11018865\n",
      "  0.10566247 0.11378377 0.10371311 0.10674587]\n",
      " [0.11044714 0.07220398 0.08044241 0.07276365 0.10683209 0.11085302\n",
      "  0.10556791 0.11267556 0.11508426 0.11313   ]\n",
      " [0.10827436 0.06290321 0.10294211 0.06142696 0.10880184 0.10802203\n",
      "  0.11278733 0.12047493 0.1067773  0.10758988]\n",
      " [0.11510689 0.07009251 0.07931808 0.06906182 0.11187256 0.11451583\n",
      "  0.11339142 0.10989311 0.10953226 0.10721551]\n",
      " [0.10613348 0.08565629 0.09576689 0.08602372 0.10655319 0.10310914\n",
      "  0.10329694 0.10518299 0.10253504 0.10574237]\n",
      " [0.10855287 0.08123542 0.09016266 0.08134294 0.10664929 0.10677369\n",
      "  0.10695091 0.10751042 0.10512893 0.10569288]\n",
      " [0.11003566 0.07686777 0.08773836 0.07710928 0.1104918  0.10784502\n",
      "  0.10985706 0.1054697  0.10989632 0.10468903]\n",
      " [0.11194387 0.07356887 0.08293476 0.07374616 0.11015477 0.11094655\n",
      "  0.10895324 0.10783748 0.1095973  0.11031708]\n",
      " [0.11310237 0.06905258 0.07916205 0.06959874 0.11275056 0.11115744\n",
      "  0.11601947 0.11217529 0.10722522 0.10975628]\n",
      " [0.10715241 0.05974811 0.12795953 0.0580088  0.10803384 0.10783438\n",
      "  0.10453331 0.11257778 0.10548224 0.10866959]\n",
      " [0.11309776 0.069071   0.08798929 0.06748211 0.11213326 0.11210419\n",
      "  0.1093754  0.10925566 0.11087476 0.10861655]\n",
      " [0.10583339 0.0858557  0.09943509 0.08576559 0.10261633 0.10273814\n",
      "  0.10650057 0.10255525 0.10281816 0.1058818 ]\n",
      " [0.10696814 0.08196908 0.09548105 0.08161941 0.10737289 0.10694364\n",
      "  0.10592588 0.10701199 0.10644159 0.10026626]\n",
      " [0.10781959 0.07813766 0.09288426 0.07807175 0.10671927 0.10524005\n",
      "  0.10753343 0.10652554 0.10909355 0.10797488]\n",
      " [0.11039615 0.07574417 0.09031715 0.07545371 0.10865421 0.10819387\n",
      "  0.11110186 0.10966137 0.10682495 0.10365261]\n",
      " [0.11110635 0.0711811  0.08681432 0.07110513 0.10967582 0.11098946\n",
      "  0.10733965 0.11362609 0.11143042 0.10673167]\n",
      " [0.11283063 0.0670076  0.0828384  0.06679998 0.11128034 0.11152477\n",
      "  0.10874517 0.12022996 0.11226019 0.106483  ]]\n",
      "\n",
      "[Epoch 7/27] Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 000] Loss: 1.5848 | Acc: 0.3125\n",
      "  [Step 001] Loss: 0.7830 | Acc: 0.6875\n",
      "  [Step 002] Loss: 0.6972 | Acc: 0.6875\n",
      "  [Step 003] Loss: 0.5621 | Acc: 0.8125\n",
      "  [Step 004] Loss: 0.9042 | Acc: 0.5938\n",
      "  [Step 005] Loss: 0.4954 | Acc: 0.8125\n",
      "  [Step 006] Loss: 0.7186 | Acc: 0.7188\n",
      "  [Step 007] Loss: 0.4404 | Acc: 0.8438\n",
      "  [Step 008] Loss: 0.8759 | Acc: 0.6562\n",
      "  [Step 009] Loss: 0.7109 | Acc: 0.6875\n",
      "  [Step 010] Loss: 0.9137 | Acc: 0.6562\n",
      "  [Step 011] Loss: 0.5257 | Acc: 0.8125\n",
      "  [Step 012] Loss: 0.7300 | Acc: 0.7188\n",
      "  [Step 013] Loss: 0.9626 | Acc: 0.7812\n",
      "  [Step 014] Loss: 1.1104 | Acc: 0.7188\n",
      "  [Step 015] Loss: 0.6509 | Acc: 0.6875\n",
      "  [Step 016] Loss: 0.8641 | Acc: 0.5938\n",
      "  [Step 017] Loss: 0.6902 | Acc: 0.6875\n",
      "  [Step 018] Loss: 0.9769 | Acc: 0.6875\n",
      "  [Step 019] Loss: 0.5850 | Acc: 0.8125\n",
      "  [Step 020] Loss: 0.9863 | Acc: 0.5938\n",
      "  [Step 021] Loss: 1.3117 | Acc: 0.6250\n",
      "  [Step 022] Loss: 0.6524 | Acc: 0.7188\n",
      "  [Step 023] Loss: 0.8854 | Acc: 0.6875\n",
      "  [Step 024] Loss: 0.6177 | Acc: 0.7500\n",
      "  [Step 025] Loss: 0.7994 | Acc: 0.7188\n",
      "  [Step 026] Loss: 0.6322 | Acc: 0.8125\n",
      "  [Step 027] Loss: 0.7995 | Acc: 0.7188\n",
      "  [Step 028] Loss: 0.6544 | Acc: 0.7500\n",
      "  [Step 029] Loss: 0.6143 | Acc: 0.9062\n",
      "  [Step 030] Loss: 0.7505 | Acc: 0.7500\n",
      "  [Step 031] Loss: 0.7362 | Acc: 0.7500\n",
      "  [Step 032] Loss: 0.6103 | Acc: 0.8125\n",
      "  [Step 033] Loss: 0.5361 | Acc: 0.8438\n",
      "  [Step 034] Loss: 0.9674 | Acc: 0.6250\n",
      "  [Step 035] Loss: 0.6907 | Acc: 0.6875\n",
      "  [Step 036] Loss: 0.6624 | Acc: 0.6562\n",
      "  [Step 037] Loss: 0.5624 | Acc: 0.8438\n",
      "  [Step 038] Loss: 0.5243 | Acc: 0.8125\n",
      "  [Step 039] Loss: 0.6437 | Acc: 0.7812\n",
      "  [Step 040] Loss: 0.6143 | Acc: 0.7812\n",
      "  [Step 041] Loss: 0.7615 | Acc: 0.6562\n",
      "  [Step 042] Loss: 0.3371 | Acc: 0.9062\n",
      "  [Step 043] Loss: 0.7508 | Acc: 0.6875\n",
      "  [Step 044] Loss: 0.4941 | Acc: 0.8125\n",
      "  [Step 045] Loss: 0.9062 | Acc: 0.7812\n",
      "  [Step 046] Loss: 0.4881 | Acc: 0.7812\n",
      "  [Step 047] Loss: 0.7884 | Acc: 0.6875\n",
      "  [Step 048] Loss: 0.5864 | Acc: 0.7500\n",
      "  [Step 049] Loss: 1.0282 | Acc: 0.8125\n",
      "  [Step 050] Loss: 0.7658 | Acc: 0.6875\n",
      "  [Step 051] Loss: 0.7357 | Acc: 0.6875\n",
      "  [Step 052] Loss: 0.7030 | Acc: 0.7188\n",
      "  [Step 053] Loss: 0.7583 | Acc: 0.6562\n",
      "  [Step 054] Loss: 0.7680 | Acc: 0.7188\n",
      "  [Step 055] Loss: 0.6609 | Acc: 0.7188\n",
      "  [Step 056] Loss: 0.9402 | Acc: 0.5625\n",
      "  [Step 057] Loss: 0.7829 | Acc: 0.6562\n",
      "  [Step 058] Loss: 0.5971 | Acc: 0.7500\n",
      "  [Step 059] Loss: 0.6389 | Acc: 0.7812\n",
      "  [Step 060] Loss: 0.7632 | Acc: 0.6562\n",
      "  [Step 061] Loss: 0.7667 | Acc: 0.6875\n",
      "  [Step 062] Loss: 0.5838 | Acc: 0.8750\n",
      "  [Step 063] Loss: 0.6865 | Acc: 0.6250\n",
      "  [Step 064] Loss: 0.6831 | Acc: 0.6250\n",
      "  [Step 065] Loss: 0.8823 | Acc: 0.6875\n",
      "  [Step 066] Loss: 0.6798 | Acc: 0.8125\n",
      "  [Step 067] Loss: 0.7993 | Acc: 0.5938\n",
      "  [Step 068] Loss: 0.5360 | Acc: 0.8438\n",
      "  [Step 069] Loss: 0.6656 | Acc: 0.7188\n",
      "  [Step 070] Loss: 0.6764 | Acc: 0.6875\n",
      "  [Step 071] Loss: 0.6589 | Acc: 0.7812\n",
      "  [Step 072] Loss: 0.4769 | Acc: 0.7812\n",
      "  [Step 073] Loss: 0.5218 | Acc: 0.8125\n",
      "  [Step 074] Loss: 0.9310 | Acc: 0.7188\n",
      "  [Step 075] Loss: 0.8588 | Acc: 0.8125\n",
      "  [Step 076] Loss: 0.4834 | Acc: 0.8750\n",
      "  [Step 077] Loss: 0.7205 | Acc: 0.7188\n",
      "  [Step 078] Loss: 0.5796 | Acc: 0.7812\n",
      "  [Step 079] Loss: 0.6422 | Acc: 0.7500\n",
      "  [Step 080] Loss: 0.8083 | Acc: 0.6875\n",
      "  [Step 081] Loss: 0.5651 | Acc: 0.8125\n",
      "  [Step 082] Loss: 0.4746 | Acc: 0.8125\n",
      "  [Step 083] Loss: 0.4900 | Acc: 0.8125\n",
      "  [Step 084] Loss: 0.5220 | Acc: 0.7812\n",
      "  [Step 085] Loss: 0.7807 | Acc: 0.7812\n",
      "  [Step 086] Loss: 0.7117 | Acc: 0.7500\n",
      "  [Step 087] Loss: 0.7387 | Acc: 0.7812\n",
      "  [Step 088] Loss: 0.5470 | Acc: 0.7188\n",
      "  [Step 089] Loss: 0.8161 | Acc: 0.7188\n",
      "  [Step 090] Loss: 0.6590 | Acc: 0.7500\n",
      "  [Step 091] Loss: 0.3645 | Acc: 0.9062\n",
      "  [Step 092] Loss: 0.6851 | Acc: 0.7188\n",
      "  [Step 093] Loss: 0.5286 | Acc: 0.8125\n",
      "  [Step 094] Loss: 0.7704 | Acc: 0.7188\n",
      "  [Step 095] Loss: 0.9084 | Acc: 0.7188\n",
      "  [Step 096] Loss: 0.8240 | Acc: 0.7812\n",
      "  [Step 097] Loss: 0.8262 | Acc: 0.6562\n",
      "  [Step 098] Loss: 0.7092 | Acc: 0.7188\n",
      "  [Step 099] Loss: 0.6828 | Acc: 0.7188\n",
      "  [Step 100] Loss: 0.8762 | Acc: 0.7500\n",
      "  [Step 101] Loss: 0.8892 | Acc: 0.6875\n",
      "  [Step 102] Loss: 0.6615 | Acc: 0.7188\n",
      "  [Step 103] Loss: 0.9277 | Acc: 0.5938\n",
      "  [Step 104] Loss: 0.5984 | Acc: 0.7812\n",
      "  [Step 105] Loss: 0.4419 | Acc: 0.7812\n",
      "  [Step 106] Loss: 0.7214 | Acc: 0.7500\n",
      "  [Step 107] Loss: 0.7115 | Acc: 0.7812\n",
      "  [Step 108] Loss: 0.6176 | Acc: 0.7500\n",
      "  [Step 109] Loss: 0.6243 | Acc: 0.7812\n",
      "  [Step 110] Loss: 0.8224 | Acc: 0.6250\n",
      "  [Step 111] Loss: 0.5973 | Acc: 0.7500\n",
      "  [Step 112] Loss: 0.3457 | Acc: 0.9062\n",
      "  [Step 113] Loss: 0.6390 | Acc: 0.8438\n",
      "  [Step 114] Loss: 0.6959 | Acc: 0.8125\n",
      "  [Step 115] Loss: 0.2963 | Acc: 0.9062\n",
      "  [Step 116] Loss: 0.5468 | Acc: 0.7812\n",
      "  [Step 117] Loss: 0.6406 | Acc: 0.7188\n",
      "  [Step 118] Loss: 0.5916 | Acc: 0.7500\n",
      "  [Step 119] Loss: 0.5389 | Acc: 0.7500\n",
      "  [Step 120] Loss: 0.8261 | Acc: 0.6875\n",
      "  [Step 121] Loss: 0.6437 | Acc: 0.7188\n",
      "  [Step 122] Loss: 0.7326 | Acc: 0.6562\n",
      "  [Step 123] Loss: 0.5476 | Acc: 0.7812\n",
      "  [Step 124] Loss: 0.5360 | Acc: 0.8438\n",
      "  [Step 125] Loss: 0.9732 | Acc: 0.6250\n",
      "  [Step 126] Loss: 0.7454 | Acc: 0.7188\n",
      "  [Step 127] Loss: 0.9139 | Acc: 0.6875\n",
      "  [Step 128] Loss: 0.7261 | Acc: 0.7812\n",
      "  [Step 129] Loss: 0.9977 | Acc: 0.6250\n",
      "  [Step 130] Loss: 0.7096 | Acc: 0.7188\n",
      "  [Step 131] Loss: 0.6811 | Acc: 0.7500\n",
      "  [Step 132] Loss: 0.7296 | Acc: 0.6875\n",
      "  [Step 133] Loss: 0.6412 | Acc: 0.6875\n",
      "  [Step 134] Loss: 0.7099 | Acc: 0.6875\n",
      "  [Step 135] Loss: 0.8196 | Acc: 0.6250\n",
      "  [Step 136] Loss: 0.9980 | Acc: 0.6250\n",
      "  [Step 137] Loss: 1.0623 | Acc: 0.6562\n",
      "  [Step 138] Loss: 0.6683 | Acc: 0.7188\n",
      "  [Step 139] Loss: 0.9136 | Acc: 0.7188\n",
      "  [Step 140] Loss: 0.8773 | Acc: 0.7500\n",
      "  [Step 141] Loss: 0.7859 | Acc: 0.7812\n",
      "  [Step 142] Loss: 0.6881 | Acc: 0.7500\n",
      "  [Step 143] Loss: 0.7640 | Acc: 0.6562\n",
      "  [Step 144] Loss: 0.6731 | Acc: 0.6562\n",
      "  [Step 145] Loss: 1.3022 | Acc: 0.4688\n",
      "  [Step 146] Loss: 0.3331 | Acc: 0.9062\n",
      "  [Step 147] Loss: 0.6103 | Acc: 0.7812\n",
      "  [Step 148] Loss: 0.8322 | Acc: 0.7188\n",
      "  [Step 149] Loss: 0.9699 | Acc: 0.6562\n",
      "  [Step 150] Loss: 0.8927 | Acc: 0.5625\n",
      "  [Step 151] Loss: 0.6845 | Acc: 0.7500\n",
      "  [Step 152] Loss: 0.6399 | Acc: 0.7188\n",
      "  [Step 153] Loss: 0.8160 | Acc: 0.5938\n",
      "  [Step 154] Loss: 0.8802 | Acc: 0.7500\n",
      "  [Step 155] Loss: 1.0748 | Acc: 0.6562\n",
      "  [Step 156] Loss: 0.6519 | Acc: 0.6875\n",
      "  [Step 157] Loss: 0.4777 | Acc: 0.8438\n",
      "  [Step 158] Loss: 0.8678 | Acc: 0.5938\n",
      "  [Step 159] Loss: 0.7577 | Acc: 0.6875\n",
      "  [Step 160] Loss: 0.9118 | Acc: 0.6250\n",
      "  [Step 161] Loss: 0.9049 | Acc: 0.6250\n",
      "  [Step 162] Loss: 0.7333 | Acc: 0.7188\n",
      "  [Step 163] Loss: 0.6751 | Acc: 0.7812\n",
      "  [Step 164] Loss: 0.6922 | Acc: 0.7812\n",
      "  [Step 165] Loss: 0.7752 | Acc: 0.6250\n",
      "  [Step 166] Loss: 0.8738 | Acc: 0.5625\n",
      "  [Step 167] Loss: 0.5819 | Acc: 0.7188\n",
      "  [Step 168] Loss: 0.7916 | Acc: 0.7188\n",
      "  [Step 169] Loss: 0.6269 | Acc: 0.7812\n",
      "  [Step 170] Loss: 0.6854 | Acc: 0.6875\n",
      "  [Step 171] Loss: 0.5969 | Acc: 0.6875\n",
      "  [Step 172] Loss: 0.5586 | Acc: 0.7812\n",
      "  [Step 173] Loss: 0.6942 | Acc: 0.7188\n",
      "  [Step 174] Loss: 0.6394 | Acc: 0.7500\n",
      "  [Step 175] Loss: 0.8378 | Acc: 0.7500\n",
      "  [Step 176] Loss: 0.7040 | Acc: 0.8125\n",
      "  [Step 177] Loss: 0.7830 | Acc: 0.7500\n",
      "  [Step 178] Loss: 0.8105 | Acc: 0.6875\n",
      "  [Step 179] Loss: 0.9937 | Acc: 0.7812\n",
      "  [Step 180] Loss: 0.8327 | Acc: 0.5938\n",
      "  [Step 181] Loss: 0.5988 | Acc: 0.8125\n",
      "  [Step 182] Loss: 0.6349 | Acc: 0.7812\n",
      "  [Step 183] Loss: 0.6384 | Acc: 0.7500\n",
      "  [Step 184] Loss: 0.6096 | Acc: 0.7812\n",
      "  [Step 185] Loss: 0.5130 | Acc: 0.8438\n",
      "  [Step 186] Loss: 0.8565 | Acc: 0.7500\n",
      "  [Step 187] Loss: 0.7225 | Acc: 0.7188\n",
      "  [Step 188] Loss: 0.9454 | Acc: 0.6562\n",
      "  [Step 189] Loss: 0.9013 | Acc: 0.6875\n",
      "  [Step 190] Loss: 0.8248 | Acc: 0.7188\n",
      "  [Step 191] Loss: 0.8114 | Acc: 0.7188\n",
      "  [Step 192] Loss: 0.4913 | Acc: 0.8438\n",
      "  [Step 193] Loss: 0.8148 | Acc: 0.7188\n",
      "  [Step 194] Loss: 0.6988 | Acc: 0.7812\n",
      "  [Step 195] Loss: 0.4820 | Acc: 0.8750\n",
      "  [Step 196] Loss: 0.7491 | Acc: 0.7500\n",
      "  [Step 197] Loss: 0.7790 | Acc: 0.7188\n",
      "  [Step 198] Loss: 0.5779 | Acc: 0.7188\n",
      "  [Step 199] Loss: 0.6579 | Acc: 0.7500\n",
      "  [Step 200] Loss: 0.5893 | Acc: 0.7812\n",
      "  [Step 201] Loss: 0.5788 | Acc: 0.8438\n",
      "  [Step 202] Loss: 0.5147 | Acc: 0.8125\n",
      "  [Step 203] Loss: 0.7004 | Acc: 0.7812\n",
      "  [Step 204] Loss: 0.5368 | Acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Step 205] Loss: 0.5354 | Acc: 0.8125\n",
      "  [Step 206] Loss: 0.9184 | Acc: 0.6562\n",
      "  [Step 207] Loss: 0.6436 | Acc: 0.7812\n",
      "  [Step 208] Loss: 0.7199 | Acc: 0.7500\n",
      "  [Step 209] Loss: 0.3105 | Acc: 0.9062\n",
      "  [Step 210] Loss: 0.5700 | Acc: 0.7812\n",
      "  [Step 211] Loss: 0.7570 | Acc: 0.6250\n",
      "  [Step 212] Loss: 0.5753 | Acc: 0.7812\n",
      "  [Step 213] Loss: 0.6155 | Acc: 0.7188\n",
      "  [Step 214] Loss: 0.5919 | Acc: 0.8125\n",
      "  [Step 215] Loss: 0.8935 | Acc: 0.7500\n",
      "  [Step 216] Loss: 0.4647 | Acc: 0.8125\n",
      "  [Step 217] Loss: 0.4957 | Acc: 0.8438\n",
      "  [Step 218] Loss: 0.3521 | Acc: 0.8438\n",
      "  [Step 219] Loss: 1.0441 | Acc: 0.6250\n",
      "  [Step 220] Loss: 0.4724 | Acc: 0.8438\n",
      "  [Step 221] Loss: 0.5890 | Acc: 0.8125\n",
      "  [Step 222] Loss: 0.5881 | Acc: 0.7812\n",
      "  [Step 223] Loss: 0.9946 | Acc: 0.6250\n",
      "  [Step 224] Loss: 0.5744 | Acc: 0.8438\n",
      "  [Step 225] Loss: 0.6345 | Acc: 0.7812\n",
      "  [Step 226] Loss: 0.6401 | Acc: 0.7500\n",
      "  [Step 227] Loss: 0.5138 | Acc: 0.8438\n",
      "  [Step 228] Loss: 0.6673 | Acc: 0.7812\n",
      "  [Step 229] Loss: 0.5869 | Acc: 0.7812\n",
      "  [Step 230] Loss: 0.5140 | Acc: 0.8125\n",
      "  [Step 231] Loss: 1.1363 | Acc: 0.5625\n",
      "  [Step 232] Loss: 0.8417 | Acc: 0.7188\n",
      "  [Step 233] Loss: 0.7256 | Acc: 0.6562\n",
      "  [Step 234] Loss: 0.7619 | Acc: 0.7188\n",
      "  [Step 235] Loss: 0.5283 | Acc: 0.8125\n",
      "  [Step 236] Loss: 0.3775 | Acc: 0.7812\n",
      "  [Step 237] Loss: 0.6878 | Acc: 0.7500\n",
      "  [Step 238] Loss: 0.7847 | Acc: 0.7188\n",
      "  [Step 239] Loss: 0.9845 | Acc: 0.6562\n",
      "  [Step 240] Loss: 0.7727 | Acc: 0.6562\n",
      "  [Step 241] Loss: 1.0953 | Acc: 0.7500\n",
      "  [Step 242] Loss: 0.6319 | Acc: 0.7500\n",
      "  [Step 243] Loss: 0.7015 | Acc: 0.7188\n",
      "  [Step 244] Loss: 0.6241 | Acc: 0.8125\n",
      "  [Step 245] Loss: 0.6826 | Acc: 0.6875\n",
      "  [Step 246] Loss: 1.0773 | Acc: 0.5938\n",
      "  [Step 247] Loss: 0.8068 | Acc: 0.7188\n",
      "  [Step 248] Loss: 0.7382 | Acc: 0.6562\n",
      "  [Step 249] Loss: 0.6322 | Acc: 0.7188\n",
      "  [Step 250] Loss: 0.7353 | Acc: 0.6562\n",
      "  [Step 251] Loss: 0.8189 | Acc: 0.6875\n",
      "  [Step 252] Loss: 0.6339 | Acc: 0.7500\n",
      "  [Step 253] Loss: 0.9304 | Acc: 0.7188\n",
      "  [Step 254] Loss: 0.5410 | Acc: 0.8750\n",
      "  [Step 255] Loss: 0.7319 | Acc: 0.7812\n",
      "  [Step 256] Loss: 0.9067 | Acc: 0.6250\n",
      "  [Step 257] Loss: 0.9128 | Acc: 0.5938\n",
      "  [Step 258] Loss: 0.7052 | Acc: 0.7188\n",
      "  [Step 259] Loss: 0.6372 | Acc: 0.8438\n",
      "  [Step 260] Loss: 0.5994 | Acc: 0.8438\n",
      "  [Step 261] Loss: 0.7743 | Acc: 0.6562\n",
      "  [Step 262] Loss: 0.8960 | Acc: 0.6562\n",
      "  [Step 263] Loss: 1.0306 | Acc: 0.6250\n",
      "  [Step 264] Loss: 1.2155 | Acc: 0.5312\n",
      "  [Step 265] Loss: 0.4988 | Acc: 0.8438\n",
      "  [Step 266] Loss: 0.5846 | Acc: 0.8438\n",
      "  [Step 267] Loss: 0.7708 | Acc: 0.6875\n",
      "  [Step 268] Loss: 0.6790 | Acc: 0.6875\n",
      "  [Step 269] Loss: 0.9703 | Acc: 0.6250\n",
      "  [Step 270] Loss: 0.7954 | Acc: 0.6562\n",
      "  [Step 271] Loss: 0.7076 | Acc: 0.7188\n",
      "  [Step 272] Loss: 1.1202 | Acc: 0.7188\n",
      "  [Step 273] Loss: 0.7013 | Acc: 0.7500\n",
      "  [Step 274] Loss: 0.7704 | Acc: 0.6250\n",
      "  [Step 275] Loss: 0.8514 | Acc: 0.6562\n",
      "  [Step 276] Loss: 0.8236 | Acc: 0.6562\n",
      "  [Step 277] Loss: 0.5962 | Acc: 0.7188\n",
      "  [Step 278] Loss: 0.6912 | Acc: 0.7188\n",
      "  [Step 279] Loss: 0.7124 | Acc: 0.7812\n",
      "  [Step 280] Loss: 0.8112 | Acc: 0.7500\n",
      "  [Step 281] Loss: 0.6628 | Acc: 0.8438\n",
      "  [Step 282] Loss: 0.5751 | Acc: 0.7812\n",
      "  [Step 283] Loss: 0.6429 | Acc: 0.8125\n",
      "  [Step 284] Loss: 0.7542 | Acc: 0.6562\n",
      "  [Step 285] Loss: 0.6298 | Acc: 0.7812\n",
      "  [Step 286] Loss: 0.5172 | Acc: 0.8750\n",
      "  [Step 287] Loss: 0.9539 | Acc: 0.6875\n",
      "  [Step 288] Loss: 0.6785 | Acc: 0.6875\n",
      "  [Step 289] Loss: 0.8773 | Acc: 0.6875\n",
      "  [Step 290] Loss: 0.6044 | Acc: 0.8125\n",
      "  [Step 291] Loss: 0.6195 | Acc: 0.6875\n",
      "  [Step 292] Loss: 0.4813 | Acc: 0.8750\n",
      "  [Step 293] Loss: 0.6016 | Acc: 0.7500\n",
      "  [Step 294] Loss: 0.5655 | Acc: 0.7812\n",
      "  [Step 295] Loss: 0.5322 | Acc: 0.8438\n",
      "  [Step 296] Loss: 0.4934 | Acc: 0.8125\n",
      "  [Step 297] Loss: 0.5205 | Acc: 0.7188\n",
      "  [Step 298] Loss: 0.8142 | Acc: 0.6875\n",
      "  [Step 299] Loss: 0.5426 | Acc: 0.7500\n",
      "  [Step 300] Loss: 0.6507 | Acc: 0.7500\n",
      "  [Step 301] Loss: 0.5216 | Acc: 0.8438\n",
      "  [Step 302] Loss: 0.6674 | Acc: 0.7812\n",
      "  [Step 303] Loss: 0.7279 | Acc: 0.6250\n",
      "  [Step 304] Loss: 0.5143 | Acc: 0.8438\n",
      "  [Step 305] Loss: 0.5447 | Acc: 0.8125\n",
      "  [Step 306] Loss: 0.6101 | Acc: 0.8125\n",
      "  [Step 307] Loss: 0.6179 | Acc: 0.7812\n",
      "  [Step 308] Loss: 1.0816 | Acc: 0.6875\n",
      "  [Step 309] Loss: 0.9276 | Acc: 0.6562\n",
      "  [Step 310] Loss: 0.5346 | Acc: 0.7812\n",
      "  [Step 311] Loss: 0.7966 | Acc: 0.6562\n",
      "  [Step 312] Loss: 0.6255 | Acc: 0.7188\n",
      "  [Step 313] Loss: 0.9298 | Acc: 0.6875\n",
      "  [Step 314] Loss: 0.5475 | Acc: 0.7812\n",
      "  [Step 315] Loss: 0.6793 | Acc: 0.7500\n",
      "  [Step 316] Loss: 0.4786 | Acc: 0.7812\n",
      "  [Step 317] Loss: 0.9151 | Acc: 0.7812\n",
      "  [Step 318] Loss: 0.9199 | Acc: 0.6875\n",
      "  [Step 319] Loss: 0.5375 | Acc: 0.8750\n",
      "  [Step 320] Loss: 0.7843 | Acc: 0.7812\n",
      "  [Step 321] Loss: 0.5332 | Acc: 0.8125\n",
      "  [Step 322] Loss: 1.0041 | Acc: 0.5938\n",
      "  [Step 323] Loss: 0.7479 | Acc: 0.7500\n",
      "  [Step 324] Loss: 0.8255 | Acc: 0.7188\n",
      "  [Step 325] Loss: 0.9291 | Acc: 0.6562\n",
      "  [Step 326] Loss: 0.8024 | Acc: 0.6875\n",
      "  [Step 327] Loss: 0.7542 | Acc: 0.6562\n",
      "  [Step 328] Loss: 0.4613 | Acc: 0.8438\n",
      "  [Step 329] Loss: 0.4721 | Acc: 0.8125\n",
      "  [Step 330] Loss: 0.7863 | Acc: 0.6562\n",
      "[Epoch 7] Train Loss: 0.2383 | Acc: 0.7345 || Val Loss: 0.6808 | Acc: 0.7445\n",
      "Precision: 0.7024 | Recall: 0.6172 | F1: 0.6230 | Time: 1081.39s\n",
      "[Alpha Softmax - Normal Cell]:\n",
      "[[0.09290922 0.04608893 0.11954284 0.05208219 0.09196148 0.08877455\n",
      "  0.11126698 0.22201821 0.08254617 0.09280945]\n",
      " [0.12280474 0.0544995  0.07605036 0.05241865 0.11140007 0.12075844\n",
      "  0.11241505 0.1192868  0.11618517 0.11418121]\n",
      " [0.10366818 0.05623241 0.1430765  0.05973022 0.10289362 0.1026743\n",
      "  0.12116423 0.11507916 0.09476094 0.10072044]\n",
      " [0.11757473 0.06026711 0.08011209 0.0601796  0.11860358 0.11631865\n",
      "  0.10994954 0.10807558 0.11407045 0.11484872]\n",
      " [0.10674883 0.08410191 0.10131798 0.08829658 0.10491979 0.10417724\n",
      "  0.10551153 0.10470182 0.10363197 0.09659233]\n",
      " [0.10655855 0.06401136 0.13429207 0.06730559 0.10506035 0.10350394\n",
      "  0.10480313 0.10983717 0.09818144 0.10644637]\n",
      " [0.11428566 0.06554979 0.08294531 0.06495695 0.11136401 0.11320737\n",
      "  0.11037206 0.11253127 0.11303663 0.11175096]\n",
      " [0.10440528 0.08642033 0.09972969 0.08959319 0.10379354 0.10190649\n",
      "  0.10590299 0.10433962 0.10366131 0.10024751]\n",
      " [0.10776623 0.08247688 0.09377523 0.08494347 0.10582684 0.10560944\n",
      "  0.10669093 0.10428197 0.10640001 0.10222899]\n",
      " [0.11069209 0.05731077 0.1234118  0.0573531  0.10989296 0.10986146\n",
      "  0.09936559 0.12214772 0.10313004 0.1068345 ]\n",
      " [0.11504056 0.063274   0.08258528 0.06314147 0.11162154 0.11475426\n",
      "  0.10662364 0.11731275 0.11137038 0.11427611]\n",
      " [0.10714471 0.08418088 0.09712114 0.08620305 0.10374468 0.10153598\n",
      "  0.10627513 0.10636976 0.10498219 0.10244246]\n",
      " [0.10875963 0.0789365  0.08900914 0.07999361 0.10507969 0.10608032\n",
      "  0.10946283 0.10832944 0.10746173 0.10688721]\n",
      " [0.11021139 0.07445022 0.08567635 0.07606749 0.10536009 0.10866426\n",
      "  0.10984454 0.11907372 0.10632776 0.10432416]\n",
      " [0.10953659 0.06192257 0.12559006 0.06066601 0.10696179 0.10763752\n",
      "  0.10647217 0.11160339 0.10642917 0.10318075]\n",
      " [0.11353675 0.06322907 0.08266798 0.0623997  0.11202706 0.1109395\n",
      "  0.11319878 0.12627977 0.10629826 0.10942318]\n",
      " [0.10603263 0.08578915 0.09834352 0.08726929 0.10539327 0.10403711\n",
      "  0.10458515 0.10522591 0.10107338 0.1022506 ]\n",
      " [0.10657187 0.08001634 0.09110091 0.08048129 0.10640291 0.10874811\n",
      "  0.10515413 0.10632822 0.10874742 0.10644881]\n",
      " [0.11016808 0.07689272 0.08816064 0.07788452 0.10750983 0.10947183\n",
      "  0.10532237 0.11466095 0.10314834 0.10678078]\n",
      " [0.11064464 0.07213648 0.08195749 0.0729728  0.10617517 0.1105068\n",
      "  0.1044379  0.11406626 0.11609829 0.11100414]\n",
      " [0.10761049 0.06079591 0.10830072 0.05948987 0.10805136 0.10753336\n",
      "  0.11122917 0.12711513 0.10427002 0.10560393]\n",
      " [0.11629409 0.0677589  0.07913044 0.06655255 0.11309294 0.11440577\n",
      "  0.11441877 0.11073116 0.10997412 0.10764123]\n",
      " [0.10559295 0.08580428 0.09932346 0.0868406  0.10585637 0.10179801\n",
      "  0.10311875 0.10514275 0.10218608 0.10433675]\n",
      " [0.10869584 0.08113512 0.09211106 0.0813766  0.10644312 0.1064504\n",
      "  0.10600531 0.10722124 0.1047175  0.10584375]\n",
      " [0.11037428 0.07685157 0.08993033 0.07733959 0.11072836 0.1074035\n",
      "  0.10930105 0.10483198 0.10901225 0.1042271 ]\n",
      " [0.11198911 0.0736122  0.0849776  0.07403636 0.10989741 0.11138983\n",
      "  0.10773575 0.10752853 0.10949576 0.10933747]\n",
      " [0.11310004 0.06888416 0.08083157 0.06974946 0.11196513 0.11130422\n",
      "  0.11768934 0.11252637 0.10511129 0.10883834]\n",
      " [0.10605747 0.05724747 0.13732138 0.05554846 0.10723884 0.10664979\n",
      "  0.10405009 0.1155219  0.10350289 0.10686173]\n",
      " [0.11429484 0.06624284 0.08802168 0.064532   0.11284831 0.11313182\n",
      "  0.11000746 0.11096251 0.11151823 0.10844027]\n",
      " [0.10544326 0.08578811 0.10349033 0.0861249  0.10175695 0.10218587\n",
      "  0.10548003 0.10206433 0.10180357 0.10586269]\n",
      " [0.10695789 0.08179805 0.09843715 0.08148614 0.10661242 0.10648765\n",
      "  0.10518709 0.10691663 0.10644378 0.09967319]\n",
      " [0.10760929 0.07780434 0.09566393 0.07785313 0.10631552 0.10495707\n",
      "  0.10709739 0.10631799 0.10872709 0.10765421]\n",
      " [0.11006118 0.07549465 0.09322727 0.07533795 0.10812435 0.10758068\n",
      "  0.11158509 0.10971373 0.10670729 0.10216783]\n",
      " [0.11099414 0.07091188 0.08958822 0.07107264 0.10932598 0.11136311\n",
      "  0.10645727 0.1137259  0.11018308 0.1063778 ]\n",
      " [0.11269824 0.06664312 0.08532254 0.06662945 0.11045091 0.11122207\n",
      "  0.10786455 0.12176691 0.11174884 0.10565342]]\n",
      "[Annealable Pruning] Applied at Epoch 7 with T = 1.5655\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'set' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Run DARTS search with pruning\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Running DARTS search with BDP...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m searched_genotype, pruned_train_loader, pruned_val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_darts_search_bdp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Duc/NASEEG1_Final/NASEEG/SleepC/sleep_nas/darts_search_bdp.py:136\u001b[0m, in \u001b[0;36mtrain_darts_search_bdp\u001b[0;34m(train_loader, val_loader, num_classes, epochs, device, prune_every, pt, pv)\u001b[0m\n\u001b[1;32m    133\u001b[0m all_V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39my)))\n\u001b[1;32m    135\u001b[0m keep_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_T \u001b[38;5;241m-\u001b[39m prune_T)\n\u001b[0;32m--> 136\u001b[0m keep_V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mall_V\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mX[keep_T]\n\u001b[1;32m    139\u001b[0m train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39my[keep_T]\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'set' and 'int'"
     ]
    }
   ],
   "source": [
    "# 3. Run DARTS search with pruning\n",
    "print(\"[INFO] Running DARTS search with BDP...\")\n",
    "searched_genotype, pruned_train_loader, pruned_val_loader = train_darts_search_bdp(\n",
    "    train_loader, val_loader, num_classes,\n",
    "    epochs=40, prune_every=7, pt=0.05, pv=0.05,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Visualize searched cells-----------------------------------------\n",
    "print(\"[INFO] Visualizing searched cells...\")\n",
    "plot_cell(searched_genotype, 'normal')\n",
    "plot_cell(searched_genotype, 'reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774adc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "# 5. Prepare data for cross-validation\n",
    "print(\"[INFO] Running 5-Fold Cross Validation on pruned data...\")\n",
    "X_all = torch.cat([pruned_train_loader.dataset.X, pruned_val_loader.dataset.X], dim=0)\n",
    "y_all = torch.cat([pruned_train_loader.dataset.y, pruned_val_loader.dataset.y], dim=0)\n",
    "dataset = TensorDataset(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, device, epochs=30, patience=15):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    best_acc = 0\n",
    "    early_stop_counter = 0\n",
    "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x.squeeze(-1)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        # Validation/Test\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {val_loss:.4f} | Test Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), \"final_model.pt\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"🛑 Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Plot\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_list, label='Train Loss')\n",
    "    plt.plot(val_loss_list, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Final Training - Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig('logs/final_loss_full.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc_list, label='Train Acc')\n",
    "    plt.plot(val_acc_list, label='Test Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Final Training - Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig('logs/final_accuracy_full.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# --- STEP 1: Tách train/test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=47\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=64)\n",
    "\n",
    "# # --- STEP 2: Định nghĩa mô hình ---\n",
    "model = FinalNetwork(C=8, num_classes=num_classes, layers=5, genotype=searched_genotype).to(device)\n",
    "\n",
    "\n",
    "# ✅ In ra cấu trúc mô hình trước khi train\n",
    "print(\"\\n[INFO] === Model Architecture ===\")\n",
    "print(model)\n",
    "print(f\"[INFO] Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"[INFO] Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- STEP 3: Train trên toàn bộ training set ---\n",
    "train_final_model(model, train_loader, test_loader, device=device, epochs=1)\n",
    "# --- STEP 4: Đánh giá trên tập test ---\n",
    "model.load_state_dict(torch.load(\"final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        test_loss_total += loss.item()\n",
    "        y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss_total / len(test_loader)\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"[✓] Final Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7accad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === structured_pruning_pipeline.py ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Compute filter importance ===\n",
    "def compute_importance(weight_tensor):\n",
    "    return weight_tensor.abs().sum(dim=(1, 2))\n",
    "\n",
    "# === Structured pruning with group-awareness ===\n",
    "def structured_prune_CLR_kRNF(model, prune_ratio=0.5):\n",
    "    keep_filters_dict = {}\n",
    "    importance_scores = {}\n",
    "    all_scores = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            score = compute_importance(module.weight.data)\n",
    "            importance_scores[name] = score\n",
    "            all_scores.append(score)\n",
    "\n",
    "    global_scores = torch.cat(all_scores)\n",
    "    threshold = torch.quantile(global_scores, prune_ratio)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            score = importance_scores[name]\n",
    "            sorted_idx = torch.argsort(score, descending=True)\n",
    "            n_keep = (score > threshold).sum().item()\n",
    "            groups = module.groups if hasattr(module, \"groups\") else 1\n",
    "            n_keep = max((n_keep // groups) * groups, groups)\n",
    "            keep_idx = sorted_idx[:n_keep]\n",
    "            keep_filters_dict[name] = keep_idx\n",
    "\n",
    "    return keep_filters_dict\n",
    "\n",
    "# === Apply pruning (Conv + BatchNorm) ===\n",
    "def apply_structured_filter_prune(model, keep_filters_dict):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and name in keep_filters_dict:\n",
    "            keep_idx = keep_filters_dict[name]\n",
    "            new_out_channels = len(keep_idx)\n",
    "\n",
    "            new_conv = nn.Conv1d(\n",
    "                in_channels=module.in_channels,\n",
    "                out_channels=new_out_channels,\n",
    "                kernel_size=module.kernel_size,\n",
    "                stride=module.stride,\n",
    "                padding=module.padding,\n",
    "                dilation=module.dilation,\n",
    "                groups=module.groups,\n",
    "                bias=module.bias is not None\n",
    "            )\n",
    "            new_conv.weight.data = module.weight.data[keep_idx].clone()\n",
    "            if module.bias is not None:\n",
    "                new_conv.bias.data = module.bias.data[keep_idx].clone()\n",
    "\n",
    "            parent = model\n",
    "            parts = name.split('.')\n",
    "            for p in parts[:-1]:\n",
    "                parent = getattr(parent, p)\n",
    "            setattr(parent, parts[-1], new_conv)\n",
    "\n",
    "            # === Try matching BN ===\n",
    "            try:\n",
    "                bn_name = parts[:-1] + [str(int(parts[-1]) + 1)]\n",
    "                bn_ref = model\n",
    "                for p in bn_name:\n",
    "                    bn_ref = getattr(bn_ref, p)\n",
    "                if isinstance(bn_ref, nn.BatchNorm1d):\n",
    "                    new_bn = nn.BatchNorm1d(new_out_channels)\n",
    "                    new_bn.weight.data = bn_ref.weight.data[keep_idx].clone()\n",
    "                    new_bn.bias.data = bn_ref.bias.data[keep_idx].clone()\n",
    "                    new_bn.running_mean = bn_ref.running_mean[keep_idx].clone()\n",
    "                    new_bn.running_var = bn_ref.running_var[keep_idx].clone()\n",
    "                    setattr(parent, str(int(parts[-1]) + 1), new_bn)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# === Save/load model ===\n",
    "def save_pruned_model(model, path=\"pruned_model_structured_full.pt\"):\n",
    "    torch.save(model, path)\n",
    "    print(f\"[\\u2713] Pruned model saved to: {path}\")\n",
    "\n",
    "def load_pruned_model(path, device):\n",
    "    model = torch.load(path)\n",
    "    return model.to(device)\n",
    "\n",
    "# === Display filter stats ===\n",
    "def print_filters_remaining(model, keep_filters_dict):\n",
    "    print(\"\\n[INFO] Remaining filters per Conv1d layer:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and name in keep_filters_dict:\n",
    "            print(f\" - {name}: {len(keep_filters_dict[name])} / {module.out_channels} kept\")\n",
    "\n",
    "# === Train pruned model ===\n",
    "def train_model(model, train_loader, test_loader, device, epochs=1):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                val_correct += (pred == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Prune model\n",
    "keep_filters = structured_prune_CLR_kRNF(model, prune_ratio=0.5)\n",
    "apply_structured_filter_prune(model, keep_filters)\n",
    "print_filters_remaining(model, keep_filters)\n",
    "\n",
    "# === 2. Save pruned model\n",
    "save_pruned_model(model, \"pruned_finalnetwork_structured_full.pt\")\n",
    "\n",
    "# === 3. Load lại và train tiếp\n",
    "model = load_pruned_model(\"pruned_finalnetwork_structured_full.pt\", device)\n",
    "# ... rồi train như bình thường\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb28ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_pruned_model(\"pruned_finalnetwork_structured_full.pt\", device)\n",
    "train_model(model, train_loader, test_loader, device, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Tính độ quan trọng (L1-norm)\n",
    "def compute_importance(weight_tensor):\n",
    "    return weight_tensor.abs().sum(dim=(1, 2))\n",
    "\n",
    "# 2. CLR + k-RNF Structured Pruning\n",
    "def structured_prune_CLR_kRNF(model, prune_ratio=0.5):\n",
    "    keep_filters_dict = {}\n",
    "    importance_scores = {}\n",
    "    all_scores = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            score = compute_importance(module.weight.data)\n",
    "            importance_scores[name] = score\n",
    "            all_scores.append(score)\n",
    "\n",
    "    global_scores = torch.cat(all_scores)\n",
    "    threshold = torch.quantile(global_scores, prune_ratio)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            score = importance_scores[name]\n",
    "            sorted_idx = torch.argsort(score, descending=True)\n",
    "            n_keep = (score > threshold).sum().item()\n",
    "            groups = module.groups if hasattr(module, \"groups\") else 1\n",
    "            n_keep = max((n_keep // groups) * groups, groups)  # đảm bảo chia hết group\n",
    "            keep_idx = sorted_idx[:n_keep]\n",
    "            keep_filters_dict[name] = keep_idx\n",
    "\n",
    "    return keep_filters_dict\n",
    "\n",
    "def apply_structured_filter_prune(model, keep_filters_dict):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and name in keep_filters_dict:\n",
    "            keep_idx = keep_filters_dict[name]\n",
    "            new_out_channels = len(keep_idx)\n",
    "\n",
    "            # Replace Conv1d\n",
    "            new_conv = nn.Conv1d(\n",
    "                in_channels=module.in_channels,\n",
    "                out_channels=new_out_channels,\n",
    "                kernel_size=module.kernel_size,\n",
    "                stride=module.stride,\n",
    "                padding=module.padding,\n",
    "                dilation=module.dilation,\n",
    "                groups=module.groups,\n",
    "                bias=module.bias is not None\n",
    "            )\n",
    "            new_conv.weight.data = module.weight.data[keep_idx].clone()\n",
    "            if module.bias is not None:\n",
    "                new_conv.bias.data = module.bias.data[keep_idx].clone()\n",
    "\n",
    "            # === Replace Conv1d ===\n",
    "            parent = model\n",
    "            parts = name.split('.')\n",
    "            for p in parts[:-1]:\n",
    "                parent = getattr(parent, p)\n",
    "            setattr(parent, parts[-1], new_conv)\n",
    "\n",
    "            # === Try to find accompanying BatchNorm ===\n",
    "            try:\n",
    "                bn_name = parts[:-1] + [str(int(parts[-1]) + 1)]  # e.g., stem.0 → stem.1\n",
    "                bn_ref = model\n",
    "                for p in bn_name:\n",
    "                    bn_ref = getattr(bn_ref, p)\n",
    "                if isinstance(bn_ref, nn.BatchNorm1d):\n",
    "                    new_bn = nn.BatchNorm1d(new_out_channels)\n",
    "                    new_bn.weight.data = bn_ref.weight.data[keep_idx].clone()\n",
    "                    new_bn.bias.data = bn_ref.bias.data[keep_idx].clone()\n",
    "                    new_bn.running_mean = bn_ref.running_mean[keep_idx].clone()\n",
    "                    new_bn.running_var = bn_ref.running_var[keep_idx].clone()\n",
    "                    setattr(parent, str(int(parts[-1]) + 1), new_bn)\n",
    "            except Exception:\n",
    "                pass  # No matching BN found\n",
    "\n",
    "\n",
    "# 4. In số lượng filter còn lại trên mỗi lớp Conv1d\n",
    "def print_filters_remaining(model, keep_filters_dict):\n",
    "    print(\"\\n[INFO] Remaining filters per Conv1d layer:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and name in keep_filters_dict:\n",
    "            total = module.out_channels\n",
    "            kept = len(keep_filters_dict[name])\n",
    "            print(f\" - {name}: {kept} / {total} filters kept\")\n",
    "\n",
    "# 5. Lưu mô hình đã prune\n",
    "def save_pruned_model(model, path=\"pruned_model_structured.pt\"):\n",
    "    torch.save(model, \"pruned_finalnetwork_structured_full.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_filters = structured_prune_CLR_kRNF(model, prune_ratio=0.5)\n",
    "print_filters_remaining(model, keep_filters)\n",
    "apply_structured_filter_prune(model, keep_filters)\n",
    "save_pruned_model(model, \"pruned_finalnetwork_structured.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# === STEP 1: Tách train/test ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=47\n",
    ")\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=64)\n",
    "\n",
    "model = torch.load(\"pruned_finalnetwork_structured_full.pt\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# === STEP 3: Train lại mô hình\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):  # huấn luyện 1 epoch (hoặc tăng nếu muốn)\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # === Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_correct += (pred == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# === STEP 4: Đánh giá sau cùng\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "test_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        test_loss_total += loss.item()\n",
    "        y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss_total / len(test_loader)\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"[✓] Final Test Loss: {test_loss:.4f} | Final Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "# === STEP 1: Tách train/test ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, stratify=y_all, random_state=47\n",
    ")\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=64)\n",
    "\n",
    "# === STEP 2: Load mô hình ===\n",
    "model = FinalNetwork(C=8, num_classes=num_classes, layers=5, genotype=searched_genotype).to(device)\n",
    "model.load_state_dict(torch.load(\"final_model.pt\"))\n",
    "\n",
    "# === STEP 3: Prune 50% trọng số theo magnitude\n",
    "def prune_model(model, amount=0.5):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "    return model\n",
    "\n",
    "model = prune_model(model, amount=0.5)\n",
    "\n",
    "# === STEP 4: XÓA MASK để cắt thật sự\n",
    "def remove_pruning_masks(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.Linear)) and hasattr(module, \"weight_mask\"):\n",
    "            prune.remove(module, \"weight\")\n",
    "\n",
    "remove_pruning_masks(model)\n",
    "\n",
    "# === STEP 5: Đếm lại số lượng weight thật sự\n",
    "def count_pruned_weights(model):\n",
    "    total, nonzero = 0, 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
    "            w = module.weight.data\n",
    "            total += w.numel()\n",
    "            nonzero += w.nonzero().size(0)\n",
    "    zero = total - nonzero\n",
    "    print(f\"[INFO] Total weights: {total}\")\n",
    "    print(f\"[INFO] Non-zero weights: {nonzero}\")\n",
    "    print(f\"[INFO] Pruned weights (==0): {zero}\")\n",
    "    print(f\"[INFO] Pruned ratio: {100 * zero / total:.2f}%\")\n",
    "\n",
    "count_pruned_weights(model)\n",
    "\n",
    "# === STEP 6: Train lại model đã prune\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    # === Training ===\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_correct += (pred == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Utility to count parameters ===\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"[INFO] Total parameters: {total:,}\")\n",
    "    print(f\"[INFO] Trainable parameters: {trainable:,}\")\n",
    "\n",
    "# === Pruning ===\n",
    "def apply_l1_pruning(model, amount=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return model\n",
    "\n",
    "# === Remove reparam hooks after pruning ===\n",
    "def finalize_pruning(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            try:\n",
    "                prune.remove(module, 'weight')\n",
    "            except:\n",
    "                pass\n",
    "    return model\n",
    "\n",
    "# === Training ===\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"[Train] Epoch {epoch+1}/{epochs} | Loss: {running_loss/total:.4f} | Acc: {acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                out = model(x)\n",
    "                val_correct += (out.argmax(1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f\"[Eval ] Epoch {epoch+1}/{epochs} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"[✓] Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# === Instantiate model ===\n",
    "model = FinalNetwork(C=8, num_classes=5, layers=5, genotype=searched_genotype)\n",
    "print(\"=== [ORIGINAL] ===\")\n",
    "count_params(model)\n",
    "\n",
    "# === Apply pruning ===\n",
    "model = apply_l1_pruning(model, amount=0.5)\n",
    "model = finalize_pruning(model)\n",
    "print(\"=== [PRUNED] ===\")\n",
    "count_params(model)\n",
    "\n",
    "# === Train / Eval ===\n",
    "# train_model(model, train_loader, test_loader, device, epochs=10)\n",
    "# evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from model_build import FinalNetwork\n",
    "from genotypes import Genotype\n",
    "\n",
    "# === GENOTYPE ===\n",
    "genotype = Genotype(\n",
    "    normal=[('sep_conv_1x5', 1), ('sep_conv_1x3', 0), ('dil_conv_1x3', 1), ('dil_conv_1x3', 0),\n",
    "            ('dil_conv_1x5', 1), ('conv_1x1', 0), ('conv_1x1', 1), ('conv_3x3', 4),\n",
    "            ('dil_conv_1x5', 5), ('sep_conv_1x5', 1), ('sep_conv_1x5', 6), ('sep_conv_1x5', 5),\n",
    "            ('dil_conv_1x3', 7), ('sep_conv_1x3', 6)],\n",
    "    normal_concat=[2, 3, 4, 5, 6, 7, 8],\n",
    "    reduce=[('dil_conv_1x3', 1), ('max_pool_3x3', 0), ('dil_conv_1x3', 1), ('max_pool_3x3', 0),\n",
    "            ('skip_connect', 1), ('max_pool_3x3', 0), ('dil_conv_1x5', 1), ('dil_conv_1x3', 3),\n",
    "            ('dil_conv_1x3', 5), ('dil_conv_1x3', 1), ('skip_connect', 1), ('sep_conv_1x5', 3),\n",
    "            ('conv_1x1', 1), ('sep_conv_1x5', 3)],\n",
    "    reduce_concat=[2, 3, 4, 5, 6, 7, 8]\n",
    ")\n",
    "\n",
    "# === IMPORTS ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "# === PARAMETER COUNT ===\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"[INFO] Total parameters: {total:,}\")\n",
    "    print(f\"[INFO] Trainable parameters: {trainable:,}\")\n",
    "\n",
    "# === L1 SCORE ===\n",
    "def compute_importance_scores(model):\n",
    "    importance = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and module.groups == 1:\n",
    "            score = torch.sum(torch.abs(module.weight.data), dim=(1, 2))\n",
    "            importance[name] = score.cpu()\n",
    "    return importance\n",
    "\n",
    "# === GLOBAL RANKING ===\n",
    "def get_global_ranking(importance_scores):\n",
    "    all_scores = []\n",
    "    mapping = {}\n",
    "    pointer = 0\n",
    "    for name, score in importance_scores.items():\n",
    "        for i, s in enumerate(score):\n",
    "            all_scores.append((s.item(), pointer))\n",
    "            mapping[pointer] = (name, i)\n",
    "            pointer += 1\n",
    "    all_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    ranked_indices = [idx for _, idx in all_scores]\n",
    "    return ranked_indices, mapping, all_scores\n",
    "\n",
    "# === TOP FILTERS ===\n",
    "def select_top_filters(ranked_indices, mapping, prune_rate=0.5):\n",
    "    num_total = len(ranked_indices)\n",
    "    num_to_keep = int(num_total * (1 - prune_rate))\n",
    "    selected = ranked_indices[:num_to_keep]\n",
    "    keep_dict = defaultdict(list)\n",
    "    for idx in selected:\n",
    "        layer_name, filt_idx = mapping[idx]\n",
    "        keep_dict[layer_name].append(filt_idx)\n",
    "    for k in keep_dict:\n",
    "        keep_dict[k] = sorted(keep_dict[k])\n",
    "    return dict(keep_dict)\n",
    "\n",
    "# === PRUNE CONV1D ===\n",
    "def build_pruned_conv1d(original, keep_out, keep_in=None):\n",
    "    in_ch = len(keep_in) if keep_in is not None else original.in_channels\n",
    "    out_ch = len(keep_out)\n",
    "    conv = nn.Conv1d(\n",
    "        in_channels=in_ch,\n",
    "        out_channels=out_ch,\n",
    "        kernel_size=original.kernel_size,\n",
    "        stride=original.stride,\n",
    "        padding=original.padding,\n",
    "        dilation=original.dilation,\n",
    "        groups=1,\n",
    "        bias=original.bias is not None\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        w = original.weight.data[keep_out]\n",
    "        if keep_in is not None:\n",
    "            w = w[:, keep_in, :]\n",
    "        conv.weight.copy_(w)\n",
    "        if original.bias is not None:\n",
    "            conv.bias.copy_(original.bias.data[keep_out])\n",
    "    return conv\n",
    "\n",
    "# === PRUNE BN1D ===\n",
    "def build_pruned_bn1d(original, keep_idxs):\n",
    "    new_bn = nn.BatchNorm1d(len(keep_idxs))\n",
    "    with torch.no_grad():\n",
    "        if len(original.weight) > max(keep_idxs):\n",
    "            new_bn.weight.copy_(original.weight.data[keep_idxs])\n",
    "            new_bn.bias.copy_(original.bias.data[keep_idxs])\n",
    "            new_bn.running_mean.copy_(original.running_mean[keep_idxs])\n",
    "            new_bn.running_var.copy_(original.running_var[keep_idxs])\n",
    "    return new_bn\n",
    "\n",
    "# === APPLY PRUNING ===\n",
    "def apply_k_rnf_pruning(model, importance_scores, keep_map, k=3):\n",
    "    new_model = deepcopy(model)\n",
    "    prev_kept = {}  # track previous layer's kept filters\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d) and module.groups == 1 and not name.startswith(\"stem.0\"):\n",
    "            keep_out = keep_map.get(name, None)\n",
    "            if keep_out is None:\n",
    "                print(f\"[WARN] No filters kept in {name}, skipping\")\n",
    "                continue\n",
    "\n",
    "            keep_in = prev_kept.get(name, None)\n",
    "            parent = new_model\n",
    "            subnames = name.split(\".\")\n",
    "            for s in subnames[:-1]:\n",
    "                parent = getattr(parent, s)\n",
    "\n",
    "            new_conv = build_pruned_conv1d(module, keep_out, keep_in)\n",
    "            setattr(parent, subnames[-1], new_conv)\n",
    "\n",
    "            # Adjust BatchNorm\n",
    "            bn_name = subnames[-1].replace(\"conv\", \"bn\")\n",
    "            if hasattr(parent, bn_name):\n",
    "                bn_module = getattr(parent, bn_name)\n",
    "                if isinstance(bn_module, nn.BatchNorm1d):\n",
    "                    try:\n",
    "                        new_bn = build_pruned_bn1d(bn_module, keep_out)\n",
    "                        setattr(parent, bn_name, new_bn)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Could not adjust BN: {bn_name} | {e}\")\n",
    "\n",
    "            prev_kept[name] = keep_out\n",
    "            print(f\"[✓] Pruned {name} → out={len(keep_out)}\" + (f\", in={len(keep_in)}\" if keep_in else \"\"))\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# === TRAINING ===\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {loss_sum/total:.4f} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "        # Eval\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                out = model(x)\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        val_acc = 100. * correct / total\n",
    "        print(f\"[Eval] Epoch {epoch+1} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# === EVALUATION ===\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            pred = model(x).argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"[✓] Final Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Khởi tạo mô hình ===\n",
    "model = FinalNetwork(C=8, num_classes=5, layers=5, genotype=genotype).to(device)\n",
    "count_params(model)\n",
    "\n",
    "# === Tính độ quan trọng, prune, và huấn luyện lại ===\n",
    "importance = compute_importance_scores(model)\n",
    "ranked, mapping, _ = get_global_ranking(importance)\n",
    "keep_map = select_top_filters(ranked, mapping, prune_rate=0.6)\n",
    "pruned_model = apply_k_rnf_pruning(model, importance, keep_map, k=3)\n",
    "\n",
    "print(\"=== [AFTER PRUNING] ===\")\n",
    "count_params(pruned_model)\n",
    "\n",
    "train_model(pruned_model, train_loader, test_loader, device, epochs=10)\n",
    "evaluate_model(pruned_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "....import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, device, fold=1, epochs=30):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_acc = 0\n",
    "    train_loss_list, val_loss_list, train_acc_list, val_acc_list = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x.squeeze(-1)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"best_final_model_fold{fold}.pt\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[Final Train Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # --- Save plots ---\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_list, label='Train Loss')\n",
    "    plt.plot(val_loss_list, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold} - Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'logs/final_loss_fold{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc_list, label='Train Acc')\n",
    "    plt.plot(val_acc_list, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Fold {fold} - Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'logs/final_accuracy_fold{fold}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "    print(f\"\\n[INFO] Fold {fold+1}/5\")\n",
    "\n",
    "    fold_train = DataLoader(TensorDataset(X_all[train_idx], y_all[train_idx]), batch_size=64, shuffle=True)\n",
    "    fold_val   = DataLoader(TensorDataset(X_all[val_idx],   y_all[val_idx]),   batch_size=64)\n",
    "\n",
    "    # ✅ Define model\n",
    "    model = FinalNetwork(C=16, num_classes=num_classes, layers=7, genotype=searched_genotype).to(device)\n",
    "\n",
    "    # ✅ Train and save best checkpoint per fold\n",
    "    train_final_model(model, fold_train, fold_val, device=device, fold=fold+1, epochs=30)\n",
    "\n",
    "    # ✅ Load best checkpoint for this fold\n",
    "    model.load_state_dict(torch.load(f\"best_final_model_fold{fold+1}.pt\"))\n",
    "\n",
    "    # ✅ Evaluate and log confusion matrix, predictions\n",
    "    evaluate_model(model, fold_val, device, fold=fold+1)\n",
    "\n",
    "    # ✅ Save acc/loss manually for final printout\n",
    "    model.eval()\n",
    "    y_true_val, y_pred_val = [], []\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in fold_val:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss_total += loss.item()\n",
    "            y_pred_val.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            y_true_val.extend(y.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss_total / len(fold_val)\n",
    "    val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "\n",
    "    # For train acc check (optional, not strictly necessary)\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    train_loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in fold_train:\n",
    "            x, y = x.to(device).squeeze(-1), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            train_loss_total += loss.item()\n",
    "            y_pred_train.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            y_true_train.extend(y.cpu().numpy())\n",
    "\n",
    "    train_loss = train_loss_total / len(fold_train)\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "\n",
    "    print(f\"[Fold {fold+1}] ✅ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} || Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    all_y_pred.extend(y_pred_val)\n",
    "    all_y_true.extend(y_true_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Confusion matrix tổng hợp\n",
    "print(\"\\n[INFO] Final Confusion Matrix (5-Fold CV on pruned data):\")\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "print(cm)\n",
    "\n",
    "# 9. Heatmap + Save\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Full Pruned Data - 5-Fold CV)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"final_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# 10. Save CSV\n",
    "pd.DataFrame(cm).to_csv(\"confusion_matrix.csv\", index=False)\n",
    "print(\"[✓] Confusion matrix saved to final_confusion_matrix.png and confusion_matrix.csv\")\n",
    "print(\"\\n[✓] Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import seaborn as sns\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     set_seed(42)\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#     print(\"[INFO] Loading 80/20 split data...\")\n",
    "#     train_loader, val_loader, num_classes = get_dataloaders_simple(batch_size=16)\n",
    "\n",
    "#     print(f\"[INFO] DARTS will run on {len(train_loader.dataset.y)} train samples and {len(val_loader.dataset.y)} val samples\")\n",
    "\n",
    "#     print(\"[INFO] Running DARTS search with BDP...\")\n",
    "#     searched_genotype, pruned_train_loader, pruned_val_loader = train_darts_search_bdp(\n",
    "#         train_loader, val_loader, num_classes,\n",
    "#         epochs=5, prune_every=5, pt=0.15, pv=0.05,\n",
    "#         device=device\n",
    "#     )\n",
    "\n",
    "#     print(\"[INFO] Visualizing searched cells...\")\n",
    "#     plot_cell(searched_genotype, 'normal')\n",
    "#     plot_cell(searched_genotype, 'reduce')\n",
    "\n",
    "#     print(\"[INFO] Running 5-Fold Cross Validation on pruned data...\")\n",
    "\n",
    "#     # ✅ Gộp lại dữ liệu sau pruning\n",
    "#     X_all = torch.cat([pruned_train_loader.dataset.X, pruned_val_loader.dataset.X], dim=0)\n",
    "#     y_all = torch.cat([pruned_train_loader.dataset.y, pruned_val_loader.dataset.y], dim=0)\n",
    "#     dataset = TensorDataset(X_all, y_all)\n",
    "\n",
    "#     # ✅ K-fold cross-validation\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     all_y_true, all_y_pred = [], []\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "#         print(f\"\\n[INFO] Fold {fold+1}/5\")\n",
    "#         DataLoader(TensorDataset(X_all[train_idx], y_all[train_idx]), batch_size=64, shuffle=True)\n",
    "#         fold_val   = DataLoader(TensorDataset(X_all[val_idx],   y_all[val_idx]),   batch_size=64)\n",
    "\n",
    "#         fold_train = model = FinalNetwork(C=16, num_classes=num_classes, layers=9, genotype=searched_genotype).to(device)\n",
    "#         train_final_model(model, fold_train, fold_val, device=device, epochs=100)\n",
    "\n",
    "#         model.load_state_dict(torch.load(\"best_final_model1.pt\"))\n",
    "\n",
    "#         # ✅ Evaluation\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for x, y in fold_val:\n",
    "#                 x = x.to(device).squeeze(-1)\n",
    "#                 logits = model(x)\n",
    "#                 preds = logits.argmax(dim=1).cpu().numpy()\n",
    "#                 all_y_pred.extend(preds)\n",
    "#                 all_y_true.extend(y.numpy())\n",
    "\n",
    "#     # ✅ Tổng hợp confusion matrix\n",
    "#     print(\"\\n[INFO] Final Confusion Matrix (5-Fold CV on pruned data):\")\n",
    "#     from sklearn.metrics import confusion_matrix\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import pandas as pd\n",
    "\n",
    "#     cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "#     print(cm)\n",
    "\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.title(\"Confusion Matrix (Full Pruned Data - 5-Fold CV)\")\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"True\")\n",
    "#     plt.savefig(\"final_confusion_matrix.png\")\n",
    "#     plt.show()\n",
    "\n",
    "#     pd.DataFrame(cm).to_csv(\"confusion_matrix.csv\", index=False)\n",
    "#     print(\"[✓] Confusion matrix saved to final_confusion_matrix.png and confusion_matrix.csv\")\n",
    "#     print(\"\\n[✓] Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c7935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.5 (NGC 24.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
